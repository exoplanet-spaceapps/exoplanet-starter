"""
æ¨è«–ç®¡ç·šï¼šTIC -> MAST ä¸‹è¼‰ -> ç‰¹å¾µèƒå– -> é æ¸¬
"""
from typing import Dict, List, Tuple, Optional, Any
import numpy as np
import pandas as pd
from pathlib import Path
import json
import joblib
import warnings
warnings.filterwarnings('ignore')

try:
    import lightkurve as lk
except ImportError:
    print("Warning: lightkurve not installed")
    lk = None

from .bls_features import run_bls, extract_features
# Note: Light curve download is handled directly by lightkurve in predict_from_tic()


def predict_from_tic(
    tic_id: str,
    model_path: str = "model/ranker.joblib",
    scaler_path: str = "model/scaler.joblib",
    feature_schema_path: str = "model/feature_schema.json",
    mission: str = "TESS",
    detrend_window: int = 401,
    verbose: bool = True
) -> Dict[str, Any]:
    """
    å¾ TIC ID é€²è¡Œç«¯åˆ°ç«¯é æ¸¬

    Parameters:
    -----------
    tic_id : str
        ç›®æ¨™è­˜åˆ¥ç¢¼ï¼ˆä¾‹å¦‚ "TIC 25155310" æˆ– "25155310"ï¼‰
    model_path : str
        è¨“ç·´å¥½çš„æ¨¡å‹è·¯å¾‘
    scaler_path : str
        ç‰¹å¾µæ¨™æº–åŒ–å™¨è·¯å¾‘
    feature_schema_path : str
        ç‰¹å¾µæ¶æ§‹æª”æ¡ˆè·¯å¾‘
    mission : str
        ä»»å‹™åç¨±ï¼ˆTESS æˆ– Keplerï¼‰
    detrend_window : int
        å»è¶¨å‹¢çª—å£å¤§å°
    verbose : bool
        æ˜¯å¦é¡¯ç¤ºè©³ç´°è³‡è¨Š

    Returns:
    --------
    dict : åŒ…å«é æ¸¬çµæœå’Œä¸­é–“è³‡æ–™çš„å­—å…¸
    """
    result = {
        'tic_id': tic_id,
        'success': False,
        'error': None,
        'probability': None,
        'bls_period': None,
        'bls_snr': None,
        'bls_depth': None,
        'features': {},
        'lightcurve': None
    }

    try:
        # 1. æ ¼å¼åŒ– TIC ID
        if not tic_id.startswith("TIC") and not tic_id.startswith("KIC"):
            tic_id = f"TIC {tic_id}"

        if verbose:
            print(f"ğŸ” è™•ç†ç›®æ¨™: {tic_id}")

        # 2. ä¸‹è¼‰å…‰æ›²ç·š
        if verbose:
            print("   ä¸‹è¼‰å…‰æ›²ç·š...")

        if lk is None:
            raise ImportError("Lightkurve not available")

        search_result = lk.search_lightcurve(
            tic_id,
            mission=mission,
            author="SPOC" if mission == "TESS" else "Kepler"
        )

        if len(search_result) == 0:
            raise ValueError(f"æ‰¾ä¸åˆ° {tic_id} çš„å…‰æ›²ç·šè³‡æ–™")

        # ä¸‹è¼‰ç¬¬ä¸€å€‹çµæœ
        lc = search_result[0].download()

        # 3. æ¸…ç†å’Œå»è¶¨å‹¢
        if verbose:
            print("   æ¸…ç†å’Œå»è¶¨å‹¢...")

        lc_clean = lc.remove_nans()
        if len(lc_clean) < 100:
            raise ValueError("å…‰æ›²ç·šè³‡æ–™é»å¤ªå°‘ï¼ˆ<100ï¼‰")

        lc_flat = lc_clean.flatten(window_length=detrend_window)

        time = lc_flat.time.value
        flux = lc_flat.flux.value

        result['lightcurve'] = {
            'time': time.tolist(),
            'flux': flux.tolist(),
            'mission': mission,
            'sector': getattr(lc, 'sector', None),
            'n_points': len(time)
        }

        # 4. åŸ·è¡Œ BLS æœå°‹
        if verbose:
            print("   åŸ·è¡Œ BLS æœå°‹...")

        bls_result = run_bls(time, flux)

        result['bls_period'] = bls_result.get('period')
        result['bls_snr'] = bls_result.get('snr')
        result['bls_depth'] = bls_result.get('depth')

        # 5. æå–ç‰¹å¾µ
        if verbose:
            print("   æå–ç‰¹å¾µ...")

        features = extract_features(time, flux, bls_result, compute_advanced=True)
        result['features'] = features

        # 6. è¼‰å…¥æ¨¡å‹å’Œç‰¹å¾µæ¶æ§‹
        if verbose:
            print("   è¼‰å…¥æ¨¡å‹...")

        model = joblib.load(model_path)

        # Handle scaler - it might be in the pipeline or separate
        if scaler_path and Path(scaler_path).exists():
            scaler = joblib.load(scaler_path)
        elif hasattr(model, 'named_steps') and 'scaler' in model.named_steps:
            scaler = model.named_steps['scaler']
            if verbose:
                print("   ä½¿ç”¨ pipeline ä¸­çš„æ¨™æº–åŒ–å™¨")
        else:
            scaler = None
            if verbose:
                print("   âš ï¸ æœªæ‰¾åˆ°æ¨™æº–åŒ–å™¨ï¼Œè·³éç‰¹å¾µæ¨™æº–åŒ–")

        # Handle feature schema
        if feature_schema_path and Path(feature_schema_path).exists():
            with open(feature_schema_path, 'r') as f:
                schema = json.load(f)
            feature_order = schema['feature_order']
        else:
            # Use default feature order
            feature_order = [
                'bls_period', 'bls_duration', 'bls_depth', 'bls_snr', 'bls_power',
                'odd_even_mismatch', 'secondary_power_ratio', 'harmonic_delta_chisq',
                'periodicity_strength', 'transit_symmetry', 'odd_even_depth_diff',
                'phase_coverage', 'ingress_egress_asymmetry', 'v_shape_indicator'
            ]
            if verbose:
                print(f"   ä½¿ç”¨é è¨­ç‰¹å¾µé †åº ({len(feature_order)} å€‹ç‰¹å¾µ)")

        # 7. æº–å‚™ç‰¹å¾µå‘é‡
        feature_vector = []
        for feat_name in feature_order:
            value = features.get(feat_name, 0.0)
            if np.isnan(value) or np.isinf(value):
                value = 0.0
            feature_vector.append(value)

        X = np.array(feature_vector).reshape(1, -1)

        # 8. æ¨™æº–åŒ–å’Œé æ¸¬
        if scaler is not None:
            X_scaled = scaler.transform(X)
        else:
            X_scaled = X

        # ç²å–é æ¸¬æ©Ÿç‡
        if hasattr(model, 'predict_proba'):
            prob = model.predict_proba(X_scaled)[0, 1]
        else:
            # å¦‚æœæ˜¯æ ¡æº–æ¨¡å‹
            prob = model.predict_proba(X)[0, 1]

        result['probability'] = float(prob)
        result['success'] = True

        if verbose:
            print(f"   âœ… é æ¸¬å®Œæˆ: {prob:.3f}")

    except Exception as e:
        result['error'] = str(e)
        if verbose:
            print(f"   âŒ éŒ¯èª¤: {e}")

    return result


def predict_batch(
    tic_list: List[str],
    model_path: str = "model/ranker.joblib",
    scaler_path: str = "model/scaler.joblib",
    feature_schema_path: str = "model/feature_schema.json",
    mission: str = "TESS",
    n_jobs: int = 1,
    verbose: bool = True
) -> pd.DataFrame:
    """
    æ‰¹æ¬¡è™•ç†å¤šå€‹ TIC

    Parameters:
    -----------
    tic_list : list
        TIC ID åˆ—è¡¨
    model_path : str
        æ¨¡å‹è·¯å¾‘
    scaler_path : str
        æ¨™æº–åŒ–å™¨è·¯å¾‘
    feature_schema_path : str
        ç‰¹å¾µæ¶æ§‹è·¯å¾‘
    mission : str
        ä»»å‹™åç¨±
    n_jobs : int
        ä¸¦è¡Œè™•ç†æ•¸ï¼ˆç›®å‰åƒ…æ”¯æ´ 1ï¼‰
    verbose : bool
        æ˜¯å¦é¡¯ç¤ºé€²åº¦

    Returns:
    --------
    pd.DataFrame : åŒ…å«æ‰€æœ‰çµæœçš„è³‡æ–™æ¡†
    """
    results = []

    for i, tic_id in enumerate(tic_list):
        if verbose:
            print(f"\n[{i+1}/{len(tic_list)}] è™•ç† {tic_id}")

        result = predict_from_tic(
            tic_id,
            model_path=model_path,
            scaler_path=scaler_path,
            feature_schema_path=feature_schema_path,
            mission=mission,
            verbose=verbose
        )

        # ç°¡åŒ–çµæœç”¨æ–¼è¡¨æ ¼
        summary = {
            'tic_id': result['tic_id'],
            'probability': result['probability'],
            'bls_period': result['bls_period'],
            'bls_snr': result['bls_snr'],
            'bls_depth': result['bls_depth'],
            'success': result['success'],
            'error': result['error']
        }

        # æ·»åŠ éƒ¨åˆ†é‡è¦ç‰¹å¾µ
        for key in ['odd_even_depth_diff', 'transit_symmetry', 'periodicity_strength']:
            if key in result.get('features', {}):
                summary[key] = result['features'][key]

        results.append(summary)

    # è½‰ç‚º DataFrame
    df = pd.DataFrame(results)

    # æ’åºï¼ˆæŒ‰æ©Ÿç‡é™åºï¼‰
    if 'probability' in df.columns:
        df = df.sort_values('probability', ascending=False)

    if verbose:
        print(f"\nâœ… æ‰¹æ¬¡è™•ç†å®Œæˆ: {len(df)} å€‹ç›®æ¨™")
        print(f"   æˆåŠŸ: {df['success'].sum()}")
        print(f"   å¤±æ•—: {(~df['success']).sum()}")

    return df


def check_gpu_availability():
    """
    æª¢æŸ¥ GPU å¯ç”¨æ€§å’Œé¡å‹

    Returns:
    --------
    dict : GPU è³‡è¨Š
    """
    gpu_info = {
        'available': False,
        'device_name': None,
        'is_l4': False,
        'supports_bfloat16': False
    }

    try:
        import torch

        if torch.cuda.is_available():
            gpu_info['available'] = True
            gpu_info['device_name'] = torch.cuda.get_device_name(0)

            # æª¢æŸ¥æ˜¯å¦ç‚º L4 GPU
            if 'L4' in gpu_info['device_name']:
                gpu_info['is_l4'] = True
                gpu_info['supports_bfloat16'] = True

            # æª¢æŸ¥ bfloat16 æ”¯æ´
            if hasattr(torch.cuda, 'is_bf16_supported'):
                gpu_info['supports_bfloat16'] = torch.cuda.is_bf16_supported()

    except ImportError:
        pass

    return gpu_info


def create_folded_lightcurve_plot(
    time: np.ndarray,
    flux: np.ndarray,
    period: float,
    t0: float = 0.0
) -> Dict[str, Any]:
    """
    å‰µå»ºæ‘ºç–Šå…‰æ›²ç·šè³‡æ–™

    Parameters:
    -----------
    time : np.ndarray
        æ™‚é–“åºåˆ—
    flux : np.ndarray
        æµé‡åºåˆ—
    period : float
        é€±æœŸ
    t0 : float
        ç¬¬ä¸€æ¬¡å‡Œæ—¥æ™‚åˆ»

    Returns:
    --------
    dict : æ‘ºç–Šå…‰æ›²ç·šè³‡æ–™
    """
    # è¨ˆç®—ç›¸ä½
    phase = ((time - t0) % period) / period
    phase[phase > 0.5] -= 1.0

    # åˆ†ç®±å¹³å‡ï¼ˆç”¨æ–¼ç¹ªåœ–ï¼‰
    n_bins = 100
    phase_bins = np.linspace(-0.5, 0.5, n_bins + 1)
    binned_phase = []
    binned_flux = []
    binned_std = []

    for i in range(n_bins):
        mask = (phase >= phase_bins[i]) & (phase < phase_bins[i + 1])
        if np.sum(mask) > 0:
            binned_phase.append((phase_bins[i] + phase_bins[i + 1]) / 2)
            binned_flux.append(np.median(flux[mask]))
            binned_std.append(np.std(flux[mask]))

    return {
        'phase': phase.tolist(),
        'flux': flux.tolist(),
        'binned_phase': binned_phase,
        'binned_flux': binned_flux,
        'binned_std': binned_std,
        'period': period,
        't0': t0
    }


def save_inference_results(
    results_df: pd.DataFrame,
    output_path: str = "results/inference_results.csv",
    include_metadata: bool = True
) -> str:
    """
    å„²å­˜æ¨è«–çµæœ

    Parameters:
    -----------
    results_df : pd.DataFrame
        æ¨è«–çµæœè³‡æ–™æ¡†
    output_path : str
        è¼¸å‡ºè·¯å¾‘
    include_metadata : bool
        æ˜¯å¦åŒ…å«å…ƒè³‡æ–™

    Returns:
    --------
    str : å„²å­˜è·¯å¾‘
    """
    # å»ºç«‹è¼¸å‡ºç›®éŒ„
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # å„²å­˜ CSV
    results_df.to_csv(output_path, index=False)

    if include_metadata:
        # å„²å­˜å…ƒè³‡æ–™
        import time
        metadata = {
            'n_targets': int(len(results_df)),
            'n_success': int(results_df['success'].sum()) if 'success' in results_df else 0,
            'inference_date': time.strftime("%Y-%m-%d %H:%M:%S"),
            'high_confidence': int(len(results_df[results_df['probability'] > 0.8])) if 'probability' in results_df else 0
        }

        metadata_path = output_path.parent / f"{output_path.stem}_metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)

    return str(output_path)