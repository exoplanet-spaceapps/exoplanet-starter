"""
è¨“ç·´èˆ‡æ ¡æº–æ¨¡çµ„ - å®Œæ•´å¯¦ä½œ
åŒ…å« LogisticRegressionã€XGBoostã€æ ¡æº–ã€äº¤å‰é©—è­‰èˆ‡æ¨¡å‹æŒä¹…åŒ–
"""
from typing import Dict, Any, Tuple, Optional, List, Union
import numpy as np
import pandas as pd
from pathlib import Path
import json
import joblib
import warnings
from datetime import datetime

from sklearn.model_selection import (
    StratifiedKFold,
    cross_val_score,
    train_test_split,
    GridSearchCV
)
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.calibration import CalibratedClassifierCV, calibration_curve
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    roc_auc_score, precision_recall_curve, auc,
    classification_report, confusion_matrix,
    precision_score, recall_score, f1_score
)
from sklearn.isotonic import IsotonicRegression

try:
    import xgboost as xgb
    HAS_XGBOOST = True
except ImportError:
    HAS_XGBOOST = False
    warnings.warn("XGBoost æœªå®‰è£ï¼Œå°‡ä½¿ç”¨ RandomForest ä½œç‚ºæ›¿ä»£")


class ExoplanetTrainer:
    """ç³»å¤–è¡Œæ˜Ÿåµæ¸¬æ¨¡å‹è¨“ç·´å™¨"""

    def __init__(
        self,
        model_type: str = "logistic",
        calibration_method: str = "isotonic",
        random_state: int = 42,
        verbose: bool = True
    ):
        """
        åˆå§‹åŒ–è¨“ç·´å™¨

        Parameters:
        -----------
        model_type : str
            æ¨¡å‹é¡å‹ ('logistic', 'xgboost', 'random_forest')
        calibration_method : str
            æ ¡æº–æ–¹æ³• ('isotonic', 'sigmoid')
        random_state : int
            éš¨æ©Ÿç¨®å­
        verbose : bool
            æ˜¯å¦è¼¸å‡ºè¨“ç·´è³‡è¨Š
        """
        self.model_type = model_type
        self.calibration_method = calibration_method
        self.random_state = random_state
        self.verbose = verbose

        self.model = None
        self.calibrated_model = None
        self.scaler = StandardScaler()
        self.feature_names = None
        self.training_metrics = {}

    def create_base_model(self) -> Any:
        """å»ºç«‹åŸºç¤æ¨¡å‹"""
        if self.model_type == "logistic":
            return LogisticRegression(
                max_iter=1000,
                random_state=self.random_state,
                class_weight='balanced'
            )
        elif self.model_type == "xgboost" and HAS_XGBOOST:
            return xgb.XGBClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                objective='binary:logistic',
                random_state=self.random_state,
                use_label_encoder=False,
                eval_metric='logloss'
            )
        else:  # random_forest or fallback
            return RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                random_state=self.random_state,
                class_weight='balanced'
            )

    def train(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: Optional[np.ndarray] = None,
        y_val: Optional[np.ndarray] = None,
        feature_names: Optional[List[str]] = None,
        cv_folds: int = 5
    ) -> Dict[str, Any]:
        """
        è¨“ç·´æ¨¡å‹ä¸¦é€²è¡Œæ ¡æº–

        Parameters:
        -----------
        X_train : np.ndarray
            è¨“ç·´ç‰¹å¾µ
        y_train : np.ndarray
            è¨“ç·´æ¨™ç±¤
        X_val : np.ndarray, optional
            é©—è­‰ç‰¹å¾µ
        y_val : np.ndarray, optional
            é©—è­‰æ¨™ç±¤
        feature_names : List[str], optional
            ç‰¹å¾µåç¨±
        cv_folds : int
            äº¤å‰é©—è­‰æŠ˜æ•¸

        Returns:
        --------
        metrics : Dict[str, Any]
            è¨“ç·´æŒ‡æ¨™
        """
        if self.verbose:
            print(f"ğŸš€ é–‹å§‹è¨“ç·´ {self.model_type} æ¨¡å‹...")
            print(f"   è¨“ç·´æ¨£æœ¬æ•¸: {len(X_train)}")
            print(f"   ç‰¹å¾µç¶­åº¦: {X_train.shape[1]}")
            print(f"   æ­£æ¨£æœ¬æ¯”ä¾‹: {y_train.mean():.2%}")

        # å„²å­˜ç‰¹å¾µåç¨±
        if feature_names is not None:
            self.feature_names = feature_names
        else:
            self.feature_names = [f"feature_{i}" for i in range(X_train.shape[1])]

        # ç‰¹å¾µæ¨™æº–åŒ–
        X_train_scaled = self.scaler.fit_transform(X_train)

        # å»ºç«‹åŸºç¤æ¨¡å‹
        base_model = self.create_base_model()

        # äº¤å‰é©—è­‰è©•ä¼°
        if self.verbose:
            print(f"\nğŸ“Š åŸ·è¡Œ {cv_folds} æŠ˜äº¤å‰é©—è­‰...")

        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.random_state)
        cv_scores = cross_val_score(base_model, X_train_scaled, y_train, cv=cv, scoring='roc_auc')

        if self.verbose:
            print(f"   CV ROC-AUC: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")

        # è¨“ç·´åŸºç¤æ¨¡å‹
        base_model.fit(X_train_scaled, y_train)
        self.model = base_model

        # æ©Ÿç‡æ ¡æº–
        if self.verbose:
            print(f"\nğŸ¯ é€²è¡Œæ©Ÿç‡æ ¡æº– (æ–¹æ³•: {self.calibration_method})...")

        self.calibrated_model = CalibratedClassifierCV(
            base_model,
            method=self.calibration_method,
            cv=3
        )
        self.calibrated_model.fit(X_train_scaled, y_train)

        # è©•ä¼°æ¨¡å‹
        metrics = self._evaluate_model(X_train_scaled, y_train, "è¨“ç·´é›†")

        # å¦‚æœæä¾›é©—è­‰é›†
        if X_val is not None and y_val is not None:
            X_val_scaled = self.scaler.transform(X_val)
            val_metrics = self._evaluate_model(X_val_scaled, y_val, "é©—è­‰é›†")
            metrics.update({f"val_{k}": v for k, v in val_metrics.items()})

        self.training_metrics = metrics

        # è¨ˆç®—ç‰¹å¾µé‡è¦æ€§
        if hasattr(self.model, 'feature_importances_'):
            self.feature_importance = pd.DataFrame({
                'feature': self.feature_names,
                'importance': self.model.feature_importances_
            }).sort_values('importance', ascending=False)

            if self.verbose:
                print("\nğŸ† å‰ 10 å€‹é‡è¦ç‰¹å¾µ:")
                for idx, row in self.feature_importance.head(10).iterrows():
                    print(f"   {row['feature']:20} : {row['importance']:.4f}")

        return metrics

    def _evaluate_model(
        self,
        X: np.ndarray,
        y: np.ndarray,
        dataset_name: str
    ) -> Dict[str, float]:
        """è©•ä¼°æ¨¡å‹æ•ˆèƒ½"""
        # å–å¾—é æ¸¬
        y_pred = self.calibrated_model.predict(X)
        y_proba = self.calibrated_model.predict_proba(X)[:, 1]

        # è¨ˆç®—æŒ‡æ¨™
        roc_auc = roc_auc_score(y, y_proba)
        precision, recall, _ = precision_recall_curve(y, y_proba)
        pr_auc = auc(recall, precision)

        # åˆ†é¡æŒ‡æ¨™
        prec = precision_score(y, y_pred)
        rec = recall_score(y, y_pred)
        f1 = f1_score(y, y_pred)

        if self.verbose:
            print(f"\nğŸ“ˆ {dataset_name}æ•ˆèƒ½:")
            print(f"   ROC-AUC: {roc_auc:.4f}")
            print(f"   PR-AUC: {pr_auc:.4f}")
            print(f"   Precision: {prec:.4f}")
            print(f"   Recall: {rec:.4f}")
            print(f"   F1-Score: {f1:.4f}")

        return {
            'roc_auc': roc_auc,
            'pr_auc': pr_auc,
            'precision': prec,
            'recall': rec,
            'f1_score': f1
        }

    def predict(
        self,
        X: np.ndarray,
        return_proba: bool = True
    ) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:
        """
        é€²è¡Œé æ¸¬

        Parameters:
        -----------
        X : np.ndarray
            ç‰¹å¾µçŸ©é™£
        return_proba : bool
            æ˜¯å¦å›å‚³æ©Ÿç‡

        Returns:
        --------
        predictions : np.ndarray or Tuple
            é æ¸¬çµæœ
        """
        if self.calibrated_model is None:
            raise ValueError("æ¨¡å‹å°šæœªè¨“ç·´")

        X_scaled = self.scaler.transform(X)

        if return_proba:
            y_proba = self.calibrated_model.predict_proba(X_scaled)[:, 1]
            y_pred = (y_proba >= 0.5).astype(int)
            return y_pred, y_proba
        else:
            return self.calibrated_model.predict(X_scaled)

    def save(
        self,
        output_dir: str = "model",
        model_name: Optional[str] = None
    ) -> Dict[str, str]:
        """
        å„²å­˜æ¨¡å‹èˆ‡ç›¸é—œæª”æ¡ˆ

        Parameters:
        -----------
        output_dir : str
            è¼¸å‡ºç›®éŒ„
        model_name : str, optional
            æ¨¡å‹åç¨±

        Returns:
        --------
        paths : Dict[str, str]
            å„²å­˜çš„æª”æ¡ˆè·¯å¾‘
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # å»ºç«‹æª”æ¡ˆåç¨±
        if model_name is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_name = f"{self.model_type}_{timestamp}"

        paths = {}

        # å„²å­˜æ ¡æº–å¾Œçš„æ¨¡å‹
        model_path = output_path / f"{model_name}_calibrated.joblib"
        joblib.dump(self.calibrated_model, model_path)
        paths['calibrated_model'] = str(model_path)

        # å„²å­˜åŸºç¤æ¨¡å‹
        base_model_path = output_path / f"{model_name}_base.joblib"
        joblib.dump(self.model, base_model_path)
        paths['base_model'] = str(base_model_path)

        # å„²å­˜ scaler
        scaler_path = output_path / f"{model_name}_scaler.joblib"
        joblib.dump(self.scaler, scaler_path)
        paths['scaler'] = str(scaler_path)

        # å„²å­˜ç‰¹å¾µçµæ§‹
        feature_schema = {
            'feature_names': self.feature_names,
            'n_features': len(self.feature_names),
            'model_type': self.model_type,
            'calibration_method': self.calibration_method,
            'training_metrics': self.training_metrics,
            'timestamp': datetime.now().isoformat()
        }

        schema_path = output_path / f"{model_name}_schema.json"
        with open(schema_path, 'w', encoding='utf-8') as f:
            json.dump(feature_schema, f, indent=2, ensure_ascii=False)
        paths['schema'] = str(schema_path)

        # å„²å­˜ç‰¹å¾µé‡è¦æ€§
        if hasattr(self, 'feature_importance'):
            importance_path = output_path / f"{model_name}_importance.csv"
            self.feature_importance.to_csv(importance_path, index=False)
            paths['importance'] = str(importance_path)

        if self.verbose:
            print(f"\nğŸ’¾ æ¨¡å‹å·²å„²å­˜è‡³ {output_dir}/")
            for key, path in paths.items():
                print(f"   {key}: {Path(path).name}")

        return paths

    @classmethod
    def load(
        cls,
        model_path: str,
        verbose: bool = True
    ) -> 'ExoplanetTrainer':
        """
        è¼‰å…¥å·²å„²å­˜çš„æ¨¡å‹

        Parameters:
        -----------
        model_path : str
            æ¨¡å‹è·¯å¾‘ (å¯ä»¥æ˜¯ç›®éŒ„æˆ–å…·é«”æª”æ¡ˆ)
        verbose : bool
            æ˜¯å¦è¼¸å‡ºè³‡è¨Š

        Returns:
        --------
        trainer : ExoplanetTrainer
            è¼‰å…¥çš„è¨“ç·´å™¨å¯¦ä¾‹
        """
        model_path = Path(model_path)

        # å¦‚æœæ˜¯ç›®éŒ„ï¼Œå°‹æ‰¾æœ€æ–°çš„æ¨¡å‹
        if model_path.is_dir():
            calibrated_files = list(model_path.glob("*_calibrated.joblib"))
            if not calibrated_files:
                raise FileNotFoundError(f"åœ¨ {model_path} æ‰¾ä¸åˆ°æ ¡æº–æ¨¡å‹")
            model_file = max(calibrated_files, key=lambda x: x.stat().st_mtime)
            base_name = str(model_file).replace("_calibrated.joblib", "")
        else:
            base_name = str(model_path).replace("_calibrated.joblib", "")
            model_file = model_path

        # è¼‰å…¥å„å€‹å…ƒä»¶
        calibrated_model = joblib.load(f"{base_name}_calibrated.joblib")
        base_model = joblib.load(f"{base_name}_base.joblib")
        scaler = joblib.load(f"{base_name}_scaler.joblib")

        # è¼‰å…¥ schema
        with open(f"{base_name}_schema.json", 'r', encoding='utf-8') as f:
            schema = json.load(f)

        # å»ºç«‹è¨“ç·´å™¨å¯¦ä¾‹
        trainer = cls(
            model_type=schema['model_type'],
            calibration_method=schema['calibration_method'],
            verbose=verbose
        )

        trainer.calibrated_model = calibrated_model
        trainer.model = base_model
        trainer.scaler = scaler
        trainer.feature_names = schema['feature_names']
        trainer.training_metrics = schema.get('training_metrics', {})

        # è¼‰å…¥ç‰¹å¾µé‡è¦æ€§
        importance_file = Path(f"{base_name}_importance.csv")
        if importance_file.exists():
            trainer.feature_importance = pd.read_csv(importance_file)

        if verbose:
            print(f"âœ… æˆåŠŸè¼‰å…¥æ¨¡å‹")
            print(f"   æ¨¡å‹é¡å‹: {trainer.model_type}")
            print(f"   æ ¡æº–æ–¹æ³•: {trainer.calibration_method}")
            print(f"   ç‰¹å¾µæ•¸é‡: {len(trainer.feature_names)}")

        return trainer


def hyperparameter_tuning(
    X_train: np.ndarray,
    y_train: np.ndarray,
    model_type: str = "xgboost",
    cv_folds: int = 3,
    verbose: bool = True
) -> Dict[str, Any]:
    """
    è¶…åƒæ•¸èª¿å„ª

    Parameters:
    -----------
    X_train : np.ndarray
        è¨“ç·´ç‰¹å¾µ
    y_train : np.ndarray
        è¨“ç·´æ¨™ç±¤
    model_type : str
        æ¨¡å‹é¡å‹
    cv_folds : int
        äº¤å‰é©—è­‰æŠ˜æ•¸
    verbose : bool
        æ˜¯å¦è¼¸å‡ºè³‡è¨Š

    Returns:
    --------
    best_params : Dict[str, Any]
        æœ€ä½³åƒæ•¸
    """
    if verbose:
        print(f"ğŸ” é–‹å§‹ {model_type} è¶…åƒæ•¸æœå°‹...")

    # å®šç¾©åƒæ•¸ç¶²æ ¼
    if model_type == "logistic":
        param_grid = {
            'C': [0.01, 0.1, 1, 10],
            'penalty': ['l1', 'l2'],
            'solver': ['liblinear', 'saga']
        }
        base_model = LogisticRegression(max_iter=1000, random_state=42)

    elif model_type == "xgboost" and HAS_XGBOOST:
        param_grid = {
            'n_estimators': [50, 100, 200],
            'max_depth': [3, 6, 10],
            'learning_rate': [0.01, 0.1, 0.3],
            'subsample': [0.8, 1.0]
        }
        base_model = xgb.XGBClassifier(
            use_label_encoder=False,
            eval_metric='logloss',
            random_state=42
        )

    else:  # random_forest
        param_grid = {
            'n_estimators': [50, 100, 200],
            'max_depth': [5, 10, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        }
        base_model = RandomForestClassifier(random_state=42)

    # åŸ·è¡Œç¶²æ ¼æœå°‹
    grid_search = GridSearchCV(
        base_model,
        param_grid,
        cv=cv_folds,
        scoring='roc_auc',
        n_jobs=-1,
        verbose=1 if verbose else 0
    )

    grid_search.fit(X_train, y_train)

    if verbose:
        print(f"\nâœ¨ æœ€ä½³åƒæ•¸:")
        for param, value in grid_search.best_params_.items():
            print(f"   {param}: {value}")
        print(f"\n   æœ€ä½³ CV åˆ†æ•¸: {grid_search.best_score_:.4f}")

    return {
        'best_params': grid_search.best_params_,
        'best_score': grid_search.best_score_,
        'grid_search': grid_search
    }


# ä¿ç•™åŸå§‹ç°¡å–®ä»‹é¢çš„ç›¸å®¹æ€§
def train_and_save(
    X: np.ndarray,
    y: np.ndarray,
    out_dir: str = "model"
) -> None:
    """
    ç°¡å–®çš„è¨“ç·´èˆ‡å„²å­˜ä»‹é¢ï¼ˆå‘å¾Œç›¸å®¹ï¼‰
    """
    trainer = ExoplanetTrainer(model_type="logistic", verbose=True)
    trainer.train(X, y)
    trainer.save(out_dir, model_name="ranker")


if __name__ == "__main__":
    # æ¸¬è©¦ç¯„ä¾‹
    print("ğŸ§ª è¨“ç·´æ¨¡çµ„æ¸¬è©¦")

    # ç”¢ç”Ÿæ¸¬è©¦è³‡æ–™
    np.random.seed(42)
    X_test = np.random.randn(1000, 14)
    y_test = np.random.randint(0, 2, 1000)

    # æ¸¬è©¦è¨“ç·´æµç¨‹
    trainer = ExoplanetTrainer(model_type="logistic")
    metrics = trainer.train(X_test, y_test, cv_folds=3)

    # å„²å­˜æ¨¡å‹
    paths = trainer.save("test_model")

    # è¼‰å…¥æ¨¡å‹æ¸¬è©¦
    loaded_trainer = ExoplanetTrainer.load("test_model")
    predictions, probas = loaded_trainer.predict(X_test[:10])

    print(f"\nâœ… æ¸¬è©¦å®Œæˆï¼é æ¸¬æ©Ÿç‡: {probas}")