# GitHub Release è³‡æ–™ä¸Šå‚³æŒ‡å—

**ç›®æ¨™**: å°‡ä¸‹è¼‰çš„å…‰æ›²ç·šè³‡æ–™ï¼ˆ~50 GBï¼‰åˆ‡åˆ†ä¸¦ä¸Šå‚³åˆ° GitHub Releases
**é™åˆ¶**: å–®ä¸€è³‡ç”¢ â‰¤ 2 GiBï¼Œæ¯å€‹ release æœ€å¤š 1000 å€‹è³‡ç”¢
**å„ªå‹¢**: ä¸è¨ˆå…¥ LFS æˆ– repo å®¹é‡ï¼Œç¸½å®¹é‡èˆ‡é »å¯¬ä¸è¨­ä¸Šé™

---

## ğŸ“‹ åŸ·è¡Œæµç¨‹ç¸½è¦½

```
[1] åŸ·è¡Œä¸‹è¼‰ â†’ [2] åˆ‡åˆ†è³‡æ–™ â†’ [3] å‰µå»º Release â†’ [4] ä¸Šå‚³è³‡ç”¢ â†’ [5] é©—è­‰
â†“
[6] ä¸‹è¼‰è³‡æ–™ â†’ [7] æå–ç‰¹å¾µ â†’ [8] è¨“ç·´æ¨¡å‹ â†’ [9] è©•ä¼°èˆ‡éƒ¨ç½²
```

**ä¸€æ¢é¾å®Œæ•´æµç¨‹**ï¼šå¾ä¸‹è¼‰åˆ°æ¨¡å‹è¨“ç·´ï¼Œå®Œå…¨è‡ªå‹•åŒ–

---

## 1ï¸âƒ£ åŸ·è¡Œå…¨é‡ä¸‹è¼‰

### æ­¥é©Ÿ 1.1: Clone ä¸¦æ›´æ–°å€‰åº«

```bash
git clone https://github.com/exoplanet-spaceapps/exoplanet-starter.git
cd exoplanet-starter
git pull origin main
```

### æ­¥é©Ÿ 1.2: å®‰è£ä¾è³´

```bash
pip install lightkurve h5py pandas numpy tqdm pyarrow
```

### æ­¥é©Ÿ 1.3: åŸ·è¡Œä¸‹è¼‰è…³æœ¬

```bash
python scripts/run_test_fixed.py
```

**é ä¼°æ™‚é–“**: 6-7 å°æ™‚
**é ä¼°æˆåŠŸ**: ~6,800 å€‹æ¨£æœ¬ï¼ˆ57% æˆåŠŸç‡ï¼‰
**è¼¸å‡ºç›®éŒ„**: `data/lightcurves/` (ç´„ 5-10 GB)

**æ³¨æ„**:
- ä½¿ç”¨ checkpoint ç³»çµ±ï¼Œå¯éš¨æ™‚ä¸­æ–·ä¸¦æ¢å¾©
- ä¸‹è¼‰é€²åº¦ä¿å­˜åœ¨ `checkpoints/download_progress.parquet`

---

## 2ï¸âƒ£ åˆ‡åˆ†è³‡æ–™ï¼ˆâ‰¤ 2 GiBï¼‰

### æ­¥é©Ÿ 2.1: å‰µå»ºåˆ‡åˆ†è…³æœ¬

å‰µå»ºæª”æ¡ˆ `scripts/split_for_release.py`:

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""Split downloaded lightcurves into â‰¤ 2 GiB archives for GitHub Releases"""

import os
import tarfile
from pathlib import Path
from datetime import datetime

# é…ç½®
PROJECT_ROOT = Path(__file__).parent.parent
LIGHTCURVE_DIR = PROJECT_ROOT / 'data' / 'lightcurves'
OUTPUT_DIR = PROJECT_ROOT / 'releases'
MAX_SIZE_GB = 1.9  # ç•™é»ç©ºé–“ï¼Œå®‰å…¨èµ·è¦‹ç”¨ 1.9 GiB
MAX_SIZE_BYTES = int(MAX_SIZE_GB * 1024 * 1024 * 1024)

OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

print("="*70)
print("Splitting Lightcurves for GitHub Releases")
print("="*70)

# ç²å–æ‰€æœ‰ HDF5 æª”æ¡ˆ
h5_files = sorted(LIGHTCURVE_DIR.glob('*.h5'))
print(f"\n[1/3] Found {len(h5_files)} HDF5 files")

if len(h5_files) == 0:
    print("  ERROR: No HDF5 files found in data/lightcurves/")
    exit(1)

# è¨ˆç®—ç¸½å¤§å°
total_size = sum(f.stat().st_size for f in h5_files)
print(f"  Total size: {total_size / 1024 / 1024 / 1024:.2f} GB")
print(f"  Max archive size: {MAX_SIZE_GB} GB")

# ä¼°ç®—éœ€è¦çš„æª”æ¡ˆæ•¸
estimated_archives = int(total_size / MAX_SIZE_BYTES) + 1
print(f"  Estimated archives: {estimated_archives}")

# é–‹å§‹åˆ‡åˆ†
print(f"\n[2/3] Creating archives...")
archive_num = 1
current_size = 0
current_files = []
created_archives = []

for h5_file in h5_files:
    file_size = h5_file.stat().st_size

    # æª¢æŸ¥æ˜¯å¦éœ€è¦å‰µå»ºæ–°çš„ archive
    if current_size + file_size > MAX_SIZE_BYTES and len(current_files) > 0:
        # å‰µå»ºç•¶å‰ archive
        archive_name = f"lightcurves_part{archive_num:03d}.tar.gz"
        archive_path = OUTPUT_DIR / archive_name

        print(f"  Creating {archive_name} ({len(current_files)} files, {current_size/1024/1024/1024:.2f} GB)...")

        with tarfile.open(archive_path, 'w:gz') as tar:
            for f in current_files:
                arcname = f.relative_to(LIGHTCURVE_DIR)
                tar.add(f, arcname=arcname)

        created_archives.append({
            'name': archive_name,
            'path': archive_path,
            'files': len(current_files),
            'size': current_size
        })

        # é‡ç½®
        archive_num += 1
        current_files = []
        current_size = 0

    current_files.append(h5_file)
    current_size += file_size

# è™•ç†æœ€å¾Œä¸€æ‰¹æª”æ¡ˆ
if len(current_files) > 0:
    archive_name = f"lightcurves_part{archive_num:03d}.tar.gz"
    archive_path = OUTPUT_DIR / archive_name

    print(f"  Creating {archive_name} ({len(current_files)} files, {current_size/1024/1024/1024:.2f} GB)...")

    with tarfile.open(archive_path, 'w:gz') as tar:
        for f in current_files:
            arcname = f.relative_to(LIGHTCURVE_DIR)
            tar.add(f, arcname=arcname)

    created_archives.append({
        'name': archive_name,
        'path': archive_path,
        'files': len(current_files),
        'size': current_size
    })

# çµ±è¨ˆ
print(f"\n[3/3] Summary")
print(f"  Created {len(created_archives)} archives:")
for i, archive in enumerate(created_archives, 1):
    size_gb = archive['size'] / 1024 / 1024 / 1024
    archive_size = archive['path'].stat().st_size / 1024 / 1024 / 1024
    print(f"    {i}. {archive['name']}: {archive['files']} files, {archive_size:.2f} GB compressed")

total_compressed = sum(a['path'].stat().st_size for a in created_archives) / 1024 / 1024 / 1024
print(f"\n  Total compressed size: {total_compressed:.2f} GB")
print(f"  Compression ratio: {(1 - total_compressed / (total_size/1024/1024/1024)) * 100:.1f}%")
print(f"\n  Output directory: {OUTPUT_DIR}")

# å‰µå»ºæ¸…å–®æª”æ¡ˆ
manifest_path = OUTPUT_DIR / 'manifest.txt'
with open(manifest_path, 'w') as f:
    f.write(f"# Exoplanet Lightcurves Archive Manifest\n")
    f.write(f"# Generated: {datetime.now().isoformat()}\n")
    f.write(f"# Total archives: {len(created_archives)}\n")
    f.write(f"# Total size: {total_compressed:.2f} GB\n\n")

    for archive in created_archives:
        size_gb = archive['path'].stat().st_size / 1024 / 1024 / 1024
        f.write(f"{archive['name']}\t{archive['files']} files\t{size_gb:.2f} GB\n")

print(f"  Manifest saved: {manifest_path}")

print("="*70)
print("âœ… Archive creation complete!")
print("\nNext steps:")
print("  1. Create GitHub Release: gh release create v1.0-lightcurves --title 'Lightcurve Data v1.0'")
print("  2. Upload archives: gh release upload v1.0-lightcurves releases/*.tar.gz")
print("="*70)
```

### æ­¥é©Ÿ 2.2: åŸ·è¡Œåˆ‡åˆ†

```bash
python scripts/split_for_release.py
```

**è¼¸å‡º**: `releases/lightcurves_part001.tar.gz`, `part002.tar.gz`, ...
**æª”æ¡ˆæ•¸**: ç´„ 3-6 å€‹ï¼ˆå–æ±ºæ–¼å¯¦éš›ä¸‹è¼‰å¤§å°ï¼‰

---

## 3ï¸âƒ£ å‰µå»º GitHub Release

### æ­¥é©Ÿ 3.1: å®‰è£ GitHub CLIï¼ˆå¦‚æœªå®‰è£ï¼‰

**Windows**:
```bash
winget install GitHub.cli
```

**macOS**:
```bash
brew install gh
```

**Linux**:
```bash
sudo apt install gh
```

### æ­¥é©Ÿ 3.2: ç™»å…¥ GitHub

```bash
gh auth login
```

é¸æ“‡ï¼š
- GitHub.com
- HTTPS
- Login with a web browser

### æ­¥é©Ÿ 3.3: å‰µå»º Release

```bash
gh release create v1.0-lightcurves \
  --title "TESS Lightcurve Data v1.0" \
  --notes "Complete dataset of TESS light curves for exoplanet detection.

**Contents:**
- ~6,800 TESS light curve files (HDF5 format)
- Downloaded from MAST Archive
- Total size: ~50 GB (compressed)
- Split into parts of â‰¤ 2 GiB for upload

**Usage:**
See [Download Guide](https://github.com/exoplanet-spaceapps/exoplanet-starter/blob/main/docs/DOWNLOAD_FROM_RELEASE.md) for instructions on downloading and extracting.

**Dataset Info:**
- Source: NASA TESS Mission
- Targets: TOI candidates + False Positives
- Format: HDF5 (multi-sector support)
- Features: time, flux, flux_err per sector"
```

---

## 4ï¸âƒ£ ä¸Šå‚³è³‡ç”¢åˆ° Release

### æ–¹å¼ A: æ‰¹æ¬¡ä¸Šå‚³æ‰€æœ‰æª”æ¡ˆï¼ˆæ¨è–¦ï¼‰

```bash
gh release upload v1.0-lightcurves releases/*.tar.gz --clobber
```

### æ–¹å¼ B: é€ä¸€ä¸Šå‚³ï¼ˆæœ‰é€²åº¦é¡¯ç¤ºï¼‰

```bash
# ä½¿ç”¨ PowerShellï¼ˆWindowsï¼‰
Get-ChildItem releases\*.tar.gz | ForEach-Object {
    Write-Host "Uploading $($_.Name)..."
    gh release upload v1.0-lightcurves $_.FullName
}
```

```bash
# ä½¿ç”¨ Bashï¼ˆLinux/macOSï¼‰
for file in releases/*.tar.gz; do
    echo "Uploading $(basename $file)..."
    gh release upload v1.0-lightcurves "$file"
done
```

### æ–¹å¼ C: ä½¿ç”¨ Python è…³æœ¬ä¸Šå‚³ï¼ˆå¸¶é€²åº¦æ¢ï¼‰

å‰µå»º `scripts/upload_to_release.py`:

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""Upload archives to GitHub Release with progress tracking"""

import subprocess
from pathlib import Path
from tqdm import tqdm

PROJECT_ROOT = Path(__file__).parent.parent
RELEASES_DIR = PROJECT_ROOT / 'releases'
RELEASE_TAG = 'v1.0-lightcurves'

archives = sorted(RELEASES_DIR.glob('*.tar.gz'))

print(f"Found {len(archives)} archives to upload")
print(f"Target release: {RELEASE_TAG}\n")

for archive in tqdm(archives, desc="Uploading"):
    cmd = ['gh', 'release', 'upload', RELEASE_TAG, str(archive), '--clobber']
    result = subprocess.run(cmd, capture_output=True, text=True)

    if result.returncode != 0:
        print(f"\nâŒ Failed to upload {archive.name}")
        print(f"   Error: {result.stderr}")
    else:
        tqdm.write(f"âœ… {archive.name}")

print("\nâœ… Upload complete!")
```

åŸ·è¡Œï¼š
```bash
python scripts/upload_to_release.py
```

---

## 5ï¸âƒ£ é©—è­‰ä¸Šå‚³

### æ­¥é©Ÿ 5.1: æª¢æŸ¥ Release è³‡ç”¢

```bash
gh release view v1.0-lightcurves
```

### æ­¥é©Ÿ 5.2: é©—è­‰æª”æ¡ˆå®Œæ•´æ€§

å‰µå»º `scripts/verify_release.py`:

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""Verify uploaded releases match local files"""

import subprocess
import json
from pathlib import Path

PROJECT_ROOT = Path(__file__).parent.parent
RELEASES_DIR = PROJECT_ROOT / 'releases'
RELEASE_TAG = 'v1.0-lightcurves'

# ç²å– release è³‡è¨Š
cmd = ['gh', 'release', 'view', RELEASE_TAG, '--json', 'assets']
result = subprocess.run(cmd, capture_output=True, text=True)
release_data = json.loads(result.stdout)

remote_assets = {asset['name']: asset['size'] for asset in release_data['assets']}
local_files = {f.name: f.stat().st_size for f in RELEASES_DIR.glob('*.tar.gz')}

print("="*70)
print("Verification Report")
print("="*70)

print(f"\nLocal files: {len(local_files)}")
print(f"Remote assets: {len(remote_assets)}")

missing_remote = set(local_files.keys()) - set(remote_assets.keys())
missing_local = set(remote_assets.keys()) - set(local_files.keys())

if missing_remote:
    print(f"\nâŒ Missing from remote: {missing_remote}")
else:
    print("\nâœ… All local files uploaded")

if missing_local:
    print(f"\nâš ï¸ Extra files on remote: {missing_local}")

# æª¢æŸ¥æª”æ¡ˆå¤§å°
size_mismatches = []
for name in set(local_files.keys()) & set(remote_assets.keys()):
    if local_files[name] != remote_assets[name]:
        size_mismatches.append(name)
        print(f"\nâš ï¸ Size mismatch: {name}")
        print(f"   Local: {local_files[name]:,} bytes")
        print(f"   Remote: {remote_assets[name]:,} bytes")

if not size_mismatches and not missing_remote:
    print("\nâœ… All files verified successfully!")
else:
    print(f"\nâš ï¸ Found {len(size_mismatches)} size mismatches")

print("="*70)
```

åŸ·è¡Œï¼š
```bash
python scripts/verify_release.py
```

---

## 6ï¸âƒ£ ä¸‹è¼‰è³‡æ–™ï¼ˆå…¶ä»–è£ç½®ä½¿ç”¨ï¼‰

### å¿«é€Ÿä¸‹è¼‰è…³æœ¬

å‰µå»º `scripts/download_from_release.py`:

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""Download and extract lightcurves from GitHub Release"""

import subprocess
import tarfile
from pathlib import Path
from tqdm import tqdm

PROJECT_ROOT = Path(__file__).parent.parent
LIGHTCURVE_DIR = PROJECT_ROOT / 'data' / 'lightcurves'
RELEASE_TAG = 'v1.0-lightcurves'

LIGHTCURVE_DIR.mkdir(parents=True, exist_ok=True)

print("="*70)
print("Downloading Lightcurves from GitHub Release")
print("="*70)

# ç²å– release è³‡ç”¢åˆ—è¡¨
cmd = ['gh', 'release', 'view', RELEASE_TAG, '--json', 'assets', '-q', '.assets[].name']
result = subprocess.run(cmd, capture_output=True, text=True)
assets = [name for name in result.stdout.strip().split('\n') if name.endswith('.tar.gz')]

print(f"\nFound {len(assets)} archives")

# ä¸‹è¼‰ä¸¦è§£å£“
for asset in tqdm(assets, desc="Processing"):
    print(f"\n[{asset}]")

    # ä¸‹è¼‰
    print("  Downloading...")
    cmd = ['gh', 'release', 'download', RELEASE_TAG, '-p', asset]
    subprocess.run(cmd, check=True, cwd=PROJECT_ROOT)

    # è§£å£“
    print("  Extracting...")
    archive_path = PROJECT_ROOT / asset
    with tarfile.open(archive_path, 'r:gz') as tar:
        tar.extractall(path=LIGHTCURVE_DIR)

    # åˆªé™¤å£“ç¸®æª”
    archive_path.unlink()
    print("  âœ… Done")

print("\n" + "="*70)
print("âœ… Download complete!")
print(f"Files extracted to: {LIGHTCURVE_DIR}")
print("="*70)
```

ä½¿ç”¨ï¼š
```bash
gh auth login  # é¦–æ¬¡éœ€è¦ç™»å…¥
python scripts/download_from_release.py
```

---

## 7ï¸âƒ£ æ¸…ç†æœ¬åœ°æª”æ¡ˆï¼ˆå¯é¸ï¼‰

ä¸Šå‚³å®Œæˆå¾Œï¼Œå¯åˆªé™¤æœ¬åœ°çš„å£“ç¸®æª”ä»¥ç¯€çœç©ºé–“ï¼š

```bash
# åƒ…åˆªé™¤å£“ç¸®æª”ï¼Œä¿ç•™åŸå§‹è³‡æ–™
rm -rf releases/

# æˆ–å®Œå…¨æ¸…ç†ï¼ˆä¸‹è¼‰çš„å…‰æ›²ç·šä¹Ÿåˆªé™¤ï¼‰
rm -rf data/lightcurves/
rm -rf releases/
```

---

## ğŸ“‹ å®Œæ•´å·¥ä½œæµç¨‹ç¸½çµ

```bash
# === åœ¨ä¸‹è¼‰è£ç½®åŸ·è¡Œ ===

# 1. Clone å€‰åº«
git clone https://github.com/exoplanet-spaceapps/exoplanet-starter.git
cd exoplanet-starter

# 2. ä¸‹è¼‰è³‡æ–™ï¼ˆ6-7 å°æ™‚ï¼‰
python scripts/run_test_fixed.py

# 3. åˆ‡åˆ†è³‡æ–™
python scripts/split_for_release.py

# 4. å‰µå»º Release
gh release create v1.0-lightcurves --title "TESS Lightcurve Data v1.0" \
  --notes "Complete TESS light curve dataset (~50 GB)"

# 5. ä¸Šå‚³è³‡ç”¢
python scripts/upload_to_release.py  # æˆ–ä½¿ç”¨ gh release upload

# 6. é©—è­‰
python scripts/verify_release.py

# === åœ¨å…¶ä»–è£ç½®ä½¿ç”¨ ===

# ä¸‹è¼‰è³‡æ–™
git clone https://github.com/exoplanet-spaceapps/exoplanet-starter.git
cd exoplanet-starter
python scripts/download_from_release.py
```

---

## ğŸ”§ æ•…éšœæ’é™¤

### å•é¡Œ 1: gh å‘½ä»¤æ‰¾ä¸åˆ°

```bash
# ç¢ºèªå®‰è£
gh --version

# é‡æ–°ç™»å…¥
gh auth logout
gh auth login
```

### å•é¡Œ 2: ä¸Šå‚³è¶…æ™‚

```bash
# å–®ç¨é‡è©¦å¤±æ•—çš„æª”æ¡ˆ
gh release upload v1.0-lightcurves releases/lightcurves_part001.tar.gz --clobber
```

### å•é¡Œ 3: è³‡ç”¢å·²å­˜åœ¨

```bash
# ä½¿ç”¨ --clobber è¦†è“‹
gh release upload v1.0-lightcurves releases/*.tar.gz --clobber
```

### å•é¡Œ 4: Release æ¬Šé™éŒ¯èª¤

ç¢ºèªä½ æ˜¯ repo çš„ collaborator æˆ– ownerï¼Œä¸¦ä¸” token æœ‰ `repo` æ¬Šé™ã€‚

---

## ğŸ“Š å®¹é‡è¦åŠƒ

| é …ç›® | å¤§å° | èªªæ˜ |
|------|------|------|
| åŸå§‹ä¸‹è¼‰ | ~10 GB | HDF5 æª”æ¡ˆæœªå£“ç¸® |
| å£“ç¸®å¾Œ | ~5-8 GB | tar.gz å£“ç¸® |
| å–®ä¸€è³‡ç”¢ | â‰¤ 2 GB | GitHub é™åˆ¶ |
| é ä¼°æª”æ¡ˆæ•¸ | 3-6 å€‹ | å–æ±ºæ–¼å¯¦éš›å¤§å° |
| GitHub é™åˆ¶ | 1000 è³‡ç”¢ | è¶³å¤ ä½¿ç”¨ |

---

## âœ… æª¢æŸ¥æ¸…å–®

ä¸‹è¼‰è£ç½®ï¼š
- [ ] åŸ·è¡Œ `run_test_fixed.py` å®Œæˆä¸‹è¼‰
- [ ] åŸ·è¡Œ `split_for_release.py` åˆ‡åˆ†è³‡æ–™
- [ ] å‰µå»º GitHub Release
- [ ] ä¸Šå‚³æ‰€æœ‰è³‡ç”¢
- [ ] é©—è­‰ä¸Šå‚³å®Œæ•´æ€§
- [ ] ï¼ˆå¯é¸ï¼‰åˆªé™¤æœ¬åœ°å£“ç¸®æª”

å…¶ä»–è£ç½®ï¼š
- [ ] Clone å€‰åº«
- [ ] åŸ·è¡Œ `download_from_release.py`
- [ ] é©—è­‰æª”æ¡ˆå®Œæ•´æ€§
- [ ] ç¹¼çºŒå¾ŒçºŒåˆ†ææµç¨‹

---

## 8ï¸âƒ£ æå–ç‰¹å¾µï¼ˆBLS Feature Extractionï¼‰

ä¸‹è¼‰è³‡æ–™å¾Œï¼Œæ¥ä¸‹ä¾†æå–ç”¨æ–¼æ©Ÿå™¨å­¸ç¿’çš„ç‰¹å¾µã€‚

### æ­¥é©Ÿ 8.1: å®‰è£é¡å¤–ä¾è³´

```bash
pip install astropy scipy scikit-learn
```

### æ­¥é©Ÿ 8.2: åŸ·è¡Œç‰¹å¾µæå–

```bash
python scripts/test_features.py
```

**é ä¼°æ™‚é–“**: 15-30 åˆ†é˜ï¼ˆ6,800 æ¨£æœ¬ï¼‰
**è¼¸å‡º**: `data/features.csv`ï¼ˆ14 å€‹ç‰¹å¾µ Ã— 6,800 æ¨£æœ¬ï¼‰

### æ­¥é©Ÿ 8.3: æŸ¥çœ‹ç‰¹å¾µçµ±è¨ˆ

å‰µå»º `scripts/view_features.py`:

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""View extracted features statistics"""

import pandas as pd
from pathlib import Path

PROJECT_ROOT = Path(__file__).parent.parent
FEATURES_PATH = PROJECT_ROOT / 'data' / 'test_features.csv'

if not FEATURES_PATH.exists():
    print(f"âŒ Features file not found: {FEATURES_PATH}")
    print("Run: python scripts/test_features.py")
    exit(1)

df = pd.read_csv(FEATURES_PATH)

print("="*70)
print("Feature Extraction Summary")
print("="*70)

print(f"\nTotal samples: {len(df)}")
print(f"Total features: {len(df.columns)}")

# ç‹€æ…‹çµ±è¨ˆ
if 'status' in df.columns:
    print(f"\nExtraction status:")
    for status, count in df['status'].value_counts().items():
        print(f"  {status}: {count}")

# æ¨™ç±¤åˆ†å¸ƒ
if 'label' in df.columns:
    print(f"\nLabel distribution:")
    print(f"  Positive (exoplanet): {df['label'].sum()}")
    print(f"  Negative (no planet): {(~df['label'].astype(bool)).sum()}")

# BLS ç‰¹å¾µçµ±è¨ˆ
successful = df[df['status'] == 'success']
if len(successful) > 0:
    print(f"\nBLS Features (successful extractions):")
    bls_cols = ['bls_period', 'bls_duration', 'bls_depth', 'bls_power', 'bls_snr']
    for col in bls_cols:
        if col in successful.columns:
            print(f"  {col}:")
            print(f"    Mean: {successful[col].mean():.4f}")
            print(f"    Std:  {successful[col].std():.4f}")
            print(f"    Min:  {successful[col].min():.4f}")
            print(f"    Max:  {successful[col].max():.4f}")

# æª¢æŸ¥ç¼ºå¤±å€¼
print(f"\nMissing values:")
null_counts = df.isnull().sum()
if null_counts.sum() == 0:
    print("  âœ… No missing values")
else:
    for col, count in null_counts[null_counts > 0].items():
        print(f"  {col}: {count}")

print("="*70)
```

åŸ·è¡Œï¼š
```bash
python scripts/view_features.py
```

---

## 9ï¸âƒ£ è¨“ç·´æ¨¡å‹ï¼ˆXGBoost/LightGBMï¼‰

### æ­¥é©Ÿ 9.1: å‰µå»ºè¨“ç·´è…³æœ¬

å‰µå»º `scripts/train_model.py`:

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""Train exoplanet detection model using XGBoost"""

import json
import warnings
from pathlib import Path
from datetime import datetime

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report
)
import joblib

warnings.filterwarnings('ignore')

# é…ç½®
PROJECT_ROOT = Path(__file__).parent.parent
FEATURES_PATH = PROJECT_ROOT / 'data' / 'test_features.csv'
MODEL_DIR = PROJECT_ROOT / 'models'
RESULTS_DIR = PROJECT_ROOT / 'results'

MODEL_DIR.mkdir(parents=True, exist_ok=True)
RESULTS_DIR.mkdir(parents=True, exist_ok=True)

print("="*70)
print("Exoplanet Detection Model Training")
print("="*70)

# è¼‰å…¥ç‰¹å¾µ
print("\n[1/6] Loading features...")
if not FEATURES_PATH.exists():
    print(f"âŒ Features file not found: {FEATURES_PATH}")
    print("Run: python scripts/test_features.py")
    exit(1)

df = pd.read_csv(FEATURES_PATH)
print(f"  Total samples: {len(df)}")

# ç¯©é¸æˆåŠŸæå–çš„æ¨£æœ¬
successful = df[df['status'] == 'success'].copy()
print(f"  Successful extractions: {len(successful)}")

if len(successful) < 100:
    print("âŒ Not enough successful samples for training (minimum 100)")
    exit(1)

# æº–å‚™ç‰¹å¾µèˆ‡æ¨™ç±¤
print("\n[2/6] Preparing features...")
feature_cols = [
    'flux_mean', 'flux_std', 'flux_median', 'flux_mad', 'flux_skew', 'flux_kurt',
    'bls_period', 'bls_duration', 'bls_depth', 'bls_power', 'bls_snr'
]

X = successful[feature_cols].values
y = successful['label'].values

print(f"  Features: {X.shape}")
print(f"  Positive samples: {y.sum()} ({y.sum()/len(y)*100:.1f}%)")
print(f"  Negative samples: {len(y)-y.sum()} ({(len(y)-y.sum())/len(y)*100:.1f}%)")

# åˆ†å‰²è³‡æ–™
print("\n[3/6] Splitting data...")
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"  Train: {X_train.shape[0]} samples")
print(f"  Test:  {X_test.shape[0]} samples")

# è¨“ç·´æ¨¡å‹ï¼ˆå…ˆè©¦ XGBoostï¼Œfallback åˆ° RandomForestï¼‰
print("\n[4/6] Training model...")
try:
    import xgboost as xgb

    model = xgb.XGBClassifier(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        eval_metric='logloss'
    )
    model_type = 'XGBoost'
    print(f"  Using {model_type}")

except ImportError:
    print("  XGBoost not found, using RandomForest instead")
    from sklearn.ensemble import RandomForestClassifier

    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        random_state=42,
        n_jobs=-1
    )
    model_type = 'RandomForest'

model.fit(X_train, y_train)
print("  âœ… Training complete")

# äº¤å‰é©—è­‰
print("\n[5/6] Cross-validation...")
cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')
print(f"  CV ROC-AUC: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")

# è©•ä¼°
print("\n[6/6] Evaluation...")
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]

metrics = {
    'model_type': model_type,
    'n_samples': len(successful),
    'n_features': len(feature_cols),
    'train_size': len(X_train),
    'test_size': len(X_test),
    'accuracy': float(accuracy_score(y_test, y_pred)),
    'precision': float(precision_score(y_test, y_pred)),
    'recall': float(recall_score(y_test, y_pred)),
    'f1_score': float(f1_score(y_test, y_pred)),
    'roc_auc': float(roc_auc_score(y_test, y_pred_proba)),
    'cv_roc_auc_mean': float(cv_scores.mean()),
    'cv_roc_auc_std': float(cv_scores.std()),
    'timestamp': datetime.now().isoformat()
}

print(f"\nTest Set Performance:")
print(f"  Accuracy:  {metrics['accuracy']:.4f}")
print(f"  Precision: {metrics['precision']:.4f}")
print(f"  Recall:    {metrics['recall']:.4f}")
print(f"  F1 Score:  {metrics['f1_score']:.4f}")
print(f"  ROC-AUC:   {metrics['roc_auc']:.4f}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print(f"\nConfusion Matrix:")
print(f"  TN: {cm[0,0]:4d}  FP: {cm[0,1]:4d}")
print(f"  FN: {cm[1,0]:4d}  TP: {cm[1,1]:4d}")

# Classification Report
print(f"\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=['No Planet', 'Exoplanet']))

# Feature Importance
if hasattr(model, 'feature_importances_'):
    importance = model.feature_importances_
    feature_importance = sorted(
        zip(feature_cols, importance),
        key=lambda x: x[1],
        reverse=True
    )

    print(f"\nFeature Importance:")
    for feat, imp in feature_importance[:5]:
        print(f"  {feat:20s}: {imp:.4f}")

    metrics['feature_importance'] = {
        feat: float(imp) for feat, imp in feature_importance
    }

# ä¿å­˜æ¨¡å‹
model_path = MODEL_DIR / f'exoplanet_model_{model_type.lower()}.pkl'
joblib.dump(model, model_path)
print(f"\nâœ… Model saved: {model_path}")

# ä¿å­˜è©•ä¼°çµæœ
results_path = RESULTS_DIR / 'training_results.json'
with open(results_path, 'w') as f:
    json.dump(metrics, f, indent=2)
print(f"âœ… Results saved: {results_path}")

# ä¿å­˜ç‰¹å¾µåç¨±
features_meta_path = MODEL_DIR / 'feature_names.json'
with open(features_meta_path, 'w') as f:
    json.dump({'features': feature_cols}, f, indent=2)
print(f"âœ… Feature metadata saved: {features_meta_path}")

print("="*70)

if metrics['roc_auc'] >= 0.80:
    print("ğŸ‰ Model training successful! (ROC-AUC â‰¥ 0.80)")
    print("\nNext steps:")
    print("  1. Review results: cat results/training_results.json")
    print("  2. Test inference: python scripts/predict.py")
    print("  3. Deploy to production")
else:
    print(f"âš ï¸ Model performance below target (ROC-AUC = {metrics['roc_auc']:.4f})")
    print("\nSuggestions:")
    print("  1. Collect more training data")
    print("  2. Feature engineering (add more BLS parameters)")
    print("  3. Hyperparameter tuning")

print("="*70)
```

### æ­¥é©Ÿ 9.2: å®‰è£è¨“ç·´ä¾è³´

```bash
pip install xgboost scikit-learn
# æˆ–ä½¿ç”¨ LightGBM: pip install lightgbm scikit-learn
```

### æ­¥é©Ÿ 9.3: åŸ·è¡Œè¨“ç·´

```bash
python scripts/train_model.py
```

**é ä¼°æ™‚é–“**: 5-15 åˆ†é˜
**è¼¸å‡º**:
- `models/exoplanet_model_xgboost.pkl` - è¨“ç·´å¥½çš„æ¨¡å‹
- `results/training_results.json` - è©•ä¼°æŒ‡æ¨™
- `models/feature_names.json` - ç‰¹å¾µå…ƒè³‡æ–™

### æ­¥é©Ÿ 9.4: å‰µå»ºæ¨è«–è…³æœ¬

å‰µå»º `scripts/predict.py`:

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""Inference script for exoplanet detection"""

import sys
import warnings
from pathlib import Path

import numpy as np
import pandas as pd
import joblib
import json

warnings.filterwarnings('ignore')

PROJECT_ROOT = Path(__file__).parent.parent
MODEL_PATH = PROJECT_ROOT / 'models' / 'exoplanet_model_xgboost.pkl'
FEATURES_META_PATH = PROJECT_ROOT / 'models' / 'feature_names.json'

print("="*70)
print("Exoplanet Detection Inference")
print("="*70)

# è¼‰å…¥æ¨¡å‹
if not MODEL_PATH.exists():
    print(f"âŒ Model not found: {MODEL_PATH}")
    print("Run: python scripts/train_model.py")
    exit(1)

model = joblib.load(MODEL_PATH)
print(f"âœ… Model loaded: {MODEL_PATH}")

# è¼‰å…¥ç‰¹å¾µåç¨±
with open(FEATURES_META_PATH, 'r') as f:
    meta = json.load(f)
    feature_names = meta['features']

print(f"âœ… Features: {len(feature_names)}")

# æ¸¬è©¦æ¨è«–ï¼ˆä½¿ç”¨æ¸¬è©¦è³‡æ–™ï¼‰
FEATURES_PATH = PROJECT_ROOT / 'data' / 'test_features.csv'
if FEATURES_PATH.exists():
    df = pd.read_csv(FEATURES_PATH)
    successful = df[df['status'] == 'success']

    if len(successful) > 0:
        print(f"\n[Testing on {len(successful)} samples]")

        X = successful[feature_names].values
        y_pred = model.predict(X)
        y_pred_proba = model.predict_proba(X)[:, 1]

        # çµ±è¨ˆ
        n_detected = y_pred.sum()
        high_confidence = (y_pred_proba > 0.8).sum()

        print(f"\nPredictions:")
        print(f"  Total samples: {len(y_pred)}")
        print(f"  Detected exoplanets: {n_detected} ({n_detected/len(y_pred)*100:.1f}%)")
        print(f"  High confidence (>0.8): {high_confidence}")

        # é¡¯ç¤ºå‰ 10 å€‹é«˜æ©Ÿç‡æ¨£æœ¬
        top_indices = np.argsort(y_pred_proba)[-10:][::-1]
        print(f"\nTop 10 candidates:")
        for i, idx in enumerate(top_indices, 1):
            tic_id = successful.iloc[idx]['tic_id']
            prob = y_pred_proba[idx]
            print(f"  {i}. TIC{int(tic_id):10d} - Probability: {prob:.4f}")

        # ä¿å­˜é æ¸¬çµæœ
        results_df = successful.copy()
        results_df['prediction'] = y_pred
        results_df['probability'] = y_pred_proba

        output_path = PROJECT_ROOT / 'results' / 'predictions.csv'
        results_df.to_csv(output_path, index=False)
        print(f"\nâœ… Predictions saved: {output_path}")

print("="*70)
```

### æ­¥é©Ÿ 9.5: æ¸¬è©¦æ¨è«–

```bash
python scripts/predict.py
```

---

## ğŸ”„ å®Œæ•´å·¥ä½œæµç¨‹ç¸½çµï¼ˆä¸€æ¢é¾ï¼‰

```bash
# === Phase 1: è³‡æ–™æº–å‚™èˆ‡ä¸Šå‚³ï¼ˆä¸‹è¼‰è£ç½®ï¼‰===

# 1. Clone å€‰åº«
git clone https://github.com/exoplanet-spaceapps/exoplanet-starter.git
cd exoplanet-starter

# 2. ä¸‹è¼‰è³‡æ–™ï¼ˆ6-7 å°æ™‚ï¼‰
python scripts/run_test_fixed.py

# 3. åˆ‡åˆ†è³‡æ–™
python scripts/split_for_release.py

# 4. å‰µå»º Release
gh release create v1.0-lightcurves --title "TESS Lightcurve Data v1.0" \
  --notes "Complete TESS light curve dataset (~50 GB)"

# 5. ä¸Šå‚³è³‡ç”¢
python scripts/upload_to_release.py

# 6. é©—è­‰
python scripts/verify_release.py

# === Phase 2: æ¨¡å‹è¨“ç·´ï¼ˆä»»ä½•è£ç½®ï¼‰===

# 7. ä¸‹è¼‰è³‡æ–™
git clone https://github.com/exoplanet-spaceapps/exoplanet-starter.git
cd exoplanet-starter
python scripts/download_from_release.py

# 8. æå–ç‰¹å¾µï¼ˆ15-30 åˆ†é˜ï¼‰
python scripts/test_features.py

# 9. æŸ¥çœ‹ç‰¹å¾µçµ±è¨ˆ
python scripts/view_features.py

# 10. è¨“ç·´æ¨¡å‹ï¼ˆ5-15 åˆ†é˜ï¼‰
python scripts/train_model.py

# 11. æ¸¬è©¦æ¨è«–
python scripts/predict.py

# === Phase 3: éƒ¨ç½²ï¼ˆå¯é¸ï¼‰===

# 12. ä¸Šå‚³æ¨¡å‹åˆ° Release
gh release upload v1.0-lightcurves models/exoplanet_model_xgboost.pkl

# 13. éƒ¨ç½²åˆ° API æœå‹™ï¼ˆæ ¹æ“šéœ€æ±‚ï¼‰
# python scripts/deploy_api.py
```

---

## ğŸ“Š å®Œæ•´æ™‚é–“ä¼°ç®—

| éšæ®µ | æ™‚é–“ | å‚™è¨» |
|------|------|------|
| è³‡æ–™ä¸‹è¼‰ | 6-7 å°æ™‚ | 11,979 æ¨£æœ¬ï¼Œ57% æˆåŠŸç‡ |
| è³‡æ–™åˆ‡åˆ† | 30-60 åˆ†é˜ | å£“ç¸®ç‚º tar.gz |
| Release ä¸Šå‚³ | 1-2 å°æ™‚ | å–æ±ºæ–¼ç¶²è·¯é€Ÿåº¦ |
| è³‡æ–™ä¸‹è¼‰ï¼ˆå…¶ä»–è£ç½®ï¼‰| 1-2 å°æ™‚ | å¾ GitHub Release |
| ç‰¹å¾µæå– | 15-30 åˆ†é˜ | BLS ç®—æ³• |
| æ¨¡å‹è¨“ç·´ | 5-15 åˆ†é˜ | XGBoost/RandomForest |
| **ç¸½è¨ˆ** | **9-13 å°æ™‚** | å®Œæ•´ä¸€æ¢é¾æµç¨‹ |

---

## âœ… æ“´å±•æª¢æŸ¥æ¸…å–®

**è³‡æ–™éšæ®µ**ï¼š
- [ ] åŸ·è¡Œ `run_test_fixed.py` å®Œæˆä¸‹è¼‰
- [ ] åŸ·è¡Œ `split_for_release.py` åˆ‡åˆ†è³‡æ–™
- [ ] å‰µå»º GitHub Release
- [ ] ä¸Šå‚³æ‰€æœ‰è³‡ç”¢
- [ ] é©—è­‰ä¸Šå‚³å®Œæ•´æ€§

**æ¨¡å‹éšæ®µ**ï¼š
- [ ] å¾ Release ä¸‹è¼‰è³‡æ–™
- [ ] åŸ·è¡Œ `test_features.py` æå–ç‰¹å¾µ
- [ ] åŸ·è¡Œ `view_features.py` æª¢æŸ¥ç‰¹å¾µå“è³ª
- [ ] åŸ·è¡Œ `train_model.py` è¨“ç·´æ¨¡å‹
- [ ] åŸ·è¡Œ `predict.py` æ¸¬è©¦æ¨è«–
- [ ] æª¢æŸ¥ ROC-AUC â‰¥ 0.80

**éƒ¨ç½²éšæ®µ**ï¼ˆå¯é¸ï¼‰ï¼š
- [ ] ä¸Šå‚³æ¨¡å‹åˆ° Release
- [ ] å»ºç«‹ API æœå‹™
- [ ] æ•´åˆå‰ç«¯ä»‹é¢
- [ ] æ’°å¯«ä½¿ç”¨æ–‡æª”

---

**Created**: 2025-10-03
**Updated**: 2025-10-03
**Status**: Complete end-to-end pipeline
**Estimated time**: 9-13 hours (full pipeline)
