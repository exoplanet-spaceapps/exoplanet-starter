# GitHub Release è³‡æ–™ä¸Šå‚³æŒ‡å—

**ç›®æ¨™**: å°‡ä¸‹è¼‰çš„å…‰æ›²ç·šè³‡æ–™ï¼ˆ~50 GBï¼‰åˆ‡åˆ†ä¸¦ä¸Šå‚³åˆ° GitHub Releases
**é™åˆ¶**: å–®ä¸€è³‡ç”¢ â‰¤ 2 GiBï¼Œæ¯å€‹ release æœ€å¤š 1000 å€‹è³‡ç”¢
**å„ªå‹¢**: ä¸è¨ˆå…¥ LFS æˆ– repo å®¹é‡ï¼Œç¸½å®¹é‡èˆ‡é »å¯¬ä¸è¨­ä¸Šé™

---

## ğŸ“‹ åŸ·è¡Œæµç¨‹ç¸½è¦½

```
[1] åŸ·è¡Œä¸‹è¼‰ â†’ [2] åˆ‡åˆ†è³‡æ–™ â†’ [3] å‰µå»º Release â†’ [4] ä¸Šå‚³è³‡ç”¢ â†’ [5] é©—è­‰
```

---

## 1ï¸âƒ£ åŸ·è¡Œå…¨é‡ä¸‹è¼‰

### æ­¥é©Ÿ 1.1: Clone ä¸¦æ›´æ–°å€‰åº«

```bash
git clone https://github.com/exoplanet-spaceapps/exoplanet-starter.git
cd exoplanet-starter
git pull origin main
```

### æ­¥é©Ÿ 1.2: å®‰è£ä¾è³´

```bash
pip install lightkurve h5py pandas numpy tqdm pyarrow
```

### æ­¥é©Ÿ 1.3: åŸ·è¡Œä¸‹è¼‰è…³æœ¬

```bash
python scripts/run_test_fixed.py
```

**é ä¼°æ™‚é–“**: 6-7 å°æ™‚
**é ä¼°æˆåŠŸ**: ~6,800 å€‹æ¨£æœ¬ï¼ˆ57% æˆåŠŸç‡ï¼‰
**è¼¸å‡ºç›®éŒ„**: `data/lightcurves/` (ç´„ 5-10 GB)

**æ³¨æ„**:
- ä½¿ç”¨ checkpoint ç³»çµ±ï¼Œå¯éš¨æ™‚ä¸­æ–·ä¸¦æ¢å¾©
- ä¸‹è¼‰é€²åº¦ä¿å­˜åœ¨ `checkpoints/download_progress.parquet`

---

## 2ï¸âƒ£ åˆ‡åˆ†è³‡æ–™ï¼ˆâ‰¤ 2 GiBï¼‰

### æ­¥é©Ÿ 2.1: å‰µå»ºåˆ‡åˆ†è…³æœ¬

å‰µå»ºæª”æ¡ˆ `scripts/split_for_release.py`:

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""Split downloaded lightcurves into â‰¤ 2 GiB archives for GitHub Releases"""

import os
import tarfile
from pathlib import Path
from datetime import datetime

# é…ç½®
PROJECT_ROOT = Path(__file__).parent.parent
LIGHTCURVE_DIR = PROJECT_ROOT / 'data' / 'lightcurves'
OUTPUT_DIR = PROJECT_ROOT / 'releases'
MAX_SIZE_GB = 1.9  # ç•™é»ç©ºé–“ï¼Œå®‰å…¨èµ·è¦‹ç”¨ 1.9 GiB
MAX_SIZE_BYTES = int(MAX_SIZE_GB * 1024 * 1024 * 1024)

OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

print("="*70)
print("Splitting Lightcurves for GitHub Releases")
print("="*70)

# ç²å–æ‰€æœ‰ HDF5 æª”æ¡ˆ
h5_files = sorted(LIGHTCURVE_DIR.glob('*.h5'))
print(f"\n[1/3] Found {len(h5_files)} HDF5 files")

if len(h5_files) == 0:
    print("  ERROR: No HDF5 files found in data/lightcurves/")
    exit(1)

# è¨ˆç®—ç¸½å¤§å°
total_size = sum(f.stat().st_size for f in h5_files)
print(f"  Total size: {total_size / 1024 / 1024 / 1024:.2f} GB")
print(f"  Max archive size: {MAX_SIZE_GB} GB")

# ä¼°ç®—éœ€è¦çš„æª”æ¡ˆæ•¸
estimated_archives = int(total_size / MAX_SIZE_BYTES) + 1
print(f"  Estimated archives: {estimated_archives}")

# é–‹å§‹åˆ‡åˆ†
print(f"\n[2/3] Creating archives...")
archive_num = 1
current_size = 0
current_files = []
created_archives = []

for h5_file in h5_files:
    file_size = h5_file.stat().st_size

    # æª¢æŸ¥æ˜¯å¦éœ€è¦å‰µå»ºæ–°çš„ archive
    if current_size + file_size > MAX_SIZE_BYTES and len(current_files) > 0:
        # å‰µå»ºç•¶å‰ archive
        archive_name = f"lightcurves_part{archive_num:03d}.tar.gz"
        archive_path = OUTPUT_DIR / archive_name

        print(f"  Creating {archive_name} ({len(current_files)} files, {current_size/1024/1024/1024:.2f} GB)...")

        with tarfile.open(archive_path, 'w:gz') as tar:
            for f in current_files:
                arcname = f.relative_to(LIGHTCURVE_DIR)
                tar.add(f, arcname=arcname)

        created_archives.append({
            'name': archive_name,
            'path': archive_path,
            'files': len(current_files),
            'size': current_size
        })

        # é‡ç½®
        archive_num += 1
        current_files = []
        current_size = 0

    current_files.append(h5_file)
    current_size += file_size

# è™•ç†æœ€å¾Œä¸€æ‰¹æª”æ¡ˆ
if len(current_files) > 0:
    archive_name = f"lightcurves_part{archive_num:03d}.tar.gz"
    archive_path = OUTPUT_DIR / archive_name

    print(f"  Creating {archive_name} ({len(current_files)} files, {current_size/1024/1024/1024:.2f} GB)...")

    with tarfile.open(archive_path, 'w:gz') as tar:
        for f in current_files:
            arcname = f.relative_to(LIGHTCURVE_DIR)
            tar.add(f, arcname=arcname)

    created_archives.append({
        'name': archive_name,
        'path': archive_path,
        'files': len(current_files),
        'size': current_size
    })

# çµ±è¨ˆ
print(f"\n[3/3] Summary")
print(f"  Created {len(created_archives)} archives:")
for i, archive in enumerate(created_archives, 1):
    size_gb = archive['size'] / 1024 / 1024 / 1024
    archive_size = archive['path'].stat().st_size / 1024 / 1024 / 1024
    print(f"    {i}. {archive['name']}: {archive['files']} files, {archive_size:.2f} GB compressed")

total_compressed = sum(a['path'].stat().st_size for a in created_archives) / 1024 / 1024 / 1024
print(f"\n  Total compressed size: {total_compressed:.2f} GB")
print(f"  Compression ratio: {(1 - total_compressed / (total_size/1024/1024/1024)) * 100:.1f}%")
print(f"\n  Output directory: {OUTPUT_DIR}")

# å‰µå»ºæ¸…å–®æª”æ¡ˆ
manifest_path = OUTPUT_DIR / 'manifest.txt'
with open(manifest_path, 'w') as f:
    f.write(f"# Exoplanet Lightcurves Archive Manifest\n")
    f.write(f"# Generated: {datetime.now().isoformat()}\n")
    f.write(f"# Total archives: {len(created_archives)}\n")
    f.write(f"# Total size: {total_compressed:.2f} GB\n\n")

    for archive in created_archives:
        size_gb = archive['path'].stat().st_size / 1024 / 1024 / 1024
        f.write(f"{archive['name']}\t{archive['files']} files\t{size_gb:.2f} GB\n")

print(f"  Manifest saved: {manifest_path}")

print("="*70)
print("âœ… Archive creation complete!")
print("\nNext steps:")
print("  1. Create GitHub Release: gh release create v1.0-lightcurves --title 'Lightcurve Data v1.0'")
print("  2. Upload archives: gh release upload v1.0-lightcurves releases/*.tar.gz")
print("="*70)
```

### æ­¥é©Ÿ 2.2: åŸ·è¡Œåˆ‡åˆ†

```bash
python scripts/split_for_release.py
```

**è¼¸å‡º**: `releases/lightcurves_part001.tar.gz`, `part002.tar.gz`, ...
**æª”æ¡ˆæ•¸**: ç´„ 3-6 å€‹ï¼ˆå–æ±ºæ–¼å¯¦éš›ä¸‹è¼‰å¤§å°ï¼‰

---

## 3ï¸âƒ£ å‰µå»º GitHub Release

### æ­¥é©Ÿ 3.1: å®‰è£ GitHub CLIï¼ˆå¦‚æœªå®‰è£ï¼‰

**Windows**:
```bash
winget install GitHub.cli
```

**macOS**:
```bash
brew install gh
```

**Linux**:
```bash
sudo apt install gh
```

### æ­¥é©Ÿ 3.2: ç™»å…¥ GitHub

```bash
gh auth login
```

é¸æ“‡ï¼š
- GitHub.com
- HTTPS
- Login with a web browser

### æ­¥é©Ÿ 3.3: å‰µå»º Release

```bash
gh release create v1.0-lightcurves \
  --title "TESS Lightcurve Data v1.0" \
  --notes "Complete dataset of TESS light curves for exoplanet detection.

**Contents:**
- ~6,800 TESS light curve files (HDF5 format)
- Downloaded from MAST Archive
- Total size: ~50 GB (compressed)
- Split into parts of â‰¤ 2 GiB for upload

**Usage:**
See [Download Guide](https://github.com/exoplanet-spaceapps/exoplanet-starter/blob/main/docs/DOWNLOAD_FROM_RELEASE.md) for instructions on downloading and extracting.

**Dataset Info:**
- Source: NASA TESS Mission
- Targets: TOI candidates + False Positives
- Format: HDF5 (multi-sector support)
- Features: time, flux, flux_err per sector"
```

---

## 4ï¸âƒ£ ä¸Šå‚³è³‡ç”¢åˆ° Release

### æ–¹å¼ A: æ‰¹æ¬¡ä¸Šå‚³æ‰€æœ‰æª”æ¡ˆï¼ˆæ¨è–¦ï¼‰

```bash
gh release upload v1.0-lightcurves releases/*.tar.gz --clobber
```

### æ–¹å¼ B: é€ä¸€ä¸Šå‚³ï¼ˆæœ‰é€²åº¦é¡¯ç¤ºï¼‰

```bash
# ä½¿ç”¨ PowerShellï¼ˆWindowsï¼‰
Get-ChildItem releases\*.tar.gz | ForEach-Object {
    Write-Host "Uploading $($_.Name)..."
    gh release upload v1.0-lightcurves $_.FullName
}
```

```bash
# ä½¿ç”¨ Bashï¼ˆLinux/macOSï¼‰
for file in releases/*.tar.gz; do
    echo "Uploading $(basename $file)..."
    gh release upload v1.0-lightcurves "$file"
done
```

### æ–¹å¼ C: ä½¿ç”¨ Python è…³æœ¬ä¸Šå‚³ï¼ˆå¸¶é€²åº¦æ¢ï¼‰

å‰µå»º `scripts/upload_to_release.py`:

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""Upload archives to GitHub Release with progress tracking"""

import subprocess
from pathlib import Path
from tqdm import tqdm

PROJECT_ROOT = Path(__file__).parent.parent
RELEASES_DIR = PROJECT_ROOT / 'releases'
RELEASE_TAG = 'v1.0-lightcurves'

archives = sorted(RELEASES_DIR.glob('*.tar.gz'))

print(f"Found {len(archives)} archives to upload")
print(f"Target release: {RELEASE_TAG}\n")

for archive in tqdm(archives, desc="Uploading"):
    cmd = ['gh', 'release', 'upload', RELEASE_TAG, str(archive), '--clobber']
    result = subprocess.run(cmd, capture_output=True, text=True)

    if result.returncode != 0:
        print(f"\nâŒ Failed to upload {archive.name}")
        print(f"   Error: {result.stderr}")
    else:
        tqdm.write(f"âœ… {archive.name}")

print("\nâœ… Upload complete!")
```

åŸ·è¡Œï¼š
```bash
python scripts/upload_to_release.py
```

---

## 5ï¸âƒ£ é©—è­‰ä¸Šå‚³

### æ­¥é©Ÿ 5.1: æª¢æŸ¥ Release è³‡ç”¢

```bash
gh release view v1.0-lightcurves
```

### æ­¥é©Ÿ 5.2: é©—è­‰æª”æ¡ˆå®Œæ•´æ€§

å‰µå»º `scripts/verify_release.py`:

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""Verify uploaded releases match local files"""

import subprocess
import json
from pathlib import Path

PROJECT_ROOT = Path(__file__).parent.parent
RELEASES_DIR = PROJECT_ROOT / 'releases'
RELEASE_TAG = 'v1.0-lightcurves'

# ç²å– release è³‡è¨Š
cmd = ['gh', 'release', 'view', RELEASE_TAG, '--json', 'assets']
result = subprocess.run(cmd, capture_output=True, text=True)
release_data = json.loads(result.stdout)

remote_assets = {asset['name']: asset['size'] for asset in release_data['assets']}
local_files = {f.name: f.stat().st_size for f in RELEASES_DIR.glob('*.tar.gz')}

print("="*70)
print("Verification Report")
print("="*70)

print(f"\nLocal files: {len(local_files)}")
print(f"Remote assets: {len(remote_assets)}")

missing_remote = set(local_files.keys()) - set(remote_assets.keys())
missing_local = set(remote_assets.keys()) - set(local_files.keys())

if missing_remote:
    print(f"\nâŒ Missing from remote: {missing_remote}")
else:
    print("\nâœ… All local files uploaded")

if missing_local:
    print(f"\nâš ï¸ Extra files on remote: {missing_local}")

# æª¢æŸ¥æª”æ¡ˆå¤§å°
size_mismatches = []
for name in set(local_files.keys()) & set(remote_assets.keys()):
    if local_files[name] != remote_assets[name]:
        size_mismatches.append(name)
        print(f"\nâš ï¸ Size mismatch: {name}")
        print(f"   Local: {local_files[name]:,} bytes")
        print(f"   Remote: {remote_assets[name]:,} bytes")

if not size_mismatches and not missing_remote:
    print("\nâœ… All files verified successfully!")
else:
    print(f"\nâš ï¸ Found {len(size_mismatches)} size mismatches")

print("="*70)
```

åŸ·è¡Œï¼š
```bash
python scripts/verify_release.py
```

---

## 6ï¸âƒ£ ä¸‹è¼‰è³‡æ–™ï¼ˆå…¶ä»–è£ç½®ä½¿ç”¨ï¼‰

### å¿«é€Ÿä¸‹è¼‰è…³æœ¬

å‰µå»º `scripts/download_from_release.py`:

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""Download and extract lightcurves from GitHub Release"""

import subprocess
import tarfile
from pathlib import Path
from tqdm import tqdm

PROJECT_ROOT = Path(__file__).parent.parent
LIGHTCURVE_DIR = PROJECT_ROOT / 'data' / 'lightcurves'
RELEASE_TAG = 'v1.0-lightcurves'

LIGHTCURVE_DIR.mkdir(parents=True, exist_ok=True)

print("="*70)
print("Downloading Lightcurves from GitHub Release")
print("="*70)

# ç²å– release è³‡ç”¢åˆ—è¡¨
cmd = ['gh', 'release', 'view', RELEASE_TAG, '--json', 'assets', '-q', '.assets[].name']
result = subprocess.run(cmd, capture_output=True, text=True)
assets = [name for name in result.stdout.strip().split('\n') if name.endswith('.tar.gz')]

print(f"\nFound {len(assets)} archives")

# ä¸‹è¼‰ä¸¦è§£å£“
for asset in tqdm(assets, desc="Processing"):
    print(f"\n[{asset}]")

    # ä¸‹è¼‰
    print("  Downloading...")
    cmd = ['gh', 'release', 'download', RELEASE_TAG, '-p', asset]
    subprocess.run(cmd, check=True, cwd=PROJECT_ROOT)

    # è§£å£“
    print("  Extracting...")
    archive_path = PROJECT_ROOT / asset
    with tarfile.open(archive_path, 'r:gz') as tar:
        tar.extractall(path=LIGHTCURVE_DIR)

    # åˆªé™¤å£“ç¸®æª”
    archive_path.unlink()
    print("  âœ… Done")

print("\n" + "="*70)
print("âœ… Download complete!")
print(f"Files extracted to: {LIGHTCURVE_DIR}")
print("="*70)
```

ä½¿ç”¨ï¼š
```bash
gh auth login  # é¦–æ¬¡éœ€è¦ç™»å…¥
python scripts/download_from_release.py
```

---

## 7ï¸âƒ£ æ¸…ç†æœ¬åœ°æª”æ¡ˆï¼ˆå¯é¸ï¼‰

ä¸Šå‚³å®Œæˆå¾Œï¼Œå¯åˆªé™¤æœ¬åœ°çš„å£“ç¸®æª”ä»¥ç¯€çœç©ºé–“ï¼š

```bash
# åƒ…åˆªé™¤å£“ç¸®æª”ï¼Œä¿ç•™åŸå§‹è³‡æ–™
rm -rf releases/

# æˆ–å®Œå…¨æ¸…ç†ï¼ˆä¸‹è¼‰çš„å…‰æ›²ç·šä¹Ÿåˆªé™¤ï¼‰
rm -rf data/lightcurves/
rm -rf releases/
```

---

## ğŸ“‹ å®Œæ•´å·¥ä½œæµç¨‹ç¸½çµ

```bash
# === åœ¨ä¸‹è¼‰è£ç½®åŸ·è¡Œ ===

# 1. Clone å€‰åº«
git clone https://github.com/exoplanet-spaceapps/exoplanet-starter.git
cd exoplanet-starter

# 2. ä¸‹è¼‰è³‡æ–™ï¼ˆ6-7 å°æ™‚ï¼‰
python scripts/run_test_fixed.py

# 3. åˆ‡åˆ†è³‡æ–™
python scripts/split_for_release.py

# 4. å‰µå»º Release
gh release create v1.0-lightcurves --title "TESS Lightcurve Data v1.0" \
  --notes "Complete TESS light curve dataset (~50 GB)"

# 5. ä¸Šå‚³è³‡ç”¢
python scripts/upload_to_release.py  # æˆ–ä½¿ç”¨ gh release upload

# 6. é©—è­‰
python scripts/verify_release.py

# === åœ¨å…¶ä»–è£ç½®ä½¿ç”¨ ===

# ä¸‹è¼‰è³‡æ–™
git clone https://github.com/exoplanet-spaceapps/exoplanet-starter.git
cd exoplanet-starter
python scripts/download_from_release.py
```

---

## ğŸ”§ æ•…éšœæ’é™¤

### å•é¡Œ 1: gh å‘½ä»¤æ‰¾ä¸åˆ°

```bash
# ç¢ºèªå®‰è£
gh --version

# é‡æ–°ç™»å…¥
gh auth logout
gh auth login
```

### å•é¡Œ 2: ä¸Šå‚³è¶…æ™‚

```bash
# å–®ç¨é‡è©¦å¤±æ•—çš„æª”æ¡ˆ
gh release upload v1.0-lightcurves releases/lightcurves_part001.tar.gz --clobber
```

### å•é¡Œ 3: è³‡ç”¢å·²å­˜åœ¨

```bash
# ä½¿ç”¨ --clobber è¦†è“‹
gh release upload v1.0-lightcurves releases/*.tar.gz --clobber
```

### å•é¡Œ 4: Release æ¬Šé™éŒ¯èª¤

ç¢ºèªä½ æ˜¯ repo çš„ collaborator æˆ– ownerï¼Œä¸¦ä¸” token æœ‰ `repo` æ¬Šé™ã€‚

---

## ğŸ“Š å®¹é‡è¦åŠƒ

| é …ç›® | å¤§å° | èªªæ˜ |
|------|------|------|
| åŸå§‹ä¸‹è¼‰ | ~10 GB | HDF5 æª”æ¡ˆæœªå£“ç¸® |
| å£“ç¸®å¾Œ | ~5-8 GB | tar.gz å£“ç¸® |
| å–®ä¸€è³‡ç”¢ | â‰¤ 2 GB | GitHub é™åˆ¶ |
| é ä¼°æª”æ¡ˆæ•¸ | 3-6 å€‹ | å–æ±ºæ–¼å¯¦éš›å¤§å° |
| GitHub é™åˆ¶ | 1000 è³‡ç”¢ | è¶³å¤ ä½¿ç”¨ |

---

## âœ… æª¢æŸ¥æ¸…å–®

ä¸‹è¼‰è£ç½®ï¼š
- [ ] åŸ·è¡Œ `run_test_fixed.py` å®Œæˆä¸‹è¼‰
- [ ] åŸ·è¡Œ `split_for_release.py` åˆ‡åˆ†è³‡æ–™
- [ ] å‰µå»º GitHub Release
- [ ] ä¸Šå‚³æ‰€æœ‰è³‡ç”¢
- [ ] é©—è­‰ä¸Šå‚³å®Œæ•´æ€§
- [ ] ï¼ˆå¯é¸ï¼‰åˆªé™¤æœ¬åœ°å£“ç¸®æª”

å…¶ä»–è£ç½®ï¼š
- [ ] Clone å€‰åº«
- [ ] åŸ·è¡Œ `download_from_release.py`
- [ ] é©—è­‰æª”æ¡ˆå®Œæ•´æ€§
- [ ] ç¹¼çºŒå¾ŒçºŒåˆ†ææµç¨‹

---

**Created**: 2025-10-03
**Status**: Ready for execution
**Estimated time**: 7-8 hours (download) + 1-2 hours (split & upload)
