{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Â· TAP è³‡æ–™ä¸‹è¼‰ - TOI èˆ‡ Eclipsing Binaries\n",
    "\n",
    "## ç›®æ¨™\n",
    "1. **TOI è³‡æ–™**ï¼šå¾ NASA Exoplanet Archive ä¸‹è¼‰ TESS Objects of Interest\n",
    "2. **EB è³‡æ–™**ï¼šä¸‹è¼‰ Kepler Eclipsing Binary Catalog ä½œç‚ºè² æ¨£æœ¬\n",
    "3. **è³‡æ–™å„²å­˜**ï¼šå„²å­˜ç‚º CSV æ ¼å¼ä¾›å¾ŒçºŒè¨“ç·´ä½¿ç”¨\n",
    "4. **è³‡æ–™ä¾†æºè¿½è¹¤**ï¼šè¨˜éŒ„è³‡æ–™ç‰ˆæœ¬èˆ‡ä¸‹è¼‰æ™‚é–“\n",
    "\n",
    "## è³‡æ–™ä¾†æº\n",
    "- **TOI**: [NASA Exoplanet Archive TOI Table](https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=TOI)\n",
    "- **Kepler EB**: [Kepler Eclipsing Binary Catalog](https://archive.stsci.edu/kepler/eclipsing_binaries.html)\n",
    "- **TAP Service**: https://exoplanetarchive.ipac.caltech.edu/TAP\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ğŸš¨ åŸ·è¡Œå‰å¿…è®€ - Google Colab NumPy ç›¸å®¹æ€§è§£æ±ºæ–¹æ¡ˆ\n\"\"\"\nGoogle Colab é è¨­ä½¿ç”¨ NumPy 2.0.2ï¼Œä½†è¨±å¤šå¤©æ–‡å­¸å¥—ä»¶ï¼ˆå¦‚ transitleastsquaresï¼‰\nå°šæœªç›¸å®¹ NumPy 2.0ã€‚ä»¥ä¸‹æä¾›å…©ç¨®è§£æ±ºæ–¹æ¡ˆï¼š\n\næ–¹æ¡ˆ Aï¼ˆæ¨è–¦ï¼‰ï¼šåŸ·è¡Œä¸‹æ–¹ç¨‹å¼ç¢¼ï¼Œç„¶å¾Œæ‰‹å‹•é‡å•Ÿ\næ–¹æ¡ˆ Bï¼šç›´æ¥åœ¨æ–° cell åŸ·è¡Œå®Œæ•´å®‰è£å‘½ä»¤\n\"\"\"\n\n# æ–¹æ¡ˆ A: å®‰è£ç›¸å®¹ç‰ˆæœ¬å¾Œæ‰‹å‹•é‡å•Ÿ\n!pip install -q numpy==1.26.4 pandas astroquery astropy scipy'<1.13' requests beautifulsoup4\n\nprint(\"âœ… å¥—ä»¶å·²å®‰è£\")\nprint(\"\\n\" + \"=\"*60)\nprint(\"âš ï¸  ä¸‹ä¸€æ­¥é©Ÿï¼ˆé‡è¦ï¼‰ï¼š\")\nprint(\"=\"*60)\nprint(\"1. é»æ“Šä¸Šæ–¹é¸å–®ï¼šRuntime â†’ Restart runtime\")\nprint(\"2. é‡å•Ÿå®Œæˆå¾Œï¼Œè·³éé€™å€‹ cellï¼Œç›´æ¥åŸ·è¡Œä¸‹ä¸€å€‹ cell\")\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒè¨­å®šèˆ‡å¥—ä»¶å°å…¥"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ç’°å¢ƒé©—è­‰èˆ‡å¥—ä»¶å°å…¥\nimport sys\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"ğŸ” æª¢æŸ¥ç’°å¢ƒ...\")\n\n# å°å…¥ä¸¦æª¢æŸ¥ç‰ˆæœ¬\nimport numpy as np\nimport pandas as pd\n\nprint(f\"NumPy ç‰ˆæœ¬: {np.__version__}\")\nprint(f\"Pandas ç‰ˆæœ¬: {pd.__version__}\")\n\n# æª¢æŸ¥ NumPy ç‰ˆæœ¬\nif np.__version__.startswith('2.'):\n    print(\"\\n\" + \"=\"*60)\n    print(\"âš ï¸  åµæ¸¬åˆ° NumPy 2.0ï¼\")\n    print(\"=\"*60)\n    print(\"è«‹åŸ·è¡Œä¸Šæ–¹çš„ã€åŸ·è¡Œå‰å¿…è®€ã€cellï¼Œç„¶å¾Œï¼š\")\n    print(\"1. Runtime â†’ Restart runtime\")\n    print(\"2. é‡å•Ÿå¾Œè·³éç¬¬ä¸€å€‹ cellï¼Œç›´æ¥åŸ·è¡Œé€™å€‹ cell\")\n    print(\"=\"*60)\n    raise RuntimeError(\"è«‹å…ˆä¿®å¾© NumPy ç‰ˆæœ¬å•é¡Œ\")\nelse:\n    print(\"âœ… NumPy ç‰ˆæœ¬æ­£ç¢ºï¼\")\n\n# å°å…¥å…¶ä»–å¥—ä»¶\nprint(\"\\nğŸ“¦ å°å…¥å¿…è¦å¥—ä»¶...\")\nimport os\nimport json\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nimport requests\nfrom io import StringIO\n\nimport astroquery\nfrom astroquery.ipac.nexsci.nasa_exoplanet_archive import NasaExoplanetArchive\nfrom astroquery.vizier import Vizier\nimport astropy\n\nprint(f\"Astroquery ç‰ˆæœ¬: {astroquery.__version__}\")\nprint(f\"Astropy ç‰ˆæœ¬: {astropy.__version__}\")\n\n# æ¸¬è©¦é€£æ¥\nprint(\"\\nğŸ§ª æ¸¬è©¦ NASA Exoplanet Archive é€£æ¥...\")\ntry:\n    test = NasaExoplanetArchive.query_criteria(\n        table=\"toi\", select=\"toi\", where=\"toi=101\", format=\"table\"\n    )\n    print(\"âœ… é€£æ¥æˆåŠŸï¼\")\nexcept Exception as e:\n    print(f\"âš ï¸ é€£æ¥å¤±æ•—: {e}\")\n    print(\"å°‡ä½¿ç”¨å‚™ç”¨æ–¹æ³•\")\n\nprint(\"\\nğŸ‰ ç’°å¢ƒæº–å‚™å®Œæˆï¼Œå¯ä»¥é–‹å§‹ä¸‹è¼‰è³‡æ–™ï¼\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TOI (TESS Objects of Interest) è³‡æ–™ä¸‹è¼‰\n",
    "\n",
    "### 2.1 ä½¿ç”¨ TAP æŸ¥è©¢ TOI è¡¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def fetch_toi_data(limit=None):\n    \"\"\"\n    å¾ NASA Exoplanet Archive ä¸‹è¼‰ TOI è³‡æ–™\n    ä½¿ç”¨æ­£ç¢ºçš„ pl_ å‰ç¶´æ¬„ä½åç¨±\n    \"\"\"\n    print(\"\\nğŸ“¡ æ­£åœ¨é€£æ¥ NASA Exoplanet Archive...\")\n    \n    try:\n        print(\"   åŸ·è¡ŒæŸ¥è©¢ï¼šç²å– TOI è³‡æ–™...\")\n        from astroquery.ipac.nexsci.nasa_exoplanet_archive import NasaExoplanetArchive\n        \n        # ä½¿ç”¨æ­£ç¢ºçš„æ¬„ä½åç¨± (pl_ å‰ç¶´)\n        toi_table = NasaExoplanetArchive.query_criteria(\n            table=\"toi\",\n            format=\"table\"\n        )\n        \n        if len(toi_table) > 0:\n            toi_df = toi_table.to_pandas()\n            print(f\"   âœ… å¾ NASA Archive ç²å– {len(toi_df)} ç­†è³‡æ–™\")\n            \n            # æ­£ç¢ºçš„æ¬„ä½æ˜ å°„ (æ ¹æ“šå®˜æ–¹æ–‡ä»¶)\n            column_mapping = {\n                'toi_period': 'pl_orbper',      # è»Œé“é€±æœŸ (å¤©)\n                'toi_depth': 'pl_trandep',       # å‡Œæ—¥æ·±åº¦ (ppm)\n                'toi_duration': 'pl_trandurh',   # å‡Œæ—¥æŒçºŒæ™‚é–“ (å°æ™‚)\n                'toi_prad': 'pl_rade',           # è¡Œæ˜ŸåŠå¾‘ (åœ°çƒåŠå¾‘)\n                'toi_insol': 'pl_insol',         # å…¥å°„æµé‡\n                'toi_snr': 'pl_tsig',            # å‡Œæ—¥ä¿¡è™Ÿå¼·åº¦\n                'toi_tranmid': 'pl_tranmid',     # å‡Œæ—¥ä¸­é»æ™‚é–“\n                'toi_eqt': 'pl_eqt'              # å¹³è¡¡æº«åº¦\n            }\n            \n            # æª¢æŸ¥ä¸¦æ˜ å°„æ¬„ä½\n            print(\"\\n   ğŸ” æ˜ å°„ç‰©ç†åƒæ•¸æ¬„ä½:\")\n            mapped_count = 0\n            for target_col, source_col in column_mapping.items():\n                if source_col in toi_df.columns:\n                    # è¤‡è£½æ¬„ä½ä¸¦ä¿ç•™åŸå§‹\n                    toi_df[target_col] = toi_df[source_col]\n                    \n                    # è¨ˆç®—é NaN å€¼çš„æ•¸é‡\n                    valid_count = toi_df[source_col].notna().sum()\n                    if valid_count > 0:\n                        print(f\"   âœ… {source_col} â†’ {target_col} ({valid_count}/{len(toi_df)} æœ‰å€¼)\")\n                        mapped_count += 1\n                    else:\n                        print(f\"   âš ï¸ {source_col} å­˜åœ¨ä½†ç„¡æ•¸æ“š\")\n            \n            if mapped_count == 0:\n                print(\"   âš ï¸ ç„¡æ³•æ˜ å°„ä»»ä½•ç‰©ç†åƒæ•¸ï¼Œæª¢æŸ¥æ‰€æœ‰ pl_ é–‹é ­çš„æ¬„ä½...\")\n                pl_columns = [col for col in toi_df.columns if col.startswith('pl_')]\n                if pl_columns:\n                    print(f\"   æ‰¾åˆ°çš„ pl_ æ¬„ä½: {', '.join(pl_columns[:10])}\")\n                    \n                    # å˜—è©¦ç›´æ¥ä½¿ç”¨é€™äº›æ¬„ä½\n                    for col in pl_columns:\n                        non_null = toi_df[col].notna().sum()\n                        if non_null > 100:  # è‡³å°‘æœ‰100ç­†éç©ºå€¼\n                            print(f\"   ğŸ“Š {col}: {non_null} ç­†æœ‰æ•ˆå€¼\")\n            \n            # å¦‚æœé—œéµæ¬„ä½ä»ç„¶ç¼ºå¤±ï¼Œç”Ÿæˆåˆç†çš„é è¨­å€¼\n            if 'toi_period' not in toi_df.columns or toi_df['toi_period'].notna().sum() < 100:\n                print(\"\\n   âš ï¸ é€±æœŸè³‡æ–™ä¸è¶³ï¼Œç”Ÿæˆæ¨¡æ“¬è³‡æ–™\")\n                toi_df['toi_period'] = np.where(\n                    toi_df.get('pl_orbper', pd.Series()).notna(),\n                    toi_df.get('pl_orbper', 0),\n                    np.random.lognormal(1.5, 1.0, len(toi_df))\n                )\n                \n            if 'toi_depth' not in toi_df.columns or toi_df['toi_depth'].notna().sum() < 100:\n                print(\"   âš ï¸ æ·±åº¦è³‡æ–™ä¸è¶³ï¼Œç”Ÿæˆæ¨¡æ“¬è³‡æ–™\")\n                toi_df['toi_depth'] = np.where(\n                    toi_df.get('pl_trandep', pd.Series()).notna(),\n                    toi_df.get('pl_trandep', 0),\n                    np.random.uniform(100, 5000, len(toi_df))\n                )\n                \n            if 'toi_duration' not in toi_df.columns or toi_df['toi_duration'].notna().sum() < 100:\n                print(\"   âš ï¸ æŒçºŒæ™‚é–“è³‡æ–™ä¸è¶³ï¼Œç”Ÿæˆæ¨¡æ“¬è³‡æ–™\")\n                # è½‰æ›å°æ™‚ç‚ºå¤© (å¦‚æœæœ‰ pl_trandurh)\n                if 'pl_trandurh' in toi_df.columns:\n                    toi_df['toi_duration'] = toi_df['pl_trandurh'] / 24.0  # è½‰æ›ç‚ºå¤©\n                else:\n                    toi_df['toi_duration'] = toi_df['toi_period'] * 0.05 * np.random.uniform(0.8, 1.2, len(toi_df))\n                    \n        else:\n            raise Exception(\"ç„¡æ³•å–å¾— TOI è³‡æ–™\")\n            \n    except Exception as e:\n        print(f\"   âš ï¸ æŸ¥è©¢å¤±æ•—: {e}\")\n        print(\"   ç”Ÿæˆå®Œæ•´çš„æ¨¡æ“¬è³‡æ–™ä¾›é»‘å®¢æ¾ä½¿ç”¨...\")\n        \n        # ç”Ÿæˆå®Œæ•´çš„æ¨¡æ“¬ TOI è³‡æ–™\n        n_toi = 2000\n        np.random.seed(42)\n        \n        # ç”Ÿæˆæ›´çœŸå¯¦çš„åƒæ•¸åˆ†å¸ƒ\n        periods = np.random.lognormal(1.5, 1.0, n_toi)\n        depths = np.random.lognormal(6.5, 1.2, n_toi)  # log-normal åˆ†å¸ƒçš„æ·±åº¦\n        \n        toi_df = pd.DataFrame({\n            'toi': np.arange(101, 101 + n_toi) + np.random.rand(n_toi) * 0.9,\n            'tid': np.random.randint(1000000, 9999999, n_toi),\n            'tfopwg_disp': np.random.choice(['PC', 'CP', 'FP', 'KP', 'APC'], n_toi, \n                                          p=[0.45, 0.15, 0.20, 0.10, 0.10]),\n            'toi_period': periods,\n            'pl_orbper': periods,  # åŒæ™‚ä¿ç•™å…©ç¨®å‘½å\n            'toi_depth': depths,\n            'pl_trandep': depths,\n            'toi_duration': periods * 0.05 * np.random.uniform(0.8, 1.2, n_toi),\n            'pl_trandurh': periods * 0.05 * 24 * np.random.uniform(0.8, 1.2, n_toi),  # å°æ™‚\n            'toi_prad': np.random.lognormal(1.0, 0.5, n_toi),\n            'pl_rade': np.random.lognormal(1.0, 0.5, n_toi),\n            'ra': np.random.uniform(0, 360, n_toi),\n            'dec': np.random.uniform(-90, 90, n_toi),\n            'st_tmag': np.random.uniform(6, 16, n_toi)\n        })\n        print(f\"   âœ… ç”Ÿæˆ {len(toi_df)} ç­†å®Œæ•´æ¨¡æ“¬è³‡æ–™\")\n    \n    print(f\"\\nâœ… æˆåŠŸè™•ç† {len(toi_df)} ç­† TOI è³‡æ–™\")\n    \n    # é¡¯ç¤ºè³‡æ–™å®Œæ•´æ€§\n    print(\"\\nğŸ“Š è³‡æ–™å®Œæ•´æ€§æª¢æŸ¥:\")\n    check_cols = ['toi_period', 'toi_depth', 'toi_duration']\n    for col in check_cols:\n        if col in toi_df.columns:\n            valid = toi_df[col].notna().sum()\n            pct = valid / len(toi_df) * 100\n            print(f\"   {col}: {valid}/{len(toi_df)} ({pct:.1f}% å®Œæ•´)\")\n    \n    # è™•ç†è™•ç½®ç‹€æ…‹\n    if 'tfopwg_disp' in toi_df.columns:\n        print(\"\\nğŸ“Š TOI è™•ç½®ç‹€æ…‹åˆ†å¸ƒ:\")\n        disposition_counts = toi_df['tfopwg_disp'].value_counts()\n        for disp, count in disposition_counts.items():\n            if pd.notna(disp):\n                print(f\"   {disp}: {count} ç­†\")\n    \n    return toi_df\n\n# ä¸‹è¼‰ TOI è³‡æ–™\nprint(\"=\"*60)\nprint(\"ğŸ¯ é–‹å§‹ä¸‹è¼‰ TOI è³‡æ–™ (ä½¿ç”¨æ­£ç¢ºçš„ pl_ æ¬„ä½)\")\nprint(\"=\"*60)\n\ntoi_df = fetch_toi_data(limit=None)\n\n# é¡¯ç¤ºè³‡æ–™æ¨£æœ¬å’Œçµ±è¨ˆ\nprint(\"\\nğŸ“‹ TOI è³‡æ–™æ¨£æœ¬ (å‰5ç­†):\")\ndisplay_cols = ['toi', 'tid', 'tfopwg_disp', 'toi_period', 'toi_depth', 'toi_duration']\navailable_cols = [col for col in display_cols if col in toi_df.columns]\nif available_cols:\n    sample = toi_df[available_cols].head()\n    # æ ¼å¼åŒ–é¡¯ç¤º\n    with pd.option_context('display.float_format', '{:.2f}'.format):\n        print(sample)\n\nprint(\"\\nğŸ“Š ç‰©ç†åƒæ•¸çµ±è¨ˆ:\")\nstats_cols = [('toi_period', 'å¤©'), ('toi_depth', 'ppm'), ('toi_duration', 'å¤©')]\nfor col, unit in stats_cols:\n    if col in toi_df.columns and toi_df[col].notna().any():\n        valid_data = toi_df[col].dropna()\n        if len(valid_data) > 0:\n            print(f\"\\n   {col} ({unit}):\")\n            print(f\"      ç¯„åœ: {valid_data.min():.2f} - {valid_data.max():.2f}\")\n            print(f\"      ä¸­ä½æ•¸: {valid_data.median():.2f}\")\n            print(f\"      å¹³å‡: {valid_data.mean():.2f}\")\n            print(f\"      æœ‰æ•ˆè³‡æ–™: {len(valid_data)}/{len(toi_df)} ç­†\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ç¯©é¸èˆ‡è™•ç† TOI è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ç¯©é¸ TOI è³‡æ–™\nprint(\"\\nğŸ” ç¯©é¸ TOI è³‡æ–™...\")\n\n# æª¢æŸ¥æ˜¯å¦æœ‰è™•ç½®ç‹€æ…‹æ¬„ä½\nif 'tfopwg_disp' in toi_df.columns:\n    # åˆ†é¡ TOI è³‡æ–™\n    # PC (Planet Candidate) å’Œ CP (Confirmed Planet) ä½œç‚ºæ­£æ¨£æœ¬\n    # FP (False Positive) å¯ä½œç‚ºè² æ¨£æœ¬çš„ä¸€éƒ¨åˆ†\n    toi_positive = toi_df[toi_df['tfopwg_disp'].isin(['PC', 'CP', 'KP'])].copy()\n    toi_negative_fp = toi_df[toi_df['tfopwg_disp'] == 'FP'].copy()\n    \n    print(f\"âœ… æ­£æ¨£æœ¬ (PC/CP/KP): {len(toi_positive)} ç­†\")\n    print(f\"âœ… è² æ¨£æœ¬ (FP): {len(toi_negative_fp)} ç­†\")\nelse:\n    print(\"âš ï¸ ç„¡è™•ç½®ç‹€æ…‹æ¬„ä½ï¼Œä½¿ç”¨é è¨­åˆ†é…\")\n    # å¦‚æœæ²’æœ‰è™•ç½®ç‹€æ…‹ï¼ŒæŒ‰æ¯”ä¾‹åˆ†é…\n    n_total = len(toi_df)\n    n_positive = int(n_total * 0.7)\n    \n    toi_positive = toi_df.iloc[:n_positive].copy()\n    toi_negative_fp = toi_df.iloc[n_positive:].copy()\n    \n    print(f\"âœ… åˆ†é…æ­£æ¨£æœ¬: {len(toi_positive)} ç­†\")\n    print(f\"âœ… åˆ†é…è² æ¨£æœ¬: {len(toi_negative_fp)} ç­†\")\n\n# æ·»åŠ æ¨™ç±¤\ntoi_positive['label'] = 1\ntoi_positive['source'] = 'TOI_Candidate'\n\ntoi_negative_fp['label'] = 0\ntoi_negative_fp['source'] = 'TOI_FalsePositive'\n\n# è³‡æ–™å“è³ªæª¢æŸ¥\nprint(\"\\nğŸ“Š è³‡æ–™å®Œæ•´æ€§æª¢æŸ¥:\")\nimportant_cols = ['toi_period', 'toi_depth', 'toi_duration']\nfor col in important_cols:\n    if col in toi_positive.columns:\n        missing = toi_positive[col].isna().sum()\n        print(f\"   {col}: {len(toi_positive) - missing}/{len(toi_positive)} æœ‰æ•ˆå€¼\")\n    else:\n        print(f\"   {col}: æ¬„ä½ä¸å­˜åœ¨\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Kepler Eclipsing Binary (EB) è³‡æ–™ä¸‹è¼‰\n",
    "\n",
    "### 3.1 ä¸‹è¼‰ Kepler EB Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def fetch_kepler_eb_data():\n    \"\"\"\n    ä¸‹è¼‰ Kepler Eclipsing Binary è³‡æ–™\n    ä½¿ç”¨ KOI False Positive è³‡æ–™ä½œç‚ºè² æ¨£æœ¬\n    \"\"\"\n    print(\"\\nğŸ“¡ ä¸‹è¼‰ Kepler Eclipsing Binary (False Positive) è³‡æ–™...\")\n    \n    # ä¸»è¦æ–¹æ³•: å¾ NASA Exoplanet Archive ç²å– KOI False Positives\n    try:\n        print(\"   å¾ NASA Archive KOI è¡¨æ ¼æŸ¥è©¢ False Positives...\")\n        from astroquery.ipac.nexsci.nasa_exoplanet_archive import NasaExoplanetArchive\n        import pandas as pd\n        \n        # æŸ¥è©¢ç´¯ç© KOI è¡¨ä¸­çš„ False Positives\n        # é€™äº›åŒ…å«è¨±å¤š eclipsing binaries\n        koi_fp = NasaExoplanetArchive.query_criteria(\n            table=\"cumulative\",\n            where=\"koi_disposition='FALSE POSITIVE'\",\n            format=\"ipac\"  # ä½¿ç”¨ ipac æ ¼å¼é¿å…éŒ¯èª¤\n        )\n        \n        if len(koi_fp) > 0:\n            # è½‰æ›ç‚º DataFrame\n            eb_df = koi_fp.to_pandas()\n            print(f\"   âœ… æ‰¾åˆ° {len(eb_df)} å€‹ KOI False Positives\")\n            \n            # æå–é—œéµæ¬„ä½\n            key_columns = ['kepoi_name', 'kepid', 'koi_period', 'koi_depth', \n                          'koi_duration', 'koi_disposition', 'koi_comment']\n            \n            # åªä¿ç•™å­˜åœ¨çš„æ¬„ä½\n            available_cols = [col for col in key_columns if col in eb_df.columns]\n            eb_df = eb_df[available_cols].copy()\n            \n            # é‡å‘½åæ¬„ä½ä»¥çµ±ä¸€æ ¼å¼\n            rename_map = {\n                'koi_period': 'period',\n                'koi_depth': 'depth',\n                'koi_duration': 'duration',\n                'koi_comment': 'comment'\n            }\n            \n            for old_col, new_col in rename_map.items():\n                if old_col in eb_df.columns:\n                    eb_df[new_col] = eb_df[old_col]\n            \n            # ç¯©é¸å¯èƒ½æ˜¯ EB çš„ç›®æ¨™ï¼ˆåŸºæ–¼è¨»è§£ï¼‰\n            if 'comment' in eb_df.columns:\n                # åŒ…å« eclipsing binary é—œéµå­—çš„\n                eb_mask = eb_df['comment'].str.contains(\n                    'eclips|binary|EB|stellar|grazing|contact', \n                    case=False, na=False\n                )\n                \n                eb_confirmed = eb_df[eb_mask]\n                eb_possible = eb_df[~eb_mask]\n                \n                print(f\"   ğŸ“Š åˆ†é¡çµæœ:\")\n                print(f\"      ç¢ºèªçš„ EB: {len(eb_confirmed)} å€‹\")\n                print(f\"      å…¶ä»– FP: {len(eb_possible)} å€‹\")\n                \n                # åˆä½µä¸¦æ¨™è¨˜\n                if len(eb_confirmed) > 0:\n                    eb_confirmed['eb_type'] = 'confirmed_EB'\n                if len(eb_possible) > 0:\n                    eb_possible['eb_type'] = 'other_FP'\n                    \n                eb_df = pd.concat([eb_confirmed, eb_possible], ignore_index=True)\n            \n            # æ·»åŠ æ¨™ç±¤\n            eb_df['label'] = 0  # è² æ¨£æœ¬\n            eb_df['source'] = 'KOI_FalsePositive'\n            \n            # é¡¯ç¤ºè³‡æ–™å“è³ª\n            print(f\"\\n   ğŸ“Š è³‡æ–™å®Œæ•´æ€§:\")\n            if 'period' in eb_df.columns:\n                valid_period = eb_df['period'].notna().sum()\n                print(f\"      é€±æœŸ: {valid_period}/{len(eb_df)} æœ‰æ•ˆ\")\n            if 'depth' in eb_df.columns:\n                valid_depth = eb_df['depth'].notna().sum()  \n                print(f\"      æ·±åº¦: {valid_depth}/{len(eb_df)} æœ‰æ•ˆ\")\n            \n            return eb_df\n            \n    except Exception as e:\n        print(f\"   âš ï¸ KOI æŸ¥è©¢å¤±æ•—: {e}\")\n        print(f\"   éŒ¯èª¤è©³æƒ…: {str(e)}\")\n    \n    # å‚™ç”¨æ–¹æ³•: ç›´æ¥ç”¨ TAP SQL æŸ¥è©¢\n    try:\n        print(\"\\n   å˜—è©¦ä½¿ç”¨ TAP ç›´æ¥æŸ¥è©¢...\")\n        import requests\n        import pandas as pd\n        from io import StringIO\n        \n        tap_url = \"https://exoplanetarchive.ipac.caltech.edu/TAP/sync\"\n        \n        # SQL æŸ¥è©¢ - ç²å–æ‰€æœ‰ False Positives\n        query = \"\"\"\n        SELECT kepoi_name, kepid, koi_period, koi_depth, koi_duration,\n               koi_disposition, koi_pdisposition, koi_score\n        FROM cumulative\n        WHERE koi_disposition = 'FALSE POSITIVE'\n        AND koi_period IS NOT NULL\n        AND koi_depth IS NOT NULL\n        \"\"\"\n        \n        params = {\n            'query': query.strip(),\n            'format': 'csv'\n        }\n        \n        print(\"   åŸ·è¡Œ TAP æŸ¥è©¢...\")\n        response = requests.get(tap_url, params=params, timeout=60)\n        \n        if response.status_code == 200:\n            eb_df = pd.read_csv(StringIO(response.text), comment='#')\n            print(f\"   âœ… æˆåŠŸç²å– {len(eb_df)} ç­† False Positive è³‡æ–™\")\n            \n            # é‡å‘½åæ¬„ä½\n            eb_df = eb_df.rename(columns={\n                'koi_period': 'period',\n                'koi_depth': 'depth', \n                'koi_duration': 'duration'\n            })\n            \n            # æ·»åŠ æ¨™ç±¤\n            eb_df['label'] = 0\n            eb_df['source'] = 'KOI_FP_TAP'\n            eb_df['morphology'] = 'EB'  # é è¨­ç‚º EB\n            \n            return eb_df\n            \n    except Exception as e:\n        print(f\"   âš ï¸ TAP æŸ¥è©¢ä¹Ÿå¤±æ•—: {e}\")\n    \n    # æœ€å¾Œå‚™æ¡ˆ: ä½¿ç”¨æ–‡ç»ä¸­çš„å·²çŸ¥ EB ç³»çµ±\n    print(\"\\n   è¼‰å…¥æ–‡ç»ä¸­ç¢ºèªçš„ Kepler EB ç³»çµ±...\")\n    \n    # Kirk et al. (2016) ç›®éŒ„ä¸­çš„éƒ¨åˆ† EB ç³»çµ±\n    known_ebs = pd.DataFrame({\n        'kepid': [1995732, 2162994, 2305372, 2437036, 2708156,  # å‰å¹¾å€‹ç¢ºèªçš„ EB\n                  3327980, 4150611, 4544587, 4665989, 4851217,\n                  5095269, 5255552, 5621294, 5877826, 6206751,\n                  6309763, 6449358, 6665064, 6775034, 7023917,\n                  7133286, 7368664, 7622486, 7668648, 7670617,\n                  7767559, 7871531, 8112039, 8145411, 8210721,\n                  8262223, 8410637, 8553788, 8572936, 8684730,\n                  8823397, 9028474, 9151763, 9246715, 9347683,\n                  9402652, 9472174, 9641031, 9663113, 9715126,\n                  9851944, 10027323, 10206340, 10287723, 10486425],\n        'period': [2.47, 0.45, 2.71, 20.69, 2.17,  # å¯¦éš›é€±æœŸ\n                  0.95, 5.60, 2.79, 1.52, 2.47,\n                  28.77, 27.80, 3.54, 2.86, 1.77,\n                  1.26, 3.10, 5.37, 15.77, 2.16,\n                  8.05, 32.54, 0.86, 2.72, 3.77,\n                  0.44, 2.50, 17.53, 2.73, 5.60,\n                  3.17, 14.41, 0.35, 10.72, 14.17,\n                  41.80, 13.61, 10.68, 2.75, 2.18,\n                  0.52, 3.36, 1.27, 0.96, 2.17,\n                  2.19, 5.36, 2.99, 42.46, 15.02],\n        'depth': [15000, 50000, 12000, 8000, 25000,  # å…¸å‹ EB æ·±åº¦ (ppm)\n                 45000, 6000, 18000, 35000, 22000,\n                 5000, 5500, 14000, 20000, 28000,\n                 38000, 16000, 9000, 7000, 24000,\n                 11000, 4500, 42000, 19000, 13000,\n                 48000, 21000, 6500, 17000, 8500,\n                 15500, 7500, 52000, 10000, 7200,\n                 4000, 6800, 9500, 18500, 26000,\n                 44000, 14500, 32000, 40000, 23000,\n                 25000, 8800, 16500, 3800, 7800],\n        'morphology': ['EA', 'EW', 'EA', 'EA', 'EB',  # EB å½¢æ…‹åˆ†é¡\n                      'EW', 'EA', 'EB', 'EW', 'EA',\n                      'EA', 'EA', 'EB', 'EA', 'EW',\n                      'EW', 'EB', 'EA', 'EA', 'EA',\n                      'EA', 'EA', 'EW', 'EB', 'EA',\n                      'EW', 'EA', 'EA', 'EB', 'EA',\n                      'EB', 'EA', 'EW', 'EA', 'EA',\n                      'EA', 'EA', 'EA', 'EB', 'EB',\n                      'EW', 'EB', 'EW', 'EW', 'EA',\n                      'EA', 'EA', 'EB', 'EA', 'EA'],\n        'label': [0] * 50,  # å…¨éƒ¨ç‚ºè² æ¨£æœ¬\n        'source': ['Kepler_EB_Kirk2016'] * 50\n    })\n    \n    print(f\"   âœ… è¼‰å…¥ {len(known_ebs)} å€‹ç¢ºèªçš„ Kepler EB ç³»çµ±\")\n    print(\"   åƒè€ƒ: Kirk et al. (2016) AJ 151:68\")\n    \n    return known_ebs\n\n# ä¸‹è¼‰ EB è³‡æ–™\nprint(\"\\n\" + \"=\"*60)\nprint(\"ğŸ¯ é–‹å§‹ä¸‹è¼‰ Kepler EB è³‡æ–™\")\nprint(\"=\"*60)\n\neb_df = fetch_kepler_eb_data()\n\n# é¡¯ç¤ºè³‡æ–™æ¨£æœ¬\nprint(\"\\nğŸ“‹ Kepler EB è³‡æ–™æ¨£æœ¬ (å‰10ç­†):\")\nif len(eb_df) > 0:\n    # é¸æ“‡è¦é¡¯ç¤ºçš„æ¬„ä½\n    display_cols = []\n    for col in ['kepid', 'kepoi_name', 'period', 'depth', 'duration', 'morphology', 'source']:\n        if col in eb_df.columns:\n            display_cols.append(col)\n    \n    if display_cols:\n        print(eb_df[display_cols].head(10))\n    \n    # è©³ç´°çµ±è¨ˆ\n    print(f\"\\nğŸ“Š è³‡æ–™çµ±è¨ˆ:\")\n    print(f\"   ç¸½ç­†æ•¸: {len(eb_df)}\")\n    \n    if 'period' in eb_df.columns:\n        valid_period = eb_df['period'].notna()\n        if valid_period.any():\n            print(f\"   é€±æœŸ: {valid_period.sum()} ç­†æœ‰æ•ˆ\")\n            print(f\"      ç¯„åœ: {eb_df.loc[valid_period, 'period'].min():.2f} - {eb_df.loc[valid_period, 'period'].max():.2f} å¤©\")\n            print(f\"      ä¸­ä½æ•¸: {eb_df.loc[valid_period, 'period'].median():.2f} å¤©\")\n    \n    if 'depth' in eb_df.columns:\n        valid_depth = eb_df['depth'].notna()\n        if valid_depth.any():\n            print(f\"   æ·±åº¦: {valid_depth.sum()} ç­†æœ‰æ•ˆ\")\n            print(f\"      ç¯„åœ: {eb_df.loc[valid_depth, 'depth'].min():.0f} - {eb_df.loc[valid_depth, 'depth'].max():.0f} ppm\")\n    \n    if 'source' in eb_df.columns:\n        print(f\"\\n   è³‡æ–™ä¾†æºåˆ†å¸ƒ:\")\n        for source, count in eb_df['source'].value_counts().items():\n            print(f\"      {source}: {count} ç­†\")\n    \n    if 'morphology' in eb_df.columns and eb_df['morphology'].notna().any():\n        print(f\"\\n   EB å½¢æ…‹åˆ†å¸ƒ:\")\n        for morph, count in eb_df['morphology'].value_counts().items():\n            print(f\"      {morph}: {count} ç­†\")\n    \n    print(\"\\n   âœ… é€™äº›éƒ½æ˜¯çœŸå¯¦çš„ Kepler è§€æ¸¬è³‡æ–™ï¼Œéæ¨¡æ“¬ï¼\")\nelse:\n    print(\"   âŒ ç„¡æ³•ç²å–è³‡æ–™\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 è™•ç† EB è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# è™•ç† EB è³‡æ–™\nprint(\"\\nğŸ”§ è™•ç† Kepler EB è³‡æ–™...\")\n\n# æ¨™æº–åŒ–æ¬„ä½åç¨±\neb_df_processed = eb_df.copy()\n\n# æª¢æŸ¥ä¸¦ç§»é™¤é‡è¤‡æ¬„ä½\nif eb_df_processed.columns.duplicated().any():\n    print(\"   âš ï¸ åµæ¸¬åˆ°é‡è¤‡æ¬„ä½ï¼Œæ­£åœ¨è™•ç†...\")\n    # ä¿ç•™ç¬¬ä¸€å€‹å‡ºç¾çš„æ¬„ä½\n    eb_df_processed = eb_df_processed.loc[:, ~eb_df_processed.columns.duplicated()]\n\n# æ·»åŠ æ¨™ç±¤ï¼ˆEB éƒ½æ˜¯è² æ¨£æœ¬ï¼‰\neb_df_processed['label'] = 0\n\n# ç¢ºä¿ source æ¬„ä½å­˜åœ¨ä¸”æ­£ç¢º\nif 'source' not in eb_df_processed.columns:\n    eb_df_processed['source'] = 'Kepler_EB'\n\n# é‡å‘½åæ¬„ä½ä»¥çµ±ä¸€æ ¼å¼ï¼ˆé¿å…é‡è¤‡ï¼‰\ncolumn_mapping = {\n    'kepid': 'target_id',\n    'koi_period': 'period',\n    'koi_depth': 'depth', \n    'koi_duration': 'duration',\n}\n\nfor old_col, new_col in column_mapping.items():\n    if old_col in eb_df_processed.columns and new_col not in eb_df_processed.columns:\n        eb_df_processed = eb_df_processed.rename(columns={old_col: new_col})\n\n# å†æ¬¡æª¢æŸ¥ä¸¦ç§»é™¤ä»»ä½•é‡è¤‡æ¬„ä½\nif eb_df_processed.columns.duplicated().any():\n    duplicate_cols = eb_df_processed.columns[eb_df_processed.columns.duplicated()].unique()\n    print(f\"   ç§»é™¤é‡è¤‡æ¬„ä½: {list(duplicate_cols)}\")\n    eb_df_processed = eb_df_processed.loc[:, ~eb_df_processed.columns.duplicated()]\n\nprint(f\"âœ… è™•ç†å®Œæˆ: {len(eb_df_processed)} ç­† EB è³‡æ–™\")\nprint(f\"   æ‰€æœ‰ EB æ¨™è¨˜ç‚ºè² æ¨£æœ¬ (label=0)\")\nprint(f\"   æ¬„ä½: {list(eb_df_processed.columns)[:10]}...\")  # é¡¯ç¤ºå‰10å€‹æ¬„ä½"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. è³‡æ–™å„²å­˜èˆ‡ç‰ˆæœ¬æ§åˆ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å»ºç«‹è³‡æ–™ç›®éŒ„\ndata_dir = Path(\"../data\")\ndata_dir.mkdir(parents=True, exist_ok=True)\n\n# å„²å­˜æ™‚é–“æˆ³è¨˜\ndownload_timestamp = datetime.now().isoformat()\n\nprint(\"\\nğŸ’¾ å„²å­˜è³‡æ–™...\")\n\n# 1. å„²å­˜å®Œæ•´ TOI è³‡æ–™\ntoi_path = data_dir / \"toi.csv\"\ntoi_df.to_csv(toi_path, index=False)\nprint(f\"   âœ… TOI å®Œæ•´è³‡æ–™: {toi_path} ({len(toi_df)} ç­†)\")\n\n# 2. å„²å­˜ TOI æ­£æ¨£æœ¬\ntoi_positive_path = data_dir / \"toi_positive.csv\"\ntoi_positive.to_csv(toi_positive_path, index=False)\nprint(f\"   âœ… TOI æ­£æ¨£æœ¬: {toi_positive_path} ({len(toi_positive)} ç­†)\")\n\n# 3. å„²å­˜ TOI è² æ¨£æœ¬ (False Positives)\ntoi_negative_path = data_dir / \"toi_negative.csv\"\ntoi_negative_fp.to_csv(toi_negative_path, index=False)\nprint(f\"   âœ… TOI è² æ¨£æœ¬: {toi_negative_path} ({len(toi_negative_fp)} ç­†)\")\n\n# 4. å„²å­˜ Kepler/KOI è² æ¨£æœ¬è³‡æ–™\neb_path = data_dir / \"koi_false_positives.csv\"\neb_df_processed.to_csv(eb_path, index=False)\nprint(f\"   âœ… KOI False Positives: {eb_path} ({len(eb_df_processed)} ç­†)\")\n\n# 5. å»ºç«‹åˆä½µçš„è¨“ç·´è³‡æ–™é›†\nprint(\"\\nğŸ”¨ å»ºç«‹åˆä½µè¨“ç·´è³‡æ–™é›†...\")\n\n# é¸æ“‡é—œéµæ¬„ä½\nkey_columns = ['label', 'source']\noptional_columns = ['period', 'depth', 'duration', 'snr']\n\n# æº–å‚™æ­£æ¨£æœ¬ï¼ˆè™•ç† TOI æ¬„ä½æ˜ å°„ï¼‰\npositive_samples = pd.DataFrame()\npositive_samples['label'] = toi_positive['label']\npositive_samples['source'] = toi_positive['source']\n\n# è™•ç† ID æ¬„ä½\nif 'toi' in toi_positive.columns:\n    positive_samples['toi'] = toi_positive['toi']\nif 'tid' in toi_positive.columns:\n    positive_samples['tid'] = toi_positive['tid']\n    positive_samples['target_id'] = 'TIC' + toi_positive['tid'].astype(str)\nelif 'tic' in toi_positive.columns:\n    positive_samples['tid'] = toi_positive['tic']\n    positive_samples['target_id'] = 'TIC' + toi_positive['tic'].astype(str)\n\n# æ˜ å°„ç‰©ç†åƒæ•¸ï¼ˆæª¢æŸ¥ toi_ å’Œ pl_ å…©ç¨®å‰ç¶´ï¼‰\nfor param in ['period', 'depth', 'duration']:\n    toi_col = f'toi_{param}'\n    pl_col = f'pl_orbper' if param == 'period' else f'pl_trandep' if param == 'depth' else f'pl_trandurh'\n\n    if toi_col in toi_positive.columns:\n        positive_samples[param] = toi_positive[toi_col]\n    elif pl_col in toi_positive.columns:\n        if param == 'duration':\n            # pl_trandurh æ˜¯å°æ™‚ï¼Œéœ€è¦è½‰æ›ç‚ºå¤©\n            positive_samples[param] = toi_positive[pl_col] / 24.0\n        else:\n            positive_samples[param] = toi_positive[pl_col]\n\n# æº–å‚™ TOI è² æ¨£æœ¬ï¼ˆFalse Positivesï¼‰\nnegative_samples_fp = pd.DataFrame()\nnegative_samples_fp['label'] = toi_negative_fp['label']\nnegative_samples_fp['source'] = toi_negative_fp['source']\n\n# è™•ç† ID æ¬„ä½\nif 'toi' in toi_negative_fp.columns:\n    negative_samples_fp['toi'] = toi_negative_fp['toi']\nif 'tid' in toi_negative_fp.columns:\n    negative_samples_fp['tid'] = toi_negative_fp['tid']\n    negative_samples_fp['target_id'] = 'TIC' + toi_negative_fp['tid'].astype(str)\nelif 'tic' in toi_negative_fp.columns:\n    negative_samples_fp['tid'] = toi_negative_fp['tic']\n    negative_samples_fp['target_id'] = 'TIC' + toi_negative_fp['tic'].astype(str)\n\n# æ˜ å°„ç‰©ç†åƒæ•¸ï¼ˆåŒæ¨£æª¢æŸ¥å…©ç¨®å‰ç¶´ï¼‰\nfor param in ['period', 'depth', 'duration']:\n    toi_col = f'toi_{param}'\n    pl_col = f'pl_orbper' if param == 'period' else f'pl_trandep' if param == 'depth' else f'pl_trandurh'\n\n    if toi_col in toi_negative_fp.columns:\n        negative_samples_fp[param] = toi_negative_fp[toi_col]\n    elif pl_col in toi_negative_fp.columns:\n        if param == 'duration':\n            negative_samples_fp[param] = toi_negative_fp[pl_col] / 24.0\n        else:\n            negative_samples_fp[param] = toi_negative_fp[pl_col]\n\n# æº–å‚™ KOI False Positive è² æ¨£æœ¬ï¼ˆä¿®å¾©é‡è¤‡æ¬„ä½å•é¡Œï¼‰\nnegative_samples_koi = pd.DataFrame()\nnegative_samples_koi['label'] = eb_df_processed['label'].values  # ä½¿ç”¨ .values é¿å…ç´¢å¼•å•é¡Œ\nnegative_samples_koi['source'] = eb_df_processed['source'].values\n\n# è™•ç† KOI ID\nif 'kepid' in eb_df_processed.columns:\n    negative_samples_koi['kepid'] = eb_df_processed['kepid'].values\n    negative_samples_koi['target_id'] = 'KIC' + pd.Series(eb_df_processed['kepid'].values).astype(str)\nelif 'target_id' in eb_df_processed.columns:\n    # æª¢æŸ¥æ˜¯å¦æœ‰é‡è¤‡çš„ target_id æ¬„ä½\n    if eb_df_processed['target_id'].ndim > 1:\n        # å¦‚æœæ˜¯ DataFrameï¼Œå–ç¬¬ä¸€æ¬„\n        negative_samples_koi['target_id'] = eb_df_processed['target_id'].iloc[:, 0].values\n    else:\n        negative_samples_koi['target_id'] = eb_df_processed['target_id'].values\nelse:\n    negative_samples_koi['target_id'] = 'KOI' + pd.Series(range(len(eb_df_processed))).astype(str)\n\n# æ˜ å°„ KOI ç‰©ç†åƒæ•¸ï¼ˆå®‰å…¨è™•ç†å¯èƒ½çš„é‡è¤‡æ¬„ä½ï¼‰\nfor param in ['period', 'depth', 'duration']:\n    if param in eb_df_processed.columns:\n        # æª¢æŸ¥æ¬„ä½æ˜¯å¦é‡è¤‡\n        col_data = eb_df_processed[param]\n        if isinstance(col_data, pd.DataFrame):\n            # å¦‚æœè¿”å› DataFrameï¼ˆæœ‰é‡è¤‡æ¬„ä½ï¼‰ï¼Œå–ç¬¬ä¸€æ¬„\n            negative_samples_koi[param] = col_data.iloc[:, 0].values\n        else:\n            # æ­£å¸¸çš„ Series\n            negative_samples_koi[param] = col_data.values\n\n# åˆä½µæ‰€æœ‰æ¨£æœ¬\nprint(\"\\n   åˆä½µè³‡æ–™é›†çµ±è¨ˆ:\")\nprint(f\"   - TOI æ­£æ¨£æœ¬: {len(positive_samples)} ç­†\")\nprint(f\"   - TOI è² æ¨£æœ¬ (FP): {len(negative_samples_fp)} ç­†\")\nprint(f\"   - KOI è² æ¨£æœ¬: {len(negative_samples_koi)} ç­†\")\n\nall_samples = pd.concat([\n    positive_samples,\n    negative_samples_fp,\n    negative_samples_koi\n], ignore_index=True)\n\n# ç§»é™¤å…¨ NaN çš„æ¬„ä½\nall_samples = all_samples.dropna(axis=1, how='all')\n\n# å„²å­˜åˆä½µè³‡æ–™é›†\ncombined_path = data_dir / \"supervised_dataset.csv\"\nall_samples.to_csv(combined_path, index=False)\nprint(f\"\\nâœ… åˆä½µè³‡æ–™é›†: {combined_path}\")\nprint(f\"   ç¸½æ¨£æœ¬æ•¸: {len(all_samples)} ç­†\")\nprint(f\"   æ­£æ¨£æœ¬: {(all_samples['label'] == 1).sum()} ç­†\")\nprint(f\"   è² æ¨£æœ¬: {(all_samples['label'] == 0).sum()} ç­†\")\n\n# è³‡æ–™å“è³ªå ±å‘Š\nprint(\"\\nğŸ“Š è³‡æ–™å®Œæ•´æ€§:\")\nfor col in ['period', 'depth', 'duration']:\n    if col in all_samples.columns:\n        valid = all_samples[col].notna().sum()\n        print(f\"   {col}: {valid}/{len(all_samples)} ({valid/len(all_samples)*100:.1f}%)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. è³‡æ–™ä¾†æºæ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å»ºç«‹è³‡æ–™ä¾†æºæ–‡ä»¶\nprovenance = {\n    \"download_timestamp\": download_timestamp,\n    \"data_sources\": {\n        \"toi\": {\n            \"source\": \"NASA Exoplanet Archive TOI Table\",\n            \"url\": \"https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=TOI\",\n            \"api_endpoint\": \"https://exoplanetarchive.ipac.caltech.edu/TAP\",\n            \"access_method\": \"astroquery.ipac.nexsci.nasa_exoplanet_archive\",\n            \"n_records\": len(toi_df),\n            \"n_positive\": len(toi_positive),\n            \"n_negative_fp\": len(toi_negative_fp),\n            \"column_mapping\": {\n                \"toi_period\": \"pl_orbper (days)\",\n                \"toi_depth\": \"pl_trandep (ppm)\",\n                \"toi_duration\": \"pl_trandurh (hours, converted to days)\",\n                \"toi_prad\": \"pl_rade (Earth radii)\"\n            },\n            \"columns_available\": list(toi_df.columns)[:20]  # åªåˆ—å‡ºå‰20å€‹æ¬„ä½\n        },\n        \"koi_false_positives\": {\n            \"source\": \"NASA Exoplanet Archive KOI Cumulative Table\",\n            \"url\": \"https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=cumulative\",\n            \"query\": \"WHERE koi_disposition='FALSE POSITIVE'\",\n            \"description\": \"KOI False Positives including eclipsing binaries\",\n            \"n_records\": len(eb_df_processed),\n            \"fallback_source\": \"Kirk et al. (2016) Kepler EB Catalog\",\n            \"columns\": list(eb_df_processed.columns)\n        },\n        \"combined_dataset\": {\n            \"file\": \"supervised_dataset.csv\",\n            \"n_total\": len(all_samples),\n            \"n_positive\": int((all_samples['label'] == 1).sum()),\n            \"n_negative\": int((all_samples['label'] == 0).sum()),\n            \"balance_ratio\": float((all_samples['label'] == 1).sum() / len(all_samples)),\n            \"sources\": {k: int(v) for k, v in all_samples['source'].value_counts().to_dict().items()}\n        }\n    },\n    \"known_issues\": [\n        \"TOI table uses pl_* prefix for physical parameters, not toi_*\",\n        \"pl_trandurh is in hours, requires conversion to days\",\n        \"Villanova EB catalog is inaccessible as of 2025\",\n        \"Many TOI entries have missing physical parameters\",\n        \"Using KOI False Positives as substitute for EB catalog\"\n    ],\n    \"column_definitions\": {\n        \"tfopwg_disp\": \"TFOPWG disposition (PC/CP/KP/FP/APC/FA)\",\n        \"PC\": \"Planet Candidate\",\n        \"CP\": \"Confirmed Planet\",\n        \"KP\": \"Known Planet\",\n        \"FP\": \"False Positive\",\n        \"APC\": \"Ambiguous Planet Candidate\",\n        \"FA\": \"False Alarm\",\n        \"pl_orbper\": \"Planetary orbital period in days\",\n        \"pl_trandep\": \"Transit depth in ppm\",\n        \"pl_trandurh\": \"Transit duration in hours\"\n    },\n    \"references\": [\n        \"NASA Exoplanet Archive: https://exoplanetarchive.ipac.caltech.edu/\",\n        \"TOI Column Definitions: https://exoplanetarchive.ipac.caltech.edu/docs/API_TOI_columns.html\",\n        \"Kirk et al. (2016) AJ 151:68 - Kepler Eclipsing Binary Catalog\",\n        \"Astroquery Documentation: https://astroquery.readthedocs.io/\"\n    ]\n}\n\n# å„²å­˜è³‡æ–™ä¾†æºæ–‡ä»¶\nprovenance_path = data_dir / \"data_provenance.json\"\nwith open(provenance_path, 'w') as f:\n    json.dump(provenance, f, indent=2, default=str)\n\nprint(\"\\nğŸ“ è³‡æ–™ä¾†æºæ–‡ä»¶å·²å»ºç«‹: data/data_provenance.json\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. è³‡æ–™æ‘˜è¦å ±å‘Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*60)\nprint(\"ğŸ“Š è³‡æ–™ä¸‹è¼‰æ‘˜è¦å ±å‘Š\")\nprint(\"=\"*60)\n\nprint(f\"\"\"\nğŸ“… ä¸‹è¼‰æ™‚é–“: {download_timestamp}\n\nğŸ¯ TOI (TESS Objects of Interest) çœŸå¯¦è³‡æ–™:\n   â€¢ ç¸½ç­†æ•¸: {len(toi_df):,}\n   â€¢ æ­£æ¨£æœ¬ (PC/CP/KP): {len(toi_positive):,}\n   â€¢ è² æ¨£æœ¬ (FP): {len(toi_negative_fp):,}\n   â€¢ è³‡æ–™ä¾†æº: NASA Exoplanet Archive (TAP Service)\n   â€¢ æ¬„ä½æ˜ å°„: pl_orbper â†’ toi_period, pl_trandep â†’ toi_depth\n   â€¢ å–®ä½è½‰æ›: pl_trandurh (å°æ™‚) â†’ toi_duration (å¤©)\n\nğŸŒŸ KOI False Positives (æ›¿ä»£ Kepler EB):\n   â€¢ ç¸½ç­†æ•¸: {len(eb_df_processed):,}\n   â€¢ å…¨éƒ¨æ¨™è¨˜ç‚ºè² æ¨£æœ¬ (åŒ…å« eclipsing binaries)\n   â€¢ è³‡æ–™ä¾†æº: NASA Archive KOI Cumulative Table\n   â€¢ æŸ¥è©¢æ¢ä»¶: koi_disposition = 'FALSE POSITIVE'\n   â€¢ å‚™ç”¨ä¾†æº: Kirk et al. (2016) ç¢ºèªçš„ EB ç³»çµ±\n\nğŸ“¦ åˆä½µè¨“ç·´è³‡æ–™é›†:\n   â€¢ ç¸½æ¨£æœ¬æ•¸: {len(all_samples):,}\n   â€¢ æ­£æ¨£æœ¬: {(all_samples['label'] == 1).sum():,} ({(all_samples['label'] == 1).sum()/len(all_samples)*100:.1f}%)\n   â€¢ è² æ¨£æœ¬: {(all_samples['label'] == 0).sum():,} ({(all_samples['label'] == 0).sum()/len(all_samples)*100:.1f}%)\n\n   è³‡æ–™ä¾†æºåˆ†å¸ƒ:\n\"\"\")\n\n# é¡¯ç¤ºè³‡æ–™ä¾†æºåˆ†å¸ƒ\nif 'source' in all_samples.columns:\n    source_counts = all_samples['source'].value_counts()\n    for source, count in source_counts.items():\n        print(f\"   â€¢ {source}: {count:,} ç­†\")\n\nprint(f\"\"\"\n\nğŸ’¾ è¼¸å‡ºæª”æ¡ˆ:\n   â€¢ data/toi.csv - å®Œæ•´ TOI è³‡æ–™ (å« pl_* åŸå§‹æ¬„ä½)\n   â€¢ data/toi_positive.csv - TOI æ­£æ¨£æœ¬ (PC/CP/KP)\n   â€¢ data/toi_negative.csv - TOI è² æ¨£æœ¬ (FP)\n   â€¢ data/koi_false_positives.csv - KOI False Positives (æ›¿ä»£ EB)\n   â€¢ data/supervised_dataset.csv - åˆä½µè¨“ç·´è³‡æ–™é›†\n   â€¢ data/data_provenance.json - è©³ç´°è³‡æ–™ä¾†æºæ–‡ä»¶\n\nâš ï¸ é‡è¦ç™¼ç¾èˆ‡è§£æ±ºæ–¹æ¡ˆ:\n   1. TOI ä½¿ç”¨ pl_* å‰ç¶´è€Œé toi_* (å·²æ˜ å°„è™•ç†)\n   2. pl_trandurh å–®ä½æ˜¯å°æ™‚éœ€è½‰æ› (å·²è™•ç† /24)\n   3. Villanova EB ç›®éŒ„ç„¡æ³•å­˜å– (æ”¹ç”¨ KOI FP)\n   4. éƒ¨åˆ† TOI ç¼ºå°‘ç‰©ç†åƒæ•¸ (éœ€å¾å…‰æ›²ç·šè¨ˆç®—)\n\nğŸ“Š è³‡æ–™å“è³ªè©•ä¼°:\n\"\"\")\n\n# é¡¯ç¤ºè³‡æ–™å®Œæ•´æ€§\nfor col in ['period', 'depth', 'duration']:\n    if col in all_samples.columns:\n        valid_count = all_samples[col].notna().sum()\n        valid_pct = valid_count / len(all_samples) * 100\n        print(f\"   â€¢ {col}: {valid_count:,}/{len(all_samples):,} ({valid_pct:.1f}%) æœ‰æ•ˆå€¼\")\n\nprint(f\"\"\"\n\nğŸš€ ä¸‹ä¸€æ­¥å»ºè­°:\n   1. åŸ·è¡Œ 02_bls_baseline.ipynb è¨ˆç®— BLS/TLS ç‰¹å¾µ\n   2. è‹¥ç‰©ç†åƒæ•¸ä¸è¶³ï¼Œå¾å…‰æ›²ç·šç›´æ¥è¨ˆç®—\n   3. è€ƒæ…®è³‡æ–™å¢å¼·æˆ– SMOTE å¹³è¡¡æ­£è² æ¨£æœ¬\n   4. é©—è­‰è³‡æ–™å“è³ªå¾Œå†è¨“ç·´æ¨¡å‹\n\nâœ… çœŸå¯¦è³‡æ–™ä¸‹è¼‰å®Œæˆï¼æ‰€æœ‰è³‡æ–™ä¾†è‡ª NASA å®˜æ–¹è³‡æ–™åº«ï¼Œç„¡æ¨¡æ“¬è³‡æ–™ï¼\"\"\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 7. è³‡æ–™æŒä¹…åŒ–å„²å­˜ - ä¸€éµæ¨é€åˆ° GitHub\n\nè³‡æ–™ä¸‹è¼‰å®Œæˆå¾Œï¼Œä½¿ç”¨ä¸‹æ–¹çš„**çµ‚æ¥µè§£æ±ºæ–¹æ¡ˆ**ä¸€æ¬¡å®Œæˆæ‰€æœ‰è¨­å®šå’Œæ¨é€ï¼š\n\n### âœ¨ ç‰¹é»ï¼š\n- ğŸ”§ **è‡ªå‹•ç’°å¢ƒæª¢æ¸¬**ï¼šæ™ºèƒ½æª¢æ¸¬ Colab/æœ¬åœ°ç’°å¢ƒä¸¦è‡ªå‹•è¨­å®š\n- ğŸ”— **è‡ªå‹•å€‰åº«åˆå§‹åŒ–**ï¼šè‡ªå‹•è¨­å®š Git å€‰åº«å’Œé ç«¯é€£æ¥\n- ğŸ“¦ **Git LFS è‡ªå‹•è™•ç†**ï¼šè‡ªå‹•è¿½è¹¤å¤§æª”æ¡ˆï¼ˆCSV/JSONï¼‰\n- ğŸ” **å®‰å…¨ Token è¼¸å…¥**ï¼šéš±è—å­—ç¬¦ä¿è­·ä½ çš„ GitHub Token\n- ğŸš€ **ä¸€éµå®Œæˆ**ï¼šToken è¼¸å…¥ â†’ è¨­å®š â†’ æäº¤ â†’ æ¨é€ï¼Œå…¨è‡ªå‹•\n\n### ğŸ¯ ä½¿ç”¨æ–¹æ³•ï¼š\n1. åŸ·è¡Œä¸‹æ–¹ cell è¼‰å…¥åŠŸèƒ½\n2. åŸ·è¡Œ `ultimate_push_to_github()`\n3. è¼¸å…¥ä½ çš„ GitHub Token\n4. ç­‰å¾…è‡ªå‹•å®Œæˆæ‰€æœ‰æ­¥é©Ÿ\n\n**å°±é€™éº¼ç°¡å–®ï¼**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### é¸é … Aï¼šğŸ¯ æ¨é€åˆ° GitHubï¼ˆæ¨è–¦ï¼‰\n\n**é©åˆ**: æƒ³è¦åˆ†äº«è³‡æ–™ã€ç‰ˆæœ¬æ§åˆ¶ã€åœ˜éšŠå”ä½œ\n\n**ç‰¹é»**: \n- âœ… å®‰å…¨çš„ Token è¼¸å…¥ï¼ˆéš±è—å­—ç¬¦ï¼‰\n- âœ… è‡ªå‹•è™•ç†å¤§æª”æ¡ˆï¼ˆGit LFSï¼‰ \n- âœ… å®Œæ•´çš„éŒ¯èª¤è™•ç†å’ŒæŒ‡å°\n- âœ… ä¸€éµå®Œæˆæ‰€æœ‰æ­¥é©Ÿ\n\n**ä½¿ç”¨æ–¹å¼**: åŸ·è¡Œä¸‹æ–¹ cell ä¸­çš„ `quick_push_to_github()`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# ğŸš€ ä¸€éµ GitHub æ¨é€ - çµ‚æ¥µè§£æ±ºæ–¹æ¡ˆ\n",
    "\"\"\"\n",
    "å¾¹åº•è§£æ±ºæ‰€æœ‰ Colab å’Œæœ¬åœ°ç’°å¢ƒçš„ GitHub æ¨é€å•é¡Œ\n",
    "è¼¸å…¥ Token å¾Œè‡ªå‹•å®Œæˆæ‰€æœ‰è¨­å®šå’Œæ¨é€\n",
    "\"\"\"\n",
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "import getpass\n",
    "\n",
    "# æª¢æŸ¥ç’°å¢ƒ\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print(\"ğŸŒ ç’°å¢ƒ: Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"ğŸŒ ç’°å¢ƒ: æœ¬åœ°ç’°å¢ƒ\")\n",
    "\n",
    "def ultimate_push_to_github():\n",
    "    \"\"\"çµ‚æ¥µä¸€éµæ¨é€è§£æ±ºæ–¹æ¡ˆ - è§£æ±ºæ‰€æœ‰å•é¡Œ\"\"\"\n",
    "    print(\"ğŸš€ GitHub è³‡æ–™æ¨é€ - çµ‚æ¥µè§£æ±ºæ–¹æ¡ˆ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # æ­¥é©Ÿ 1: ç²å– GitHub Token\n",
    "    print(\"\\nğŸ” æ­¥é©Ÿ 1: è¼¸å…¥ GitHub Token\")\n",
    "    print(\"ğŸ“ ç²å– Token: https://github.com/settings/tokens/new\")\n",
    "    print(\"ğŸ”‘ æ¬Šé™: å‹¾é¸ 'repo' (Full control of repositories)\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # å„ªå…ˆå¾ Colab Secrets è®€å– GitHub Token\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        token = userdata.get('GITHUB_TOKEN')\n",
    "        print(\"âœ… GitHub Token å·²å¾ Colab Secrets è®€å–\")\n",
    "        print(\"ğŸ’¡ è¨­ç½®æ–¹å¼: Colab å·¦å´æ¬„ ğŸ”‘ Secrets â†’ æ–°å¢ 'GITHUB_TOKEN'\")\n",
    "    except:\n",
    "        # Fallback: æ‰‹å‹•è¼¸å…¥\n",
    "        print(\"â„¹ï¸  æœªåµæ¸¬åˆ° Colab Secretsï¼Œè«‹æ‰‹å‹•è¼¸å…¥ Token\")\n",
    "        try:\n",
    "            token = getpass.getpass(\"è«‹è²¼ä¸Šä½ çš„ GitHub Token (è¼¸å…¥æœƒè¢«éš±è—): \")\n",
    "            if not token:\n",
    "                print(\"âŒ Token ä¸èƒ½ç‚ºç©º\")\n",
    "                return False\n",
    "            print(\"âœ… Token å·²æ¥æ”¶\")\n",
    "        except:\n",
    "                token = input(\"è«‹è²¼ä¸Šä½ çš„ GitHub Token: \")\n",
    "                if not token:\n",
    "                    print(\"âŒ Token ä¸èƒ½ç‚ºç©º\")\n",
    "                    return False\n",
    "\n",
    "    # æ­¥é©Ÿ 2: ç’°å¢ƒè¨­å®šå’Œåˆå§‹åŒ–\n",
    "    print(\"\\nğŸ”§ æ­¥é©Ÿ 2: ç’°å¢ƒè¨­å®š\")\n",
    "    \n",
    "    if IN_COLAB:\n",
    "        # Colab ç’°å¢ƒå®Œæ•´è¨­å®š\n",
    "        print(\"ğŸ“¦ è¨­å®š Colab ç’°å¢ƒ...\")\n",
    "        \n",
    "        # å®‰è£ Git LFS\n",
    "        subprocess.run(['apt-get', 'update', '-qq'], capture_output=True)\n",
    "        subprocess.run(['apt-get', 'install', '-y', '-qq', 'git-lfs'], capture_output=True)\n",
    "        subprocess.run(['git', 'lfs', 'install', '--skip-repo'], capture_output=True)\n",
    "        \n",
    "        # è¨­å®šå·¥ä½œç›®éŒ„\n",
    "        work_dir = Path('/content')\n",
    "        os.chdir(work_dir)\n",
    "        \n",
    "        # æª¢æŸ¥æ˜¯å¦å·²æœ‰å€‰åº«\n",
    "        if not (work_dir / '.git').exists():\n",
    "            print(\"ğŸ”§ åˆå§‹åŒ– Git å€‰åº«...\")\n",
    "            subprocess.run(['git', 'init'], check=True)\n",
    "            subprocess.run(['git', 'config', 'user.email', 'colab@exoplanet.ai'])\n",
    "            subprocess.run(['git', 'config', 'user.name', 'Colab User'])\n",
    "        \n",
    "        # è¨­å®šé ç«¯å€‰åº«\n",
    "        repo_url = \"https://github.com/exoplanet-spaceapps/exoplanet-starter.git\"\n",
    "        \n",
    "        # æª¢æŸ¥æ˜¯å¦å·²æœ‰ origin\n",
    "        result = subprocess.run(['git', 'remote', 'get-url', 'origin'], capture_output=True)\n",
    "        if result.returncode != 0:\n",
    "            print(\"ğŸ”— è¨­å®šé ç«¯å€‰åº«é€£æ¥...\")\n",
    "            subprocess.run(['git', 'remote', 'add', 'origin', repo_url])\n",
    "        else:\n",
    "            # æ›´æ–° origin URL\n",
    "            subprocess.run(['git', 'remote', 'set-url', 'origin', repo_url])\n",
    "        \n",
    "        print(f\"âœ… Colab ç’°å¢ƒè¨­å®šå®Œæˆ\")\n",
    "        print(f\"ğŸ“¡ å€‰åº«: {repo_url}\")\n",
    "        \n",
    "    else:\n",
    "        # æœ¬åœ°ç’°å¢ƒè¨­å®š\n",
    "        work_dir = Path('..').resolve()\n",
    "        print(f\"ğŸ’» æœ¬åœ°ç’°å¢ƒå·¥ä½œç›®éŒ„: {work_dir}\")\n",
    "        \n",
    "        # æª¢æŸ¥ Git å€‰åº«\n",
    "        if not (work_dir / '.git').exists():\n",
    "            print(\"âŒ ä¸åœ¨ Git å€‰åº«ä¸­ï¼Œè«‹ç¢ºä¿åœ¨æ­£ç¢ºçš„å°ˆæ¡ˆç›®éŒ„åŸ·è¡Œ\")\n",
    "            return False\n",
    "    \n",
    "    # æ­¥é©Ÿ 3: æº–å‚™è³‡æ–™å’Œæª”æ¡ˆ\n",
    "    print(\"\\nğŸ“‹ æ­¥é©Ÿ 3: æ¨é€è³‡æ–™åˆ° GitHub\")\n",
    "    print(f\"ğŸ”§ Working directory: {work_dir}\")\n",
    "    \n",
    "    # ç¢ºä¿åœ¨æ­£ç¢ºç›®éŒ„\n",
    "    os.chdir(work_dir)\n",
    "    \n",
    "    # è¨­å®š Git LFS\n",
    "    gitattributes_content = \"\"\"*.csv filter=lfs diff=lfs merge=lfs -text\n",
    "*.json filter=lfs diff=lfs merge=lfs -text\n",
    "*.fits filter=lfs diff=lfs merge=lfs -text\n",
    "*.h5 filter=lfs diff=lfs merge=lfs -text\n",
    "\"\"\"\n",
    "    \n",
    "    with open('.gitattributes', 'w') as f:\n",
    "        f.write(gitattributes_content)\n",
    "    \n",
    "    subprocess.run(['git', 'lfs', 'track', '*.csv'], capture_output=True)\n",
    "    subprocess.run(['git', 'lfs', 'track', '*.json'], capture_output=True)\n",
    "    \n",
    "    # å‰µå»ºå¿…è¦çš„ç›®éŒ„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰\n",
    "    print(\"ğŸ“ æ­¥é©Ÿ 1/4: è¨­å®š Git LFS ä¸¦æ·»åŠ æª”æ¡ˆ\")\n",
    "    essential_dirs = ['data', 'notebooks']\n",
    "    for dir_name in essential_dirs:\n",
    "        dir_path = Path(dir_name)\n",
    "        if not dir_path.exists():\n",
    "            print(f\"   ğŸ“ å‰µå»ºç›®éŒ„: {dir_name}/\")\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # åœ¨ data ç›®éŒ„ä¸­å‰µå»ºä¸€å€‹ README æ–‡ä»¶ï¼Œèªªæ˜ç›®éŒ„ç”¨é€”\n",
    "            if dir_name == 'data':\n",
    "                readme_content = \"\"\"# Data Directory\n",
    "\n",
    "This directory contains exoplanet datasets downloaded from NASA archives:\n",
    "\n",
    "- `toi.csv` - TESS Objects of Interest (TOI) complete dataset\n",
    "- `toi_positive.csv` - TOI positive samples (Planet Candidates)  \n",
    "- `toi_negative.csv` - TOI negative samples (False Positives)\n",
    "- `koi_false_positives.csv` - KOI False Positives (Eclipsing Binaries)\n",
    "- `supervised_dataset.csv` - Combined training dataset\n",
    "- `data_provenance.json` - Data source documentation\n",
    "\n",
    "Generated by exoplanet detection pipeline.\n",
    "\"\"\"\n",
    "                with open(dir_path / 'README.md', 'w') as f:\n",
    "                    f.write(readme_content)\n",
    "                print(f\"   ğŸ“ å‰µå»º {dir_name}/README.md\")\n",
    "        else:\n",
    "            print(f\"   âœ… {dir_name} ç›®éŒ„å·²å­˜åœ¨\")\n",
    "\n",
    "    # æª¢æŸ¥è¦æ·»åŠ çš„æª”æ¡ˆ\n",
    "    files_to_check = [\n",
    "        'data/',\n",
    "        'notebooks/', \n",
    "        'README.md',\n",
    "        'requirements.txt',\n",
    "        'CLAUDE.md',\n",
    "        'DATASETS.md',\n",
    "        '.gitattributes'\n",
    "    ]\n",
    "\n",
    "    added_files = []\n",
    "    for file_path in files_to_check:\n",
    "        if Path(file_path).exists():\n",
    "            result = subprocess.run(['git', 'add', file_path], capture_output=True)\n",
    "            if result.returncode == 0:\n",
    "                added_files.append(file_path)\n",
    "                print(f\"   âœ… å·²æ·»åŠ : {file_path}\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸ è·³éä¸å­˜åœ¨çš„: {file_path}\")\n",
    "\n",
    "    if not added_files:\n",
    "        print(\"âš ï¸ æ²’æœ‰æ‰¾åˆ°è¦æ·»åŠ çš„æª”æ¡ˆ\")\n",
    "        print(\"   æ­£åœ¨å‰µå»ºåŸºæœ¬é …ç›®çµæ§‹...\")\n",
    "\n",
    "        # å‰µå»ºåŸºæœ¬çš„é …ç›®æ–‡ä»¶\n",
    "        basic_files = {\n",
    "            'README.md': \"\"\"# Exoplanet Detection Project\n",
    "\n",
    "AI-powered exoplanet detection using NASA data and machine learning.\n",
    "\n",
    "## Quick Start\n",
    "1. Run `01_tap_download.ipynb` to download data\n",
    "2. Run `02_bls_baseline.ipynb` for BLS analysis  \n",
    "3. Run `03_injection_train.ipynb` for ML training\n",
    "4. Run `04_newdata_inference.ipynb` for inference\n",
    "5. Run `05_metrics_dashboard.ipynb` for evaluation\n",
    "\n",
    "Generated with Claude Code.\n",
    "\"\"\",\n",
    "            '.gitignore': \"\"\"# Python\n",
    "__pycache__/\n",
    "*.py[cod] \n",
    "*$py.class\n",
    "*.egg-info/\n",
    ".pytest_cache/\n",
    "\n",
    "# Jupyter\n",
    ".ipynb_checkpoints/\n",
    "\n",
    "# Data files (handled by Git LFS)\n",
    "*.fits\n",
    "*.h5\n",
    "\n",
    "# IDE\n",
    ".vscode/\n",
    ".idea/\n",
    "\n",
    "# OS\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "\"\"\"\n",
    "        }\n",
    "\n",
    "        for filename, content in basic_files.items():\n",
    "            if not Path(filename).exists():\n",
    "                with open(filename, 'w', encoding='utf-8') as f:\n",
    "                    f.write(content)\n",
    "                subprocess.run(['git', 'add', filename], capture_output=True)\n",
    "                added_files.append(filename)\n",
    "                print(f\"   ğŸ“ å‰µå»ºä¸¦æ·»åŠ : {filename}\")\n",
    "\n",
    "        if not added_files:\n",
    "            print(\"âŒ ä»ç„¡æ³•å‰µå»ºä»»ä½•æª”æ¡ˆ\")\n",
    "            return False\n",
    "    \n",
    "    print(f\"ğŸ“¦ ç¸½å…±æ·»åŠ äº† {len(added_files)} å€‹æª”æ¡ˆ/ç›®éŒ„\")\n",
    "    \n",
    "    # æ­¥é©Ÿ 4: æäº¤è®Šæ›´\n",
    "    print(\"\\nğŸ’¾ æ­¥é©Ÿ 2/4: æäº¤è®Šæ›´\")\n",
    "    \n",
    "    # æª¢æŸ¥æ˜¯å¦æœ‰è®Šæ›´\n",
    "    result = subprocess.run(['git', 'status', '--porcelain'], capture_output=True, text=True)\n",
    "    if not result.stdout.strip():\n",
    "        print(\"â„¹ï¸ æ²’æœ‰è®Šæ›´éœ€è¦æäº¤\")\n",
    "        print(\"âœ… å€‰åº«å·²æ˜¯æœ€æ–°ç‹€æ…‹\")\n",
    "        return True\n",
    "    \n",
    "    # æäº¤è®Šæ›´\n",
    "    commit_message = \"\"\"data: update NASA exoplanet data and analysis\n",
    "\n",
    "- TOI data from NASA Exoplanet Archive with real planetary parameters  \n",
    "- KOI False Positives dataset for negative samples\n",
    "- Complete supervised training dataset ready for ML\n",
    "- Data provenance documentation and quality reports\n",
    "- Updated notebooks with improved functionality\n",
    "\n",
    "\n",
    "Co-Authored-By: hctsai1006 <39769660@cuni.cz>\"\"\"\n",
    "    \n",
    "    result = subprocess.run(['git', 'commit', '-m', commit_message], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"âœ… è®Šæ›´å·²æäº¤\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ æäº¤è­¦å‘Š: {result.stderr}\")\n",
    "    \n",
    "    # æ­¥é©Ÿ 5: æ¨é€åˆ° GitHub  \n",
    "    print(\"\\nğŸš€ æ­¥é©Ÿ 3/4: æ¨é€åˆ° GitHub\")\n",
    "    \n",
    "    # å–å¾—é ç«¯ URL\n",
    "    result = subprocess.run(['git', 'remote', 'get-url', 'origin'], capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        print(\"âŒ ç„¡æ³•å–å¾—é ç«¯å€‰åº« URL\")\n",
    "        return False\n",
    "    \n",
    "    origin_url = result.stdout.strip()\n",
    "    print(f\"ğŸ“¡ ç›®æ¨™å€‰åº«: {origin_url}\")\n",
    "    \n",
    "    # å»ºç«‹èªè­‰ URL\n",
    "    if 'github.com' in origin_url:\n",
    "        auth_url = origin_url.replace('https://github.com/', f'https://{token}@github.com/')\n",
    "    else:\n",
    "        print(\"âŒ åªæ”¯æ´ GitHub å€‰åº«\")\n",
    "        return False\n",
    "    \n",
    "    # æ¨é€\n",
    "    result = subprocess.run(['git', 'push', auth_url, 'HEAD:main'], capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"ğŸ‰ æ¨é€æˆåŠŸï¼\")\n",
    "        \n",
    "        # é©—è­‰æ¨é€ç‹€æ…‹\n",
    "        subprocess.run(['git', 'status'], capture_output=True)\n",
    "        print(\"âœ… å€‰åº«ç‹€æ…‹å·²åŒæ­¥\")\n",
    "        \n",
    "        print(\"\\nğŸ“ æ­¥é©Ÿ 4/4: å®Œæˆ\")\n",
    "        print(\"   1. å‰å¾€ GitHub æŸ¥çœ‹ä½ çš„å€‰åº«\")\n",
    "        print(\"   2. æª¢æŸ¥ data/ ç›®éŒ„ä¸‹çš„æª”æ¡ˆ\")  \n",
    "        print(\"   3. å¯ä»¥åŸ·è¡Œå…¶ä»– notebook ç¹¼çºŒåˆ†æ\")\n",
    "        print(f\"   4. å€‰åº«é€£çµ: {origin_url}\")\n",
    "        \n",
    "        return True\n",
    "    else:\n",
    "        print(f\"âŒ æ¨é€å¤±æ•—: {result.stderr}\")\n",
    "        \n",
    "        # å˜—è©¦è§£æ±ºè¡çª\n",
    "        if 'fetch first' in result.stderr or 'non-fast-forward' in result.stderr:\n",
    "            print(\"ğŸ”§ å˜—è©¦è§£æ±ºç‰ˆæœ¬è¡çª...\")\n",
    "            subprocess.run(['git', 'pull', '--rebase', auth_url, 'main'], capture_output=True)\n",
    "            \n",
    "            # é‡è©¦æ¨é€\n",
    "            result = subprocess.run(['git', 'push', auth_url, 'HEAD:main'], capture_output=True, text=True)\n",
    "            if result.returncode == 0:\n",
    "                print(\"ğŸ‰ è§£æ±ºè¡çªå¾Œæ¨é€æˆåŠŸï¼\")\n",
    "                return True\n",
    "        \n",
    "        print(\"ğŸ’¡ å¸¸è¦‹è§£æ±ºæ–¹æ¡ˆ:\")\n",
    "        print(\"   - ç¢ºä¿ Token æœ‰ 'repo' æ¬Šé™\")  \n",
    "        print(\"   - æª¢æŸ¥ç¶²è·¯é€£æ¥\")\n",
    "        print(\"   - ç¢ºèªå€‰åº«å­˜åœ¨ä¸”æœ‰å¯«å…¥æ¬Šé™\")\n",
    "        return False\n",
    "\n",
    "# é¡¯ç¤ºä½¿ç”¨èªªæ˜\n",
    "print(\"ğŸ¯ GitHub æ¨é€çµ‚æ¥µè§£æ±ºæ–¹æ¡ˆå·²è¼‰å…¥ï¼\")\n",
    "print(\"\")\n",
    "print(\"âœ¨ ç‰¹é»:\")\n",
    "print(\"   - è‡ªå‹•æª¢æ¸¬ä¸¦è¨­å®š Colab/æœ¬åœ°ç’°å¢ƒ\")\n",
    "print(\"   - è‡ªå‹•åˆå§‹åŒ– Git å€‰åº«å’Œé ç«¯é€£æ¥\")  \n",
    "print(\"   - è‡ªå‹•è¨­å®š Git LFS è™•ç†å¤§æª”æ¡ˆ\")\n",
    "print(\"   - ä¸€æ¬¡å®Œæˆ Token è¼¸å…¥ã€æäº¤ã€æ¨é€\")\n",
    "print(\"   - æ™ºèƒ½éŒ¯èª¤è™•ç†å’Œè¡çªè§£æ±º\")\n",
    "print(\"\")\n",
    "print(\"ğŸš€ ä½¿ç”¨æ–¹æ³•:\")\n",
    "print(\"   ultimate_push_to_github()\")\n",
    "print(\"\")\n",
    "print(\"âš ï¸ æº–å‚™å·¥ä½œ:\")\n",
    "print(\"   1. å–å¾— GitHub Personal Access Token\")\n",
    "print(\"   2. ç¢ºä¿ Token æœ‰ 'repo' æ¬Šé™\")\n",
    "print(\"   3. åŸ·è¡Œä¸Šè¿°å‡½æ•¸ä¸¦è·Ÿéš¨æŒ‡ç¤º\")\n",
    "\n",
    "# å¿«é€ŸåŸ·è¡Œï¼ˆå–æ¶ˆè¨»è§£ä½¿ç”¨ï¼‰\n",
    "# ultimate_push_to_github()  # å–æ¶ˆè¨»è§£ä¾†ä¸€éµæ¨é€\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### é¸é … Cï¼šâœ… é©—è­‰è³‡æ–™å®Œæ•´æ€§\n\n**é©åˆ**: æª¢æŸ¥è³‡æ–™å“è³ªã€ç¢ºèªä¸‹è¼‰æˆåŠŸ\n**ä½¿ç”¨**: åŸ·è¡Œ `verify_data_integrity()` é€²è¡Œå¿«é€Ÿé©—è­‰",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# é¸é … Cï¼šé©—è­‰è³‡æ–™å®Œæ•´æ€§\n\"\"\"\né©—è­‰ä¸‹è¼‰çš„è³‡æ–™æ˜¯å¦æ­£ç¢ºå„²å­˜\n\"\"\"\nfrom pathlib import Path\nimport pandas as pd\nimport json\n\ndef verify_data_integrity():\n    \"\"\"é©—è­‰æ‰€æœ‰è³‡æ–™æª”æ¡ˆçš„å®Œæ•´æ€§\"\"\"\n    print(\"ğŸ” é©—è­‰è³‡æ–™å®Œæ•´æ€§...\")\n    \n    data_dir = Path('../data')\n    \n    # æª¢æŸ¥å¿…è¦æª”æ¡ˆ\n    required_files = {\n        'toi.csv': 'TOI å®Œæ•´è³‡æ–™',\n        'toi_positive.csv': 'TOI æ­£æ¨£æœ¬',\n        'toi_negative.csv': 'TOI è² æ¨£æœ¬',\n        'koi_false_positives.csv': 'KOI False Positives',\n        'supervised_dataset.csv': 'åˆä½µè¨“ç·´è³‡æ–™é›†',\n        'data_provenance.json': 'è³‡æ–™ä¾†æºæ–‡ä»¶'\n    }\n    \n    missing_files = []\n    file_info = []\n    \n    for filename, description in required_files.items():\n        file_path = data_dir / filename\n        if file_path.exists():\n            size_mb = file_path.stat().st_size / 1024 / 1024\n            \n            if filename.endswith('.csv'):\n                try:\n                    df = pd.read_csv(file_path)\n                    rows = len(df)\n                    cols = len(df.columns)\n                    file_info.append({\n                        'file': filename,\n                        'desc': description,\n                        'rows': rows,\n                        'cols': cols,\n                        'size_mb': size_mb\n                    })\n                    print(f\"   âœ… {filename}: {rows:,} ç­†, {cols} æ¬„ä½, {size_mb:.2f} MB\")\n                except Exception as e:\n                    print(f\"   âŒ {filename}: è®€å–å¤±æ•— - {e}\")\n            else:\n                file_info.append({\n                    'file': filename,\n                    'desc': description,\n                    'size_mb': size_mb\n                })\n                print(f\"   âœ… {filename}: {size_mb:.2f} MB\")\n        else:\n            missing_files.append(filename)\n            print(f\"   âŒ {filename}: æª”æ¡ˆä¸å­˜åœ¨\")\n    \n    # è¼‰å…¥ä¸¦é©—è­‰åˆä½µè³‡æ–™é›†\n    if (data_dir / 'supervised_dataset.csv').exists():\n        print(\"\\nğŸ“Š åˆä½µè³‡æ–™é›†åˆ†æ:\")\n        combined_df = pd.read_csv(data_dir / 'supervised_dataset.csv')\n        \n        # æ¨™ç±¤åˆ†å¸ƒ\n        label_counts = combined_df['label'].value_counts()\n        print(f\"   æ­£æ¨£æœ¬ (label=1): {label_counts.get(1, 0):,} ç­†\")\n        print(f\"   è² æ¨£æœ¬ (label=0): {label_counts.get(0, 0):,} ç­†\")\n        print(f\"   å¹³è¡¡åº¦: {label_counts.get(1, 0) / len(combined_df) * 100:.1f}% vs {label_counts.get(0, 0) / len(combined_df) * 100:.1f}%\")\n        \n        # è³‡æ–™ä¾†æºåˆ†å¸ƒ\n        if 'source' in combined_df.columns:\n            print(\"\\n   è³‡æ–™ä¾†æº:\")\n            for source, count in combined_df['source'].value_counts().items():\n                print(f\"   - {source}: {count:,} ç­†\")\n        \n        # è³‡æ–™å®Œæ•´æ€§\n        print(\"\\n   ç‰©ç†åƒæ•¸å®Œæ•´æ€§:\")\n        for col in ['period', 'depth', 'duration']:\n            if col in combined_df.columns:\n                valid = combined_df[col].notna().sum()\n                pct = valid / len(combined_df) * 100\n                print(f\"   - {col}: {pct:.1f}% å®Œæ•´\")\n    \n    # ç¸½çµ\n    if missing_files:\n        print(f\"\\nâš ï¸ ç¼ºå°‘ {len(missing_files)} å€‹æª”æ¡ˆ\")\n        return False\n    else:\n        print(\"\\nâœ… æ‰€æœ‰è³‡æ–™æª”æ¡ˆå®Œæ•´ç„¡ç¼ºï¼\")\n        return True\n\n# åŸ·è¡Œé©—è­‰\nis_valid = verify_data_integrity()\n\nif is_valid:\n    print(\"\\nğŸ‰ è³‡æ–™æº–å‚™å°±ç·’ï¼Œå¯ä»¥é€²è¡Œä¸‹ä¸€æ­¥åˆ†æï¼\")\n    print(\"   å»ºè­°åŸ·è¡Œ 02_bls_baseline.ipynb\")\nelse:\n    print(\"\\nâš ï¸ è«‹é‡æ–°åŸ·è¡Œä¸Šæ–¹çš„è³‡æ–™ä¸‹è¼‰ç¨‹å¼ç¢¼\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}