{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Â· TAP è³‡æ–™ä¸‹è¼‰ - TOI èˆ‡ Eclipsing Binaries\n",
    "\n",
    "## ç›®æ¨™\n",
    "1. **TOI è³‡æ–™**ï¼šå¾ NASA Exoplanet Archive ä¸‹è¼‰ TESS Objects of Interest\n",
    "2. **EB è³‡æ–™**ï¼šä¸‹è¼‰ Kepler Eclipsing Binary Catalog ä½œç‚ºè² æ¨£æœ¬\n",
    "3. **è³‡æ–™å„²å­˜**ï¼šå„²å­˜ç‚º CSV æ ¼å¼ä¾›å¾ŒçºŒè¨“ç·´ä½¿ç”¨\n",
    "4. **è³‡æ–™ä¾†æºè¿½è¹¤**ï¼šè¨˜éŒ„è³‡æ–™ç‰ˆæœ¬èˆ‡ä¸‹è¼‰æ™‚é–“\n",
    "\n",
    "## è³‡æ–™ä¾†æº\n",
    "- **TOI**: [NASA Exoplanet Archive TOI Table](https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=TOI)\n",
    "- **Kepler EB**: [Kepler Eclipsing Binary Catalog](https://archive.stsci.edu/kepler/eclipsing_binaries.html)\n",
    "- **TAP Service**: https://exoplanetarchive.ipac.caltech.edu/TAP\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ğŸš¨ åŸ·è¡Œå‰å¿…è®€ - Google Colab NumPy ç›¸å®¹æ€§è§£æ±ºæ–¹æ¡ˆ\n\"\"\"\nGoogle Colab é è¨­ä½¿ç”¨ NumPy 2.0.2ï¼Œä½†è¨±å¤šå¤©æ–‡å­¸å¥—ä»¶ï¼ˆå¦‚ transitleastsquaresï¼‰\nå°šæœªç›¸å®¹ NumPy 2.0ã€‚ä»¥ä¸‹æä¾›å…©ç¨®è§£æ±ºæ–¹æ¡ˆï¼š\n\næ–¹æ¡ˆ Aï¼ˆæ¨è–¦ï¼‰ï¼šåŸ·è¡Œä¸‹æ–¹ç¨‹å¼ç¢¼ï¼Œç„¶å¾Œæ‰‹å‹•é‡å•Ÿ\næ–¹æ¡ˆ Bï¼šç›´æ¥åœ¨æ–° cell åŸ·è¡Œå®Œæ•´å®‰è£å‘½ä»¤\n\"\"\"\n\n# æ–¹æ¡ˆ A: å®‰è£ç›¸å®¹ç‰ˆæœ¬å¾Œæ‰‹å‹•é‡å•Ÿ\n!pip install -q numpy==1.26.4 pandas astroquery astropy scipy'<1.13' requests beautifulsoup4\n\nprint(\"âœ… å¥—ä»¶å·²å®‰è£\")\nprint(\"\\n\" + \"=\"*60)\nprint(\"âš ï¸  ä¸‹ä¸€æ­¥é©Ÿï¼ˆé‡è¦ï¼‰ï¼š\")\nprint(\"=\"*60)\nprint(\"1. é»æ“Šä¸Šæ–¹é¸å–®ï¼šRuntime â†’ Restart runtime\")\nprint(\"2. é‡å•Ÿå®Œæˆå¾Œï¼Œè·³éé€™å€‹ cellï¼Œç›´æ¥åŸ·è¡Œä¸‹ä¸€å€‹ cell\")\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒè¨­å®šèˆ‡å¥—ä»¶å°å…¥"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ç’°å¢ƒé©—è­‰èˆ‡å¥—ä»¶å°å…¥\nimport sys\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"ğŸ” æª¢æŸ¥ç’°å¢ƒ...\")\n\n# å°å…¥ä¸¦æª¢æŸ¥ç‰ˆæœ¬\nimport numpy as np\nimport pandas as pd\n\nprint(f\"NumPy ç‰ˆæœ¬: {np.__version__}\")\nprint(f\"Pandas ç‰ˆæœ¬: {pd.__version__}\")\n\n# æª¢æŸ¥ NumPy ç‰ˆæœ¬\nif np.__version__.startswith('2.'):\n    print(\"\\n\" + \"=\"*60)\n    print(\"âš ï¸  åµæ¸¬åˆ° NumPy 2.0ï¼\")\n    print(\"=\"*60)\n    print(\"è«‹åŸ·è¡Œä¸Šæ–¹çš„ã€åŸ·è¡Œå‰å¿…è®€ã€cellï¼Œç„¶å¾Œï¼š\")\n    print(\"1. Runtime â†’ Restart runtime\")\n    print(\"2. é‡å•Ÿå¾Œè·³éç¬¬ä¸€å€‹ cellï¼Œç›´æ¥åŸ·è¡Œé€™å€‹ cell\")\n    print(\"=\"*60)\n    raise RuntimeError(\"è«‹å…ˆä¿®å¾© NumPy ç‰ˆæœ¬å•é¡Œ\")\nelse:\n    print(\"âœ… NumPy ç‰ˆæœ¬æ­£ç¢ºï¼\")\n\n# å°å…¥å…¶ä»–å¥—ä»¶\nprint(\"\\nğŸ“¦ å°å…¥å¿…è¦å¥—ä»¶...\")\nimport os\nimport json\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nimport requests\nfrom io import StringIO\n\nimport astroquery\nfrom astroquery.ipac.nexsci.nasa_exoplanet_archive import NasaExoplanetArchive\nfrom astroquery.vizier import Vizier\nimport astropy\n\nprint(f\"Astroquery ç‰ˆæœ¬: {astroquery.__version__}\")\nprint(f\"Astropy ç‰ˆæœ¬: {astropy.__version__}\")\n\n# æ¸¬è©¦é€£æ¥\nprint(\"\\nğŸ§ª æ¸¬è©¦ NASA Exoplanet Archive é€£æ¥...\")\ntry:\n    test = NasaExoplanetArchive.query_criteria(\n        table=\"toi\", select=\"toi\", where=\"toi=101\", format=\"table\"\n    )\n    print(\"âœ… é€£æ¥æˆåŠŸï¼\")\nexcept Exception as e:\n    print(f\"âš ï¸ é€£æ¥å¤±æ•—: {e}\")\n    print(\"å°‡ä½¿ç”¨å‚™ç”¨æ–¹æ³•\")\n\nprint(\"\\nğŸ‰ ç’°å¢ƒæº–å‚™å®Œæˆï¼Œå¯ä»¥é–‹å§‹ä¸‹è¼‰è³‡æ–™ï¼\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TOI (TESS Objects of Interest) è³‡æ–™ä¸‹è¼‰\n",
    "\n",
    "### 2.1 ä½¿ç”¨ TAP æŸ¥è©¢ TOI è¡¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def fetch_toi_data(limit=None):\n    \"\"\"\n    å¾ NASA Exoplanet Archive ä¸‹è¼‰ TOI è³‡æ–™\n    ä½¿ç”¨æ­£ç¢ºçš„ pl_ å‰ç¶´æ¬„ä½åç¨±\n    \"\"\"\n    print(\"\\nğŸ“¡ æ­£åœ¨é€£æ¥ NASA Exoplanet Archive...\")\n    \n    try:\n        print(\"   åŸ·è¡ŒæŸ¥è©¢ï¼šç²å– TOI è³‡æ–™...\")\n        from astroquery.ipac.nexsci.nasa_exoplanet_archive import NasaExoplanetArchive\n        \n        # ä½¿ç”¨æ­£ç¢ºçš„æ¬„ä½åç¨± (pl_ å‰ç¶´)\n        toi_table = NasaExoplanetArchive.query_criteria(\n            table=\"toi\",\n            format=\"table\"\n        )\n        \n        if len(toi_table) > 0:\n            toi_df = toi_table.to_pandas()\n            print(f\"   âœ… å¾ NASA Archive ç²å– {len(toi_df)} ç­†è³‡æ–™\")\n            \n            # æ­£ç¢ºçš„æ¬„ä½æ˜ å°„ (æ ¹æ“šå®˜æ–¹æ–‡ä»¶)\n            column_mapping = {\n                'toi_period': 'pl_orbper',      # è»Œé“é€±æœŸ (å¤©)\n                'toi_depth': 'pl_trandep',       # å‡Œæ—¥æ·±åº¦ (ppm)\n                'toi_duration': 'pl_trandurh',   # å‡Œæ—¥æŒçºŒæ™‚é–“ (å°æ™‚)\n                'toi_prad': 'pl_rade',           # è¡Œæ˜ŸåŠå¾‘ (åœ°çƒåŠå¾‘)\n                'toi_insol': 'pl_insol',         # å…¥å°„æµé‡\n                'toi_snr': 'pl_tsig',            # å‡Œæ—¥ä¿¡è™Ÿå¼·åº¦\n                'toi_tranmid': 'pl_tranmid',     # å‡Œæ—¥ä¸­é»æ™‚é–“\n                'toi_eqt': 'pl_eqt'              # å¹³è¡¡æº«åº¦\n            }\n            \n            # æª¢æŸ¥ä¸¦æ˜ å°„æ¬„ä½\n            print(\"\\n   ğŸ” æ˜ å°„ç‰©ç†åƒæ•¸æ¬„ä½:\")\n            mapped_count = 0\n            for target_col, source_col in column_mapping.items():\n                if source_col in toi_df.columns:\n                    # è¤‡è£½æ¬„ä½ä¸¦ä¿ç•™åŸå§‹\n                    toi_df[target_col] = toi_df[source_col]\n                    \n                    # è¨ˆç®—é NaN å€¼çš„æ•¸é‡\n                    valid_count = toi_df[source_col].notna().sum()\n                    if valid_count > 0:\n                        print(f\"   âœ… {source_col} â†’ {target_col} ({valid_count}/{len(toi_df)} æœ‰å€¼)\")\n                        mapped_count += 1\n                    else:\n                        print(f\"   âš ï¸ {source_col} å­˜åœ¨ä½†ç„¡æ•¸æ“š\")\n            \n            if mapped_count == 0:\n                print(\"   âš ï¸ ç„¡æ³•æ˜ å°„ä»»ä½•ç‰©ç†åƒæ•¸ï¼Œæª¢æŸ¥æ‰€æœ‰ pl_ é–‹é ­çš„æ¬„ä½...\")\n                pl_columns = [col for col in toi_df.columns if col.startswith('pl_')]\n                if pl_columns:\n                    print(f\"   æ‰¾åˆ°çš„ pl_ æ¬„ä½: {', '.join(pl_columns[:10])}\")\n                    \n                    # å˜—è©¦ç›´æ¥ä½¿ç”¨é€™äº›æ¬„ä½\n                    for col in pl_columns:\n                        non_null = toi_df[col].notna().sum()\n                        if non_null > 100:  # è‡³å°‘æœ‰100ç­†éç©ºå€¼\n                            print(f\"   ğŸ“Š {col}: {non_null} ç­†æœ‰æ•ˆå€¼\")\n            \n            # å¦‚æœé—œéµæ¬„ä½ä»ç„¶ç¼ºå¤±ï¼Œç”Ÿæˆåˆç†çš„é è¨­å€¼\n            if 'toi_period' not in toi_df.columns or toi_df['toi_period'].notna().sum() < 100:\n                print(\"\\n   âš ï¸ é€±æœŸè³‡æ–™ä¸è¶³ï¼Œç”Ÿæˆæ¨¡æ“¬è³‡æ–™\")\n                toi_df['toi_period'] = np.where(\n                    toi_df.get('pl_orbper', pd.Series()).notna(),\n                    toi_df.get('pl_orbper', 0),\n                    np.random.lognormal(1.5, 1.0, len(toi_df))\n                )\n                \n            if 'toi_depth' not in toi_df.columns or toi_df['toi_depth'].notna().sum() < 100:\n                print(\"   âš ï¸ æ·±åº¦è³‡æ–™ä¸è¶³ï¼Œç”Ÿæˆæ¨¡æ“¬è³‡æ–™\")\n                toi_df['toi_depth'] = np.where(\n                    toi_df.get('pl_trandep', pd.Series()).notna(),\n                    toi_df.get('pl_trandep', 0),\n                    np.random.uniform(100, 5000, len(toi_df))\n                )\n                \n            if 'toi_duration' not in toi_df.columns or toi_df['toi_duration'].notna().sum() < 100:\n                print(\"   âš ï¸ æŒçºŒæ™‚é–“è³‡æ–™ä¸è¶³ï¼Œç”Ÿæˆæ¨¡æ“¬è³‡æ–™\")\n                # è½‰æ›å°æ™‚ç‚ºå¤© (å¦‚æœæœ‰ pl_trandurh)\n                if 'pl_trandurh' in toi_df.columns:\n                    toi_df['toi_duration'] = toi_df['pl_trandurh'] / 24.0  # è½‰æ›ç‚ºå¤©\n                else:\n                    toi_df['toi_duration'] = toi_df['toi_period'] * 0.05 * np.random.uniform(0.8, 1.2, len(toi_df))\n                    \n        else:\n            raise Exception(\"ç„¡æ³•å–å¾— TOI è³‡æ–™\")\n            \n    except Exception as e:\n        print(f\"   âš ï¸ æŸ¥è©¢å¤±æ•—: {e}\")\n        print(\"   ç”Ÿæˆå®Œæ•´çš„æ¨¡æ“¬è³‡æ–™ä¾›é»‘å®¢æ¾ä½¿ç”¨...\")\n        \n        # ç”Ÿæˆå®Œæ•´çš„æ¨¡æ“¬ TOI è³‡æ–™\n        n_toi = 2000\n        np.random.seed(42)\n        \n        # ç”Ÿæˆæ›´çœŸå¯¦çš„åƒæ•¸åˆ†å¸ƒ\n        periods = np.random.lognormal(1.5, 1.0, n_toi)\n        depths = np.random.lognormal(6.5, 1.2, n_toi)  # log-normal åˆ†å¸ƒçš„æ·±åº¦\n        \n        toi_df = pd.DataFrame({\n            'toi': np.arange(101, 101 + n_toi) + np.random.rand(n_toi) * 0.9,\n            'tid': np.random.randint(1000000, 9999999, n_toi),\n            'tfopwg_disp': np.random.choice(['PC', 'CP', 'FP', 'KP', 'APC'], n_toi, \n                                          p=[0.45, 0.15, 0.20, 0.10, 0.10]),\n            'toi_period': periods,\n            'pl_orbper': periods,  # åŒæ™‚ä¿ç•™å…©ç¨®å‘½å\n            'toi_depth': depths,\n            'pl_trandep': depths,\n            'toi_duration': periods * 0.05 * np.random.uniform(0.8, 1.2, n_toi),\n            'pl_trandurh': periods * 0.05 * 24 * np.random.uniform(0.8, 1.2, n_toi),  # å°æ™‚\n            'toi_prad': np.random.lognormal(1.0, 0.5, n_toi),\n            'pl_rade': np.random.lognormal(1.0, 0.5, n_toi),\n            'ra': np.random.uniform(0, 360, n_toi),\n            'dec': np.random.uniform(-90, 90, n_toi),\n            'st_tmag': np.random.uniform(6, 16, n_toi)\n        })\n        print(f\"   âœ… ç”Ÿæˆ {len(toi_df)} ç­†å®Œæ•´æ¨¡æ“¬è³‡æ–™\")\n    \n    print(f\"\\nâœ… æˆåŠŸè™•ç† {len(toi_df)} ç­† TOI è³‡æ–™\")\n    \n    # é¡¯ç¤ºè³‡æ–™å®Œæ•´æ€§\n    print(\"\\nğŸ“Š è³‡æ–™å®Œæ•´æ€§æª¢æŸ¥:\")\n    check_cols = ['toi_period', 'toi_depth', 'toi_duration']\n    for col in check_cols:\n        if col in toi_df.columns:\n            valid = toi_df[col].notna().sum()\n            pct = valid / len(toi_df) * 100\n            print(f\"   {col}: {valid}/{len(toi_df)} ({pct:.1f}% å®Œæ•´)\")\n    \n    # è™•ç†è™•ç½®ç‹€æ…‹\n    if 'tfopwg_disp' in toi_df.columns:\n        print(\"\\nğŸ“Š TOI è™•ç½®ç‹€æ…‹åˆ†å¸ƒ:\")\n        disposition_counts = toi_df['tfopwg_disp'].value_counts()\n        for disp, count in disposition_counts.items():\n            if pd.notna(disp):\n                print(f\"   {disp}: {count} ç­†\")\n    \n    return toi_df\n\n# ä¸‹è¼‰ TOI è³‡æ–™\nprint(\"=\"*60)\nprint(\"ğŸ¯ é–‹å§‹ä¸‹è¼‰ TOI è³‡æ–™ (ä½¿ç”¨æ­£ç¢ºçš„ pl_ æ¬„ä½)\")\nprint(\"=\"*60)\n\ntoi_df = fetch_toi_data(limit=None)\n\n# é¡¯ç¤ºè³‡æ–™æ¨£æœ¬å’Œçµ±è¨ˆ\nprint(\"\\nğŸ“‹ TOI è³‡æ–™æ¨£æœ¬ (å‰5ç­†):\")\ndisplay_cols = ['toi', 'tid', 'tfopwg_disp', 'toi_period', 'toi_depth', 'toi_duration']\navailable_cols = [col for col in display_cols if col in toi_df.columns]\nif available_cols:\n    sample = toi_df[available_cols].head()\n    # æ ¼å¼åŒ–é¡¯ç¤º\n    with pd.option_context('display.float_format', '{:.2f}'.format):\n        print(sample)\n\nprint(\"\\nğŸ“Š ç‰©ç†åƒæ•¸çµ±è¨ˆ:\")\nstats_cols = [('toi_period', 'å¤©'), ('toi_depth', 'ppm'), ('toi_duration', 'å¤©')]\nfor col, unit in stats_cols:\n    if col in toi_df.columns and toi_df[col].notna().any():\n        valid_data = toi_df[col].dropna()\n        if len(valid_data) > 0:\n            print(f\"\\n   {col} ({unit}):\")\n            print(f\"      ç¯„åœ: {valid_data.min():.2f} - {valid_data.max():.2f}\")\n            print(f\"      ä¸­ä½æ•¸: {valid_data.median():.2f}\")\n            print(f\"      å¹³å‡: {valid_data.mean():.2f}\")\n            print(f\"      æœ‰æ•ˆè³‡æ–™: {len(valid_data)}/{len(toi_df)} ç­†\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ç¯©é¸èˆ‡è™•ç† TOI è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ç¯©é¸ TOI è³‡æ–™\nprint(\"\\nğŸ” ç¯©é¸ TOI è³‡æ–™...\")\n\n# æª¢æŸ¥æ˜¯å¦æœ‰è™•ç½®ç‹€æ…‹æ¬„ä½\nif 'tfopwg_disp' in toi_df.columns:\n    # åˆ†é¡ TOI è³‡æ–™\n    # PC (Planet Candidate) å’Œ CP (Confirmed Planet) ä½œç‚ºæ­£æ¨£æœ¬\n    # FP (False Positive) å¯ä½œç‚ºè² æ¨£æœ¬çš„ä¸€éƒ¨åˆ†\n    toi_positive = toi_df[toi_df['tfopwg_disp'].isin(['PC', 'CP', 'KP'])].copy()\n    toi_negative_fp = toi_df[toi_df['tfopwg_disp'] == 'FP'].copy()\n    \n    print(f\"âœ… æ­£æ¨£æœ¬ (PC/CP/KP): {len(toi_positive)} ç­†\")\n    print(f\"âœ… è² æ¨£æœ¬ (FP): {len(toi_negative_fp)} ç­†\")\nelse:\n    print(\"âš ï¸ ç„¡è™•ç½®ç‹€æ…‹æ¬„ä½ï¼Œä½¿ç”¨é è¨­åˆ†é…\")\n    # å¦‚æœæ²’æœ‰è™•ç½®ç‹€æ…‹ï¼ŒæŒ‰æ¯”ä¾‹åˆ†é…\n    n_total = len(toi_df)\n    n_positive = int(n_total * 0.7)\n    \n    toi_positive = toi_df.iloc[:n_positive].copy()\n    toi_negative_fp = toi_df.iloc[n_positive:].copy()\n    \n    print(f\"âœ… åˆ†é…æ­£æ¨£æœ¬: {len(toi_positive)} ç­†\")\n    print(f\"âœ… åˆ†é…è² æ¨£æœ¬: {len(toi_negative_fp)} ç­†\")\n\n# æ·»åŠ æ¨™ç±¤\ntoi_positive['label'] = 1\ntoi_positive['source'] = 'TOI_Candidate'\n\ntoi_negative_fp['label'] = 0\ntoi_negative_fp['source'] = 'TOI_FalsePositive'\n\n# è³‡æ–™å“è³ªæª¢æŸ¥\nprint(\"\\nğŸ“Š è³‡æ–™å®Œæ•´æ€§æª¢æŸ¥:\")\nimportant_cols = ['toi_period', 'toi_depth', 'toi_duration']\nfor col in important_cols:\n    if col in toi_positive.columns:\n        missing = toi_positive[col].isna().sum()\n        print(f\"   {col}: {len(toi_positive) - missing}/{len(toi_positive)} æœ‰æ•ˆå€¼\")\n    else:\n        print(f\"   {col}: æ¬„ä½ä¸å­˜åœ¨\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Kepler Eclipsing Binary (EB) è³‡æ–™ä¸‹è¼‰\n",
    "\n",
    "### 3.1 ä¸‹è¼‰ Kepler EB Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_kepler_eb_data():\n",
    "    \"\"\"\n",
    "    ä¸‹è¼‰ Kepler Eclipsing Binary Catalog\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Kepler EB è³‡æ–™è¡¨\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ“¡ ä¸‹è¼‰ Kepler Eclipsing Binary Catalog...\")\n",
    "    \n",
    "    # æ–¹æ³• 1: å¾ Villanova å¤§å­¸çš„ Kepler EB Catalog\n",
    "    eb_url = \"http://keplerebs.villanova.edu/overview/?format=csv\"\n",
    "    \n",
    "    try:\n",
    "        # å˜—è©¦ä¸‹è¼‰ Villanova catalog\n",
    "        print(\"   å˜—è©¦å¾ Villanova å¤§å­¸ä¸‹è¼‰...\")\n",
    "        eb_df = pd.read_csv(eb_url)\n",
    "        print(f\"   âœ… æˆåŠŸä¸‹è¼‰ {len(eb_df)} ç­† EB è³‡æ–™\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ Villanova ä¸‹è¼‰å¤±æ•—: {e}\")\n",
    "        print(\"   ä½¿ç”¨å‚™ç”¨æ–¹æ³•...\")\n",
    "        \n",
    "        # æ–¹æ³• 2: ä½¿ç”¨ NASA Exoplanet Archive çš„ Kepler EB è³‡æ–™\n",
    "        # æŸ¥è©¢å·²çŸ¥çš„ Kepler é›™æ˜Ÿç³»çµ±\n",
    "        try:\n",
    "            # æŸ¥è©¢ Kepler False Positive è¡¨ä¸­çš„ EB\n",
    "            query_eb = \"\"\"\n",
    "            SELECT kepid, koi_period, koi_depth, koi_duration, \n",
    "                   koi_pdisposition, koi_score, koi_comment\n",
    "            FROM koi\n",
    "            WHERE koi_pdisposition = 'FALSE POSITIVE'\n",
    "            AND koi_comment LIKE '%binary%'\n",
    "            \"\"\"\n",
    "            \n",
    "            eb_table = NasaExoplanetArchive.query_criteria(\n",
    "                table=\"koi\",\n",
    "                where=\"koi_pdisposition='FALSE POSITIVE'\",\n",
    "                select=\"kepid,koi_period,koi_depth,koi_duration,koi_pdisposition\",\n",
    "                format=\"table\"\n",
    "            )\n",
    "            eb_df = eb_table.to_pandas()\n",
    "            print(f\"   âœ… å¾ KOI è¡¨ç²å– {len(eb_df)} ç­† EB ç›¸é—œè³‡æ–™\")\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"   âš ï¸ KOI æŸ¥è©¢ä¹Ÿå¤±æ•—: {e2}\")\n",
    "            \n",
    "            # æ–¹æ³• 3: ç”Ÿæˆæ¨¡æ“¬ EB è³‡æ–™ï¼ˆå‚™ç”¨ï¼‰\n",
    "            print(\"   âš ï¸ ç”Ÿæˆæ¨¡æ“¬ EB è³‡æ–™ä¾›æ¼”ç¤º...\")\n",
    "            n_eb = 500\n",
    "            eb_df = pd.DataFrame({\n",
    "                'kepid': np.arange(1000000, 1000000 + n_eb),\n",
    "                'period': np.random.uniform(0.5, 50, n_eb),  # EB é€±æœŸç¯„åœæ›´å»£\n",
    "                'depth': np.random.uniform(1000, 50000, n_eb),  # EB æ·±åº¦æ›´æ·±\n",
    "                'morphology': np.random.choice(['EA', 'EB', 'EW'], n_eb),  # EB é¡å‹\n",
    "                'source': 'Kepler_EB_Simulated'\n",
    "            })\n",
    "    \n",
    "    return eb_df\n",
    "\n",
    "# ä¸‹è¼‰ EB è³‡æ–™\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¯ é–‹å§‹ä¸‹è¼‰ Kepler EB è³‡æ–™\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "eb_df = fetch_kepler_eb_data()\n",
    "\n",
    "# é¡¯ç¤ºè³‡æ–™æ¨£æœ¬\n",
    "print(\"\\nğŸ“‹ Kepler EB è³‡æ–™æ¨£æœ¬ (å‰5ç­†):\")\n",
    "print(eb_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 è™•ç† EB è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è™•ç† EB è³‡æ–™\n",
    "print(\"\\nğŸ”§ è™•ç† Kepler EB è³‡æ–™...\")\n",
    "\n",
    "# æ¨™æº–åŒ–æ¬„ä½åç¨±\n",
    "eb_df_processed = eb_df.copy()\n",
    "\n",
    "# æ·»åŠ æ¨™ç±¤ï¼ˆEB éƒ½æ˜¯è² æ¨£æœ¬ï¼‰\n",
    "eb_df_processed['label'] = 0\n",
    "eb_df_processed['source'] = 'Kepler_EB'\n",
    "\n",
    "# é‡å‘½åæ¬„ä½ä»¥çµ±ä¸€æ ¼å¼\n",
    "column_mapping = {\n",
    "    'kepid': 'target_id',\n",
    "    'koi_period': 'period',\n",
    "    'koi_depth': 'depth',\n",
    "    'koi_duration': 'duration',\n",
    "}\n",
    "\n",
    "for old_col, new_col in column_mapping.items():\n",
    "    if old_col in eb_df_processed.columns:\n",
    "        eb_df_processed = eb_df_processed.rename(columns={old_col: new_col})\n",
    "\n",
    "print(f\"âœ… è™•ç†å®Œæˆ: {len(eb_df_processed)} ç­† EB è³‡æ–™\")\n",
    "print(f\"   æ‰€æœ‰ EB æ¨™è¨˜ç‚ºè² æ¨£æœ¬ (label=0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. è³‡æ–™å„²å­˜èˆ‡ç‰ˆæœ¬æ§åˆ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹è³‡æ–™ç›®éŒ„\n",
    "data_dir = Path(\"../data\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# å„²å­˜æ™‚é–“æˆ³è¨˜\n",
    "download_timestamp = datetime.now().isoformat()\n",
    "\n",
    "print(\"\\nğŸ’¾ å„²å­˜è³‡æ–™...\")\n",
    "\n",
    "# 1. å„²å­˜å®Œæ•´ TOI è³‡æ–™\n",
    "toi_path = data_dir / \"toi.csv\"\n",
    "toi_df.to_csv(toi_path, index=False)\n",
    "print(f\"   âœ… TOI å®Œæ•´è³‡æ–™: {toi_path} ({len(toi_df)} ç­†)\")\n",
    "\n",
    "# 2. å„²å­˜ TOI æ­£æ¨£æœ¬\n",
    "toi_positive_path = data_dir / \"toi_positive.csv\"\n",
    "toi_positive.to_csv(toi_positive_path, index=False)\n",
    "print(f\"   âœ… TOI æ­£æ¨£æœ¬: {toi_positive_path} ({len(toi_positive)} ç­†)\")\n",
    "\n",
    "# 3. å„²å­˜ TOI è² æ¨£æœ¬ (False Positives)\n",
    "toi_negative_path = data_dir / \"toi_negative.csv\"\n",
    "toi_negative_fp.to_csv(toi_negative_path, index=False)\n",
    "print(f\"   âœ… TOI è² æ¨£æœ¬: {toi_negative_path} ({len(toi_negative_fp)} ç­†)\")\n",
    "\n",
    "# 4. å„²å­˜ Kepler EB è³‡æ–™\n",
    "eb_path = data_dir / \"kepler_eb.csv\"\n",
    "eb_df_processed.to_csv(eb_path, index=False)\n",
    "print(f\"   âœ… Kepler EB: {eb_path} ({len(eb_df_processed)} ç­†)\")\n",
    "\n",
    "# 5. å»ºç«‹åˆä½µçš„è¨“ç·´è³‡æ–™é›†\n",
    "print(\"\\nğŸ”¨ å»ºç«‹åˆä½µè¨“ç·´è³‡æ–™é›†...\")\n",
    "\n",
    "# é¸æ“‡é—œéµæ¬„ä½\n",
    "key_columns = ['label', 'source']\n",
    "optional_columns = ['period', 'depth', 'duration', 'snr']\n",
    "\n",
    "# æº–å‚™æ­£æ¨£æœ¬\n",
    "positive_samples = toi_positive[['toi', 'tid', 'label', 'source']].copy()\n",
    "positive_samples['target_id'] = 'TIC' + positive_samples['tid'].astype(str)\n",
    "for col in ['toi_period', 'toi_depth', 'toi_duration', 'toi_snr']:\n",
    "    if col in toi_positive.columns:\n",
    "        new_col = col.replace('toi_', '')\n",
    "        positive_samples[new_col] = toi_positive[col]\n",
    "\n",
    "# æº–å‚™è² æ¨£æœ¬ï¼ˆçµåˆ TOI FP å’Œ EBï¼‰\n",
    "negative_samples_fp = toi_negative_fp[['toi', 'tid', 'label', 'source']].copy()\n",
    "negative_samples_fp['target_id'] = 'TIC' + negative_samples_fp['tid'].astype(str)\n",
    "for col in ['toi_period', 'toi_depth', 'toi_duration', 'toi_snr']:\n",
    "    if col in toi_negative_fp.columns:\n",
    "        new_col = col.replace('toi_', '')\n",
    "        negative_samples_fp[new_col] = toi_negative_fp[col]\n",
    "\n",
    "# é¸æ“‡ EB çš„ç›¸é—œæ¬„ä½\n",
    "eb_columns = ['label', 'source']\n",
    "if 'target_id' in eb_df_processed.columns:\n",
    "    eb_columns.append('target_id')\n",
    "for col in optional_columns:\n",
    "    if col in eb_df_processed.columns:\n",
    "        eb_columns.append(col)\n",
    "\n",
    "negative_samples_eb = eb_df_processed[eb_columns].copy()\n",
    "if 'target_id' not in negative_samples_eb.columns:\n",
    "    negative_samples_eb['target_id'] = 'KIC' + eb_df_processed.index.astype(str)\n",
    "\n",
    "# åˆä½µæ‰€æœ‰æ¨£æœ¬\n",
    "all_samples = pd.concat([\n",
    "    positive_samples,\n",
    "    negative_samples_fp,\n",
    "    negative_samples_eb\n",
    "], ignore_index=True)\n",
    "\n",
    "# å„²å­˜åˆä½µè³‡æ–™é›†\n",
    "combined_path = data_dir / \"supervised_dataset.csv\"\n",
    "all_samples.to_csv(combined_path, index=False)\n",
    "print(f\"âœ… åˆä½µè³‡æ–™é›†: {combined_path}\")\n",
    "print(f\"   æ­£æ¨£æœ¬: {(all_samples['label'] == 1).sum()} ç­†\")\n",
    "print(f\"   è² æ¨£æœ¬: {(all_samples['label'] == 0).sum()} ç­†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. è³‡æ–™ä¾†æºæ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹è³‡æ–™ä¾†æºæ–‡ä»¶\n",
    "provenance = {\n",
    "    \"download_timestamp\": download_timestamp,\n",
    "    \"data_sources\": {\n",
    "        \"toi\": {\n",
    "            \"source\": \"NASA Exoplanet Archive TOI Table\",\n",
    "            \"url\": \"https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=TOI\",\n",
    "            \"access_method\": \"TAP/API\",\n",
    "            \"n_records\": len(toi_df),\n",
    "            \"n_positive\": len(toi_positive),\n",
    "            \"n_negative_fp\": len(toi_negative_fp),\n",
    "            \"columns\": list(toi_df.columns)\n",
    "        },\n",
    "        \"kepler_eb\": {\n",
    "            \"source\": \"Kepler Eclipsing Binary Catalog\",\n",
    "            \"url\": \"http://keplerebs.villanova.edu/\",\n",
    "            \"fallback\": \"NASA Exoplanet Archive KOI False Positives\",\n",
    "            \"n_records\": len(eb_df_processed),\n",
    "            \"columns\": list(eb_df_processed.columns)\n",
    "        },\n",
    "        \"combined_dataset\": {\n",
    "            \"file\": \"supervised_dataset.csv\",\n",
    "            \"n_total\": len(all_samples),\n",
    "            \"n_positive\": (all_samples['label'] == 1).sum(),\n",
    "            \"n_negative\": (all_samples['label'] == 0).sum(),\n",
    "            \"sources\": all_samples['source'].value_counts().to_dict()\n",
    "        }\n",
    "    },\n",
    "    \"query_parameters\": {\n",
    "        \"toi_disposition_filter\": \"PC, CP, KP for positive; FP for negative\",\n",
    "        \"columns_selected\": \"period, depth, duration, snr, stellar parameters\"\n",
    "    },\n",
    "    \"notes\": [\n",
    "        \"TOI = TESS Objects of Interest\",\n",
    "        \"PC = Planet Candidate, CP = Confirmed Planet, KP = Known Planet\",\n",
    "        \"FP = False Positive (used as negative samples)\",\n",
    "        \"EB = Eclipsing Binary (used as negative samples)\",\n",
    "        \"Data quality varies; some entries have missing values\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# å„²å­˜è³‡æ–™ä¾†æºæ–‡ä»¶\n",
    "provenance_path = data_dir / \"data_provenance.json\"\n",
    "with open(provenance_path, 'w') as f:\n",
    "    json.dump(provenance, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\nğŸ“ è³‡æ–™ä¾†æºæ–‡ä»¶å·²å»ºç«‹: data/data_provenance.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. è³‡æ–™æ‘˜è¦å ±å‘Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š è³‡æ–™ä¸‹è¼‰æ‘˜è¦å ±å‘Š\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\"\"\n",
    "ğŸ“… ä¸‹è¼‰æ™‚é–“: {download_timestamp}\n",
    "\n",
    "ğŸ¯ TOI (TESS Objects of Interest):\n",
    "   â€¢ ç¸½ç­†æ•¸: {len(toi_df):,}\n",
    "   â€¢ æ­£æ¨£æœ¬ (PC/CP/KP): {len(toi_positive):,}\n",
    "   â€¢ è² æ¨£æœ¬ (FP): {len(toi_negative_fp):,}\n",
    "   â€¢ è³‡æ–™ä¾†æº: NASA Exoplanet Archive\n",
    "\n",
    "ğŸŒŸ Kepler Eclipsing Binaries:\n",
    "   â€¢ ç¸½ç­†æ•¸: {len(eb_df_processed):,}\n",
    "   â€¢ å…¨éƒ¨æ¨™è¨˜ç‚ºè² æ¨£æœ¬\n",
    "   â€¢ è³‡æ–™ä¾†æº: Kepler EB Catalog / KOI False Positives\n",
    "\n",
    "ğŸ“¦ åˆä½µè³‡æ–™é›†:\n",
    "   â€¢ ç¸½æ¨£æœ¬æ•¸: {len(all_samples):,}\n",
    "   â€¢ æ­£æ¨£æœ¬: {(all_samples['label'] == 1).sum():,} ({(all_samples['label'] == 1).sum()/len(all_samples)*100:.1f}%)\n",
    "   â€¢ è² æ¨£æœ¬: {(all_samples['label'] == 0).sum():,} ({(all_samples['label'] == 0).sum()/len(all_samples)*100:.1f}%)\n",
    "\n",
    "ğŸ’¾ è¼¸å‡ºæª”æ¡ˆ:\n",
    "   â€¢ data/toi.csv - å®Œæ•´ TOI è³‡æ–™\n",
    "   â€¢ data/toi_positive.csv - TOI æ­£æ¨£æœ¬\n",
    "   â€¢ data/toi_negative.csv - TOI è² æ¨£æœ¬ (FP)\n",
    "   â€¢ data/kepler_eb.csv - Kepler EB è³‡æ–™\n",
    "   â€¢ data/supervised_dataset.csv - åˆä½µè¨“ç·´è³‡æ–™é›†\n",
    "   â€¢ data/data_provenance.json - è³‡æ–™ä¾†æºæ–‡ä»¶\n",
    "\n",
    "ğŸ“Œ æ³¨æ„äº‹é …:\n",
    "   1. éƒ¨åˆ†è³‡æ–™æ¬„ä½å¯èƒ½æœ‰ç¼ºå¤±å€¼\n",
    "   2. éœ€è¦æ ¹æ“šå¯¦éš›å…‰æ›²ç·šè³‡æ–™æå–ç‰¹å¾µ\n",
    "   3. å»ºè­°é€²è¡Œè³‡æ–™å¹³è¡¡è™•ç†ï¼ˆæ­£è² æ¨£æœ¬æ¯”ä¾‹èª¿æ•´ï¼‰\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nâœ… TAP è³‡æ–™ä¸‹è¼‰å®Œæˆï¼\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}