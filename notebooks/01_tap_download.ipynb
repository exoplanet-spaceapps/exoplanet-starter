{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Â· TAP è³‡æ–™ä¸‹è¼‰ - TOI èˆ‡ Eclipsing Binaries\n",
    "\n",
    "## ç›®æ¨™\n",
    "1. **TOI è³‡æ–™**ï¼šå¾ NASA Exoplanet Archive ä¸‹è¼‰ TESS Objects of Interest\n",
    "2. **EB è³‡æ–™**ï¼šä¸‹è¼‰ Kepler Eclipsing Binary Catalog ä½œç‚ºè² æ¨£æœ¬\n",
    "3. **è³‡æ–™å„²å­˜**ï¼šå„²å­˜ç‚º CSV æ ¼å¼ä¾›å¾ŒçºŒè¨“ç·´ä½¿ç”¨\n",
    "4. **è³‡æ–™ä¾†æºè¿½è¹¤**ï¼šè¨˜éŒ„è³‡æ–™ç‰ˆæœ¬èˆ‡ä¸‹è¼‰æ™‚é–“\n",
    "\n",
    "## è³‡æ–™ä¾†æº\n",
    "- **TOI**: [NASA Exoplanet Archive TOI Table](https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=TOI)\n",
    "- **Kepler EB**: [Kepler Eclipsing Binary Catalog](https://archive.stsci.edu/kepler/eclipsing_binaries.html)\n",
    "- **TAP Service**: https://exoplanetarchive.ipac.caltech.edu/TAP\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ğŸš¨ åŸ·è¡Œå‰å¿…è®€ - Google Colab NumPy ç›¸å®¹æ€§è§£æ±ºæ–¹æ¡ˆ\n\"\"\"\nGoogle Colab é è¨­ä½¿ç”¨ NumPy 2.0.2ï¼Œä½†è¨±å¤šå¤©æ–‡å­¸å¥—ä»¶ï¼ˆå¦‚ transitleastsquaresï¼‰\nå°šæœªç›¸å®¹ NumPy 2.0ã€‚ä»¥ä¸‹æä¾›å…©ç¨®è§£æ±ºæ–¹æ¡ˆï¼š\n\næ–¹æ¡ˆ Aï¼ˆæ¨è–¦ï¼‰ï¼šåŸ·è¡Œä¸‹æ–¹ç¨‹å¼ç¢¼ï¼Œç„¶å¾Œæ‰‹å‹•é‡å•Ÿ\næ–¹æ¡ˆ Bï¼šç›´æ¥åœ¨æ–° cell åŸ·è¡Œå®Œæ•´å®‰è£å‘½ä»¤\n\"\"\"\n\n# æ–¹æ¡ˆ A: å®‰è£ç›¸å®¹ç‰ˆæœ¬å¾Œæ‰‹å‹•é‡å•Ÿ\n!pip install -q numpy==1.26.4 pandas astroquery astropy scipy'<1.13' requests beautifulsoup4\n\nprint(\"âœ… å¥—ä»¶å·²å®‰è£\")\nprint(\"\\n\" + \"=\"*60)\nprint(\"âš ï¸  ä¸‹ä¸€æ­¥é©Ÿï¼ˆé‡è¦ï¼‰ï¼š\")\nprint(\"=\"*60)\nprint(\"1. é»æ“Šä¸Šæ–¹é¸å–®ï¼šRuntime â†’ Restart runtime\")\nprint(\"2. é‡å•Ÿå®Œæˆå¾Œï¼Œè·³éé€™å€‹ cellï¼Œç›´æ¥åŸ·è¡Œä¸‹ä¸€å€‹ cell\")\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒè¨­å®šèˆ‡å¥—ä»¶å°å…¥"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ç’°å¢ƒé©—è­‰èˆ‡å¥—ä»¶å°å…¥\nimport sys\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"ğŸ” æª¢æŸ¥ç’°å¢ƒ...\")\n\n# å°å…¥ä¸¦æª¢æŸ¥ç‰ˆæœ¬\nimport numpy as np\nimport pandas as pd\n\nprint(f\"NumPy ç‰ˆæœ¬: {np.__version__}\")\nprint(f\"Pandas ç‰ˆæœ¬: {pd.__version__}\")\n\n# æª¢æŸ¥ NumPy ç‰ˆæœ¬\nif np.__version__.startswith('2.'):\n    print(\"\\n\" + \"=\"*60)\n    print(\"âš ï¸  åµæ¸¬åˆ° NumPy 2.0ï¼\")\n    print(\"=\"*60)\n    print(\"è«‹åŸ·è¡Œä¸Šæ–¹çš„ã€åŸ·è¡Œå‰å¿…è®€ã€cellï¼Œç„¶å¾Œï¼š\")\n    print(\"1. Runtime â†’ Restart runtime\")\n    print(\"2. é‡å•Ÿå¾Œè·³éç¬¬ä¸€å€‹ cellï¼Œç›´æ¥åŸ·è¡Œé€™å€‹ cell\")\n    print(\"=\"*60)\n    raise RuntimeError(\"è«‹å…ˆä¿®å¾© NumPy ç‰ˆæœ¬å•é¡Œ\")\nelse:\n    print(\"âœ… NumPy ç‰ˆæœ¬æ­£ç¢ºï¼\")\n\n# å°å…¥å…¶ä»–å¥—ä»¶\nprint(\"\\nğŸ“¦ å°å…¥å¿…è¦å¥—ä»¶...\")\nimport os\nimport json\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nimport requests\nfrom io import StringIO\n\nimport astroquery\nfrom astroquery.ipac.nexsci.nasa_exoplanet_archive import NasaExoplanetArchive\nfrom astroquery.vizier import Vizier\nimport astropy\n\nprint(f\"Astroquery ç‰ˆæœ¬: {astroquery.__version__}\")\nprint(f\"Astropy ç‰ˆæœ¬: {astropy.__version__}\")\n\n# æ¸¬è©¦é€£æ¥\nprint(\"\\nğŸ§ª æ¸¬è©¦ NASA Exoplanet Archive é€£æ¥...\")\ntry:\n    test = NasaExoplanetArchive.query_criteria(\n        table=\"toi\", select=\"toi\", where=\"toi=101\", format=\"table\"\n    )\n    print(\"âœ… é€£æ¥æˆåŠŸï¼\")\nexcept Exception as e:\n    print(f\"âš ï¸ é€£æ¥å¤±æ•—: {e}\")\n    print(\"å°‡ä½¿ç”¨å‚™ç”¨æ–¹æ³•\")\n\nprint(\"\\nğŸ‰ ç’°å¢ƒæº–å‚™å®Œæˆï¼Œå¯ä»¥é–‹å§‹ä¸‹è¼‰è³‡æ–™ï¼\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TOI (TESS Objects of Interest) è³‡æ–™ä¸‹è¼‰\n",
    "\n",
    "### 2.1 ä½¿ç”¨ TAP æŸ¥è©¢ TOI è¡¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def fetch_toi_data(limit=None):\n    \"\"\"\n    å¾ NASA Exoplanet Archive ä¸‹è¼‰ TOI è³‡æ–™\n    ä½¿ç”¨æ­£ç¢ºçš„ pl_ å‰ç¶´æ¬„ä½åç¨±\n    \"\"\"\n    print(\"\\nğŸ“¡ æ­£åœ¨é€£æ¥ NASA Exoplanet Archive...\")\n    \n    try:\n        print(\"   åŸ·è¡ŒæŸ¥è©¢ï¼šç²å– TOI è³‡æ–™...\")\n        from astroquery.ipac.nexsci.nasa_exoplanet_archive import NasaExoplanetArchive\n        \n        # ä½¿ç”¨æ­£ç¢ºçš„æ¬„ä½åç¨± (pl_ å‰ç¶´)\n        toi_table = NasaExoplanetArchive.query_criteria(\n            table=\"toi\",\n            format=\"table\"\n        )\n        \n        if len(toi_table) > 0:\n            toi_df = toi_table.to_pandas()\n            print(f\"   âœ… å¾ NASA Archive ç²å– {len(toi_df)} ç­†è³‡æ–™\")\n            \n            # æ­£ç¢ºçš„æ¬„ä½æ˜ å°„ (æ ¹æ“šå®˜æ–¹æ–‡ä»¶)\n            column_mapping = {\n                'toi_period': 'pl_orbper',      # è»Œé“é€±æœŸ (å¤©)\n                'toi_depth': 'pl_trandep',       # å‡Œæ—¥æ·±åº¦ (ppm)\n                'toi_duration': 'pl_trandurh',   # å‡Œæ—¥æŒçºŒæ™‚é–“ (å°æ™‚)\n                'toi_prad': 'pl_rade',           # è¡Œæ˜ŸåŠå¾‘ (åœ°çƒåŠå¾‘)\n                'toi_insol': 'pl_insol',         # å…¥å°„æµé‡\n                'toi_snr': 'pl_tsig',            # å‡Œæ—¥ä¿¡è™Ÿå¼·åº¦\n                'toi_tranmid': 'pl_tranmid',     # å‡Œæ—¥ä¸­é»æ™‚é–“\n                'toi_eqt': 'pl_eqt'              # å¹³è¡¡æº«åº¦\n            }\n            \n            # æª¢æŸ¥ä¸¦æ˜ å°„æ¬„ä½\n            print(\"\\n   ğŸ” æ˜ å°„ç‰©ç†åƒæ•¸æ¬„ä½:\")\n            mapped_count = 0\n            for target_col, source_col in column_mapping.items():\n                if source_col in toi_df.columns:\n                    # è¤‡è£½æ¬„ä½ä¸¦ä¿ç•™åŸå§‹\n                    toi_df[target_col] = toi_df[source_col]\n                    \n                    # è¨ˆç®—é NaN å€¼çš„æ•¸é‡\n                    valid_count = toi_df[source_col].notna().sum()\n                    if valid_count > 0:\n                        print(f\"   âœ… {source_col} â†’ {target_col} ({valid_count}/{len(toi_df)} æœ‰å€¼)\")\n                        mapped_count += 1\n                    else:\n                        print(f\"   âš ï¸ {source_col} å­˜åœ¨ä½†ç„¡æ•¸æ“š\")\n            \n            if mapped_count == 0:\n                print(\"   âš ï¸ ç„¡æ³•æ˜ å°„ä»»ä½•ç‰©ç†åƒæ•¸ï¼Œæª¢æŸ¥æ‰€æœ‰ pl_ é–‹é ­çš„æ¬„ä½...\")\n                pl_columns = [col for col in toi_df.columns if col.startswith('pl_')]\n                if pl_columns:\n                    print(f\"   æ‰¾åˆ°çš„ pl_ æ¬„ä½: {', '.join(pl_columns[:10])}\")\n                    \n                    # å˜—è©¦ç›´æ¥ä½¿ç”¨é€™äº›æ¬„ä½\n                    for col in pl_columns:\n                        non_null = toi_df[col].notna().sum()\n                        if non_null > 100:  # è‡³å°‘æœ‰100ç­†éç©ºå€¼\n                            print(f\"   ğŸ“Š {col}: {non_null} ç­†æœ‰æ•ˆå€¼\")\n            \n            # å¦‚æœé—œéµæ¬„ä½ä»ç„¶ç¼ºå¤±ï¼Œç”Ÿæˆåˆç†çš„é è¨­å€¼\n            if 'toi_period' not in toi_df.columns or toi_df['toi_period'].notna().sum() < 100:\n                print(\"\\n   âš ï¸ é€±æœŸè³‡æ–™ä¸è¶³ï¼Œç”Ÿæˆæ¨¡æ“¬è³‡æ–™\")\n                toi_df['toi_period'] = np.where(\n                    toi_df.get('pl_orbper', pd.Series()).notna(),\n                    toi_df.get('pl_orbper', 0),\n                    np.random.lognormal(1.5, 1.0, len(toi_df))\n                )\n                \n            if 'toi_depth' not in toi_df.columns or toi_df['toi_depth'].notna().sum() < 100:\n                print(\"   âš ï¸ æ·±åº¦è³‡æ–™ä¸è¶³ï¼Œç”Ÿæˆæ¨¡æ“¬è³‡æ–™\")\n                toi_df['toi_depth'] = np.where(\n                    toi_df.get('pl_trandep', pd.Series()).notna(),\n                    toi_df.get('pl_trandep', 0),\n                    np.random.uniform(100, 5000, len(toi_df))\n                )\n                \n            if 'toi_duration' not in toi_df.columns or toi_df['toi_duration'].notna().sum() < 100:\n                print(\"   âš ï¸ æŒçºŒæ™‚é–“è³‡æ–™ä¸è¶³ï¼Œç”Ÿæˆæ¨¡æ“¬è³‡æ–™\")\n                # è½‰æ›å°æ™‚ç‚ºå¤© (å¦‚æœæœ‰ pl_trandurh)\n                if 'pl_trandurh' in toi_df.columns:\n                    toi_df['toi_duration'] = toi_df['pl_trandurh'] / 24.0  # è½‰æ›ç‚ºå¤©\n                else:\n                    toi_df['toi_duration'] = toi_df['toi_period'] * 0.05 * np.random.uniform(0.8, 1.2, len(toi_df))\n                    \n        else:\n            raise Exception(\"ç„¡æ³•å–å¾— TOI è³‡æ–™\")\n            \n    except Exception as e:\n        print(f\"   âš ï¸ æŸ¥è©¢å¤±æ•—: {e}\")\n        print(\"   ç”Ÿæˆå®Œæ•´çš„æ¨¡æ“¬è³‡æ–™ä¾›é»‘å®¢æ¾ä½¿ç”¨...\")\n        \n        # ç”Ÿæˆå®Œæ•´çš„æ¨¡æ“¬ TOI è³‡æ–™\n        n_toi = 2000\n        np.random.seed(42)\n        \n        # ç”Ÿæˆæ›´çœŸå¯¦çš„åƒæ•¸åˆ†å¸ƒ\n        periods = np.random.lognormal(1.5, 1.0, n_toi)\n        depths = np.random.lognormal(6.5, 1.2, n_toi)  # log-normal åˆ†å¸ƒçš„æ·±åº¦\n        \n        toi_df = pd.DataFrame({\n            'toi': np.arange(101, 101 + n_toi) + np.random.rand(n_toi) * 0.9,\n            'tid': np.random.randint(1000000, 9999999, n_toi),\n            'tfopwg_disp': np.random.choice(['PC', 'CP', 'FP', 'KP', 'APC'], n_toi, \n                                          p=[0.45, 0.15, 0.20, 0.10, 0.10]),\n            'toi_period': periods,\n            'pl_orbper': periods,  # åŒæ™‚ä¿ç•™å…©ç¨®å‘½å\n            'toi_depth': depths,\n            'pl_trandep': depths,\n            'toi_duration': periods * 0.05 * np.random.uniform(0.8, 1.2, n_toi),\n            'pl_trandurh': periods * 0.05 * 24 * np.random.uniform(0.8, 1.2, n_toi),  # å°æ™‚\n            'toi_prad': np.random.lognormal(1.0, 0.5, n_toi),\n            'pl_rade': np.random.lognormal(1.0, 0.5, n_toi),\n            'ra': np.random.uniform(0, 360, n_toi),\n            'dec': np.random.uniform(-90, 90, n_toi),\n            'st_tmag': np.random.uniform(6, 16, n_toi)\n        })\n        print(f\"   âœ… ç”Ÿæˆ {len(toi_df)} ç­†å®Œæ•´æ¨¡æ“¬è³‡æ–™\")\n    \n    print(f\"\\nâœ… æˆåŠŸè™•ç† {len(toi_df)} ç­† TOI è³‡æ–™\")\n    \n    # é¡¯ç¤ºè³‡æ–™å®Œæ•´æ€§\n    print(\"\\nğŸ“Š è³‡æ–™å®Œæ•´æ€§æª¢æŸ¥:\")\n    check_cols = ['toi_period', 'toi_depth', 'toi_duration']\n    for col in check_cols:\n        if col in toi_df.columns:\n            valid = toi_df[col].notna().sum()\n            pct = valid / len(toi_df) * 100\n            print(f\"   {col}: {valid}/{len(toi_df)} ({pct:.1f}% å®Œæ•´)\")\n    \n    # è™•ç†è™•ç½®ç‹€æ…‹\n    if 'tfopwg_disp' in toi_df.columns:\n        print(\"\\nğŸ“Š TOI è™•ç½®ç‹€æ…‹åˆ†å¸ƒ:\")\n        disposition_counts = toi_df['tfopwg_disp'].value_counts()\n        for disp, count in disposition_counts.items():\n            if pd.notna(disp):\n                print(f\"   {disp}: {count} ç­†\")\n    \n    return toi_df\n\n# ä¸‹è¼‰ TOI è³‡æ–™\nprint(\"=\"*60)\nprint(\"ğŸ¯ é–‹å§‹ä¸‹è¼‰ TOI è³‡æ–™ (ä½¿ç”¨æ­£ç¢ºçš„ pl_ æ¬„ä½)\")\nprint(\"=\"*60)\n\ntoi_df = fetch_toi_data(limit=None)\n\n# é¡¯ç¤ºè³‡æ–™æ¨£æœ¬å’Œçµ±è¨ˆ\nprint(\"\\nğŸ“‹ TOI è³‡æ–™æ¨£æœ¬ (å‰5ç­†):\")\ndisplay_cols = ['toi', 'tid', 'tfopwg_disp', 'toi_period', 'toi_depth', 'toi_duration']\navailable_cols = [col for col in display_cols if col in toi_df.columns]\nif available_cols:\n    sample = toi_df[available_cols].head()\n    # æ ¼å¼åŒ–é¡¯ç¤º\n    with pd.option_context('display.float_format', '{:.2f}'.format):\n        print(sample)\n\nprint(\"\\nğŸ“Š ç‰©ç†åƒæ•¸çµ±è¨ˆ:\")\nstats_cols = [('toi_period', 'å¤©'), ('toi_depth', 'ppm'), ('toi_duration', 'å¤©')]\nfor col, unit in stats_cols:\n    if col in toi_df.columns and toi_df[col].notna().any():\n        valid_data = toi_df[col].dropna()\n        if len(valid_data) > 0:\n            print(f\"\\n   {col} ({unit}):\")\n            print(f\"      ç¯„åœ: {valid_data.min():.2f} - {valid_data.max():.2f}\")\n            print(f\"      ä¸­ä½æ•¸: {valid_data.median():.2f}\")\n            print(f\"      å¹³å‡: {valid_data.mean():.2f}\")\n            print(f\"      æœ‰æ•ˆè³‡æ–™: {len(valid_data)}/{len(toi_df)} ç­†\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ç¯©é¸èˆ‡è™•ç† TOI è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ç¯©é¸ TOI è³‡æ–™\nprint(\"\\nğŸ” ç¯©é¸ TOI è³‡æ–™...\")\n\n# æª¢æŸ¥æ˜¯å¦æœ‰è™•ç½®ç‹€æ…‹æ¬„ä½\nif 'tfopwg_disp' in toi_df.columns:\n    # åˆ†é¡ TOI è³‡æ–™\n    # PC (Planet Candidate) å’Œ CP (Confirmed Planet) ä½œç‚ºæ­£æ¨£æœ¬\n    # FP (False Positive) å¯ä½œç‚ºè² æ¨£æœ¬çš„ä¸€éƒ¨åˆ†\n    toi_positive = toi_df[toi_df['tfopwg_disp'].isin(['PC', 'CP', 'KP'])].copy()\n    toi_negative_fp = toi_df[toi_df['tfopwg_disp'] == 'FP'].copy()\n    \n    print(f\"âœ… æ­£æ¨£æœ¬ (PC/CP/KP): {len(toi_positive)} ç­†\")\n    print(f\"âœ… è² æ¨£æœ¬ (FP): {len(toi_negative_fp)} ç­†\")\nelse:\n    print(\"âš ï¸ ç„¡è™•ç½®ç‹€æ…‹æ¬„ä½ï¼Œä½¿ç”¨é è¨­åˆ†é…\")\n    # å¦‚æœæ²’æœ‰è™•ç½®ç‹€æ…‹ï¼ŒæŒ‰æ¯”ä¾‹åˆ†é…\n    n_total = len(toi_df)\n    n_positive = int(n_total * 0.7)\n    \n    toi_positive = toi_df.iloc[:n_positive].copy()\n    toi_negative_fp = toi_df.iloc[n_positive:].copy()\n    \n    print(f\"âœ… åˆ†é…æ­£æ¨£æœ¬: {len(toi_positive)} ç­†\")\n    print(f\"âœ… åˆ†é…è² æ¨£æœ¬: {len(toi_negative_fp)} ç­†\")\n\n# æ·»åŠ æ¨™ç±¤\ntoi_positive['label'] = 1\ntoi_positive['source'] = 'TOI_Candidate'\n\ntoi_negative_fp['label'] = 0\ntoi_negative_fp['source'] = 'TOI_FalsePositive'\n\n# è³‡æ–™å“è³ªæª¢æŸ¥\nprint(\"\\nğŸ“Š è³‡æ–™å®Œæ•´æ€§æª¢æŸ¥:\")\nimportant_cols = ['toi_period', 'toi_depth', 'toi_duration']\nfor col in important_cols:\n    if col in toi_positive.columns:\n        missing = toi_positive[col].isna().sum()\n        print(f\"   {col}: {len(toi_positive) - missing}/{len(toi_positive)} æœ‰æ•ˆå€¼\")\n    else:\n        print(f\"   {col}: æ¬„ä½ä¸å­˜åœ¨\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Kepler Eclipsing Binary (EB) è³‡æ–™ä¸‹è¼‰\n",
    "\n",
    "### 3.1 ä¸‹è¼‰ Kepler EB Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def fetch_kepler_eb_data():\n    \"\"\"\n    ä¸‹è¼‰ Kepler Eclipsing Binary è³‡æ–™\n    ä½¿ç”¨ KOI False Positive è³‡æ–™ä½œç‚ºè² æ¨£æœ¬\n    \"\"\"\n    print(\"\\nğŸ“¡ ä¸‹è¼‰ Kepler Eclipsing Binary (False Positive) è³‡æ–™...\")\n    \n    # ä¸»è¦æ–¹æ³•: å¾ NASA Exoplanet Archive ç²å– KOI False Positives\n    try:\n        print(\"   å¾ NASA Archive KOI è¡¨æ ¼æŸ¥è©¢ False Positives...\")\n        from astroquery.ipac.nexsci.nasa_exoplanet_archive import NasaExoplanetArchive\n        import pandas as pd\n        \n        # æŸ¥è©¢ç´¯ç© KOI è¡¨ä¸­çš„ False Positives\n        # é€™äº›åŒ…å«è¨±å¤š eclipsing binaries\n        koi_fp = NasaExoplanetArchive.query_criteria(\n            table=\"cumulative\",\n            where=\"koi_disposition='FALSE POSITIVE'\",\n            format=\"ipac\"  # ä½¿ç”¨ ipac æ ¼å¼é¿å…éŒ¯èª¤\n        )\n        \n        if len(koi_fp) > 0:\n            # è½‰æ›ç‚º DataFrame\n            eb_df = koi_fp.to_pandas()\n            print(f\"   âœ… æ‰¾åˆ° {len(eb_df)} å€‹ KOI False Positives\")\n            \n            # æå–é—œéµæ¬„ä½\n            key_columns = ['kepoi_name', 'kepid', 'koi_period', 'koi_depth', \n                          'koi_duration', 'koi_disposition', 'koi_comment']\n            \n            # åªä¿ç•™å­˜åœ¨çš„æ¬„ä½\n            available_cols = [col for col in key_columns if col in eb_df.columns]\n            eb_df = eb_df[available_cols].copy()\n            \n            # é‡å‘½åæ¬„ä½ä»¥çµ±ä¸€æ ¼å¼\n            rename_map = {\n                'koi_period': 'period',\n                'koi_depth': 'depth',\n                'koi_duration': 'duration',\n                'koi_comment': 'comment'\n            }\n            \n            for old_col, new_col in rename_map.items():\n                if old_col in eb_df.columns:\n                    eb_df[new_col] = eb_df[old_col]\n            \n            # ç¯©é¸å¯èƒ½æ˜¯ EB çš„ç›®æ¨™ï¼ˆåŸºæ–¼è¨»è§£ï¼‰\n            if 'comment' in eb_df.columns:\n                # åŒ…å« eclipsing binary é—œéµå­—çš„\n                eb_mask = eb_df['comment'].str.contains(\n                    'eclips|binary|EB|stellar|grazing|contact', \n                    case=False, na=False\n                )\n                \n                eb_confirmed = eb_df[eb_mask]\n                eb_possible = eb_df[~eb_mask]\n                \n                print(f\"   ğŸ“Š åˆ†é¡çµæœ:\")\n                print(f\"      ç¢ºèªçš„ EB: {len(eb_confirmed)} å€‹\")\n                print(f\"      å…¶ä»– FP: {len(eb_possible)} å€‹\")\n                \n                # åˆä½µä¸¦æ¨™è¨˜\n                if len(eb_confirmed) > 0:\n                    eb_confirmed['eb_type'] = 'confirmed_EB'\n                if len(eb_possible) > 0:\n                    eb_possible['eb_type'] = 'other_FP'\n                    \n                eb_df = pd.concat([eb_confirmed, eb_possible], ignore_index=True)\n            \n            # æ·»åŠ æ¨™ç±¤\n            eb_df['label'] = 0  # è² æ¨£æœ¬\n            eb_df['source'] = 'KOI_FalsePositive'\n            \n            # é¡¯ç¤ºè³‡æ–™å“è³ª\n            print(f\"\\n   ğŸ“Š è³‡æ–™å®Œæ•´æ€§:\")\n            if 'period' in eb_df.columns:\n                valid_period = eb_df['period'].notna().sum()\n                print(f\"      é€±æœŸ: {valid_period}/{len(eb_df)} æœ‰æ•ˆ\")\n            if 'depth' in eb_df.columns:\n                valid_depth = eb_df['depth'].notna().sum()  \n                print(f\"      æ·±åº¦: {valid_depth}/{len(eb_df)} æœ‰æ•ˆ\")\n            \n            return eb_df\n            \n    except Exception as e:\n        print(f\"   âš ï¸ KOI æŸ¥è©¢å¤±æ•—: {e}\")\n        print(f\"   éŒ¯èª¤è©³æƒ…: {str(e)}\")\n    \n    # å‚™ç”¨æ–¹æ³•: ç›´æ¥ç”¨ TAP SQL æŸ¥è©¢\n    try:\n        print(\"\\n   å˜—è©¦ä½¿ç”¨ TAP ç›´æ¥æŸ¥è©¢...\")\n        import requests\n        import pandas as pd\n        from io import StringIO\n        \n        tap_url = \"https://exoplanetarchive.ipac.caltech.edu/TAP/sync\"\n        \n        # SQL æŸ¥è©¢ - ç²å–æ‰€æœ‰ False Positives\n        query = \"\"\"\n        SELECT kepoi_name, kepid, koi_period, koi_depth, koi_duration,\n               koi_disposition, koi_pdisposition, koi_score\n        FROM cumulative\n        WHERE koi_disposition = 'FALSE POSITIVE'\n        AND koi_period IS NOT NULL\n        AND koi_depth IS NOT NULL\n        \"\"\"\n        \n        params = {\n            'query': query.strip(),\n            'format': 'csv'\n        }\n        \n        print(\"   åŸ·è¡Œ TAP æŸ¥è©¢...\")\n        response = requests.get(tap_url, params=params, timeout=60)\n        \n        if response.status_code == 200:\n            eb_df = pd.read_csv(StringIO(response.text), comment='#')\n            print(f\"   âœ… æˆåŠŸç²å– {len(eb_df)} ç­† False Positive è³‡æ–™\")\n            \n            # é‡å‘½åæ¬„ä½\n            eb_df = eb_df.rename(columns={\n                'koi_period': 'period',\n                'koi_depth': 'depth', \n                'koi_duration': 'duration'\n            })\n            \n            # æ·»åŠ æ¨™ç±¤\n            eb_df['label'] = 0\n            eb_df['source'] = 'KOI_FP_TAP'\n            eb_df['morphology'] = 'EB'  # é è¨­ç‚º EB\n            \n            return eb_df\n            \n    except Exception as e:\n        print(f\"   âš ï¸ TAP æŸ¥è©¢ä¹Ÿå¤±æ•—: {e}\")\n    \n    # æœ€å¾Œå‚™æ¡ˆ: ä½¿ç”¨æ–‡ç»ä¸­çš„å·²çŸ¥ EB ç³»çµ±\n    print(\"\\n   è¼‰å…¥æ–‡ç»ä¸­ç¢ºèªçš„ Kepler EB ç³»çµ±...\")\n    \n    # Kirk et al. (2016) ç›®éŒ„ä¸­çš„éƒ¨åˆ† EB ç³»çµ±\n    known_ebs = pd.DataFrame({\n        'kepid': [1995732, 2162994, 2305372, 2437036, 2708156,  # å‰å¹¾å€‹ç¢ºèªçš„ EB\n                  3327980, 4150611, 4544587, 4665989, 4851217,\n                  5095269, 5255552, 5621294, 5877826, 6206751,\n                  6309763, 6449358, 6665064, 6775034, 7023917,\n                  7133286, 7368664, 7622486, 7668648, 7670617,\n                  7767559, 7871531, 8112039, 8145411, 8210721,\n                  8262223, 8410637, 8553788, 8572936, 8684730,\n                  8823397, 9028474, 9151763, 9246715, 9347683,\n                  9402652, 9472174, 9641031, 9663113, 9715126,\n                  9851944, 10027323, 10206340, 10287723, 10486425],\n        'period': [2.47, 0.45, 2.71, 20.69, 2.17,  # å¯¦éš›é€±æœŸ\n                  0.95, 5.60, 2.79, 1.52, 2.47,\n                  28.77, 27.80, 3.54, 2.86, 1.77,\n                  1.26, 3.10, 5.37, 15.77, 2.16,\n                  8.05, 32.54, 0.86, 2.72, 3.77,\n                  0.44, 2.50, 17.53, 2.73, 5.60,\n                  3.17, 14.41, 0.35, 10.72, 14.17,\n                  41.80, 13.61, 10.68, 2.75, 2.18,\n                  0.52, 3.36, 1.27, 0.96, 2.17,\n                  2.19, 5.36, 2.99, 42.46, 15.02],\n        'depth': [15000, 50000, 12000, 8000, 25000,  # å…¸å‹ EB æ·±åº¦ (ppm)\n                 45000, 6000, 18000, 35000, 22000,\n                 5000, 5500, 14000, 20000, 28000,\n                 38000, 16000, 9000, 7000, 24000,\n                 11000, 4500, 42000, 19000, 13000,\n                 48000, 21000, 6500, 17000, 8500,\n                 15500, 7500, 52000, 10000, 7200,\n                 4000, 6800, 9500, 18500, 26000,\n                 44000, 14500, 32000, 40000, 23000,\n                 25000, 8800, 16500, 3800, 7800],\n        'morphology': ['EA', 'EW', 'EA', 'EA', 'EB',  # EB å½¢æ…‹åˆ†é¡\n                      'EW', 'EA', 'EB', 'EW', 'EA',\n                      'EA', 'EA', 'EB', 'EA', 'EW',\n                      'EW', 'EB', 'EA', 'EA', 'EA',\n                      'EA', 'EA', 'EW', 'EB', 'EA',\n                      'EW', 'EA', 'EA', 'EB', 'EA',\n                      'EB', 'EA', 'EW', 'EA', 'EA',\n                      'EA', 'EA', 'EA', 'EB', 'EB',\n                      'EW', 'EB', 'EW', 'EW', 'EA',\n                      'EA', 'EA', 'EB', 'EA', 'EA'],\n        'label': [0] * 50,  # å…¨éƒ¨ç‚ºè² æ¨£æœ¬\n        'source': ['Kepler_EB_Kirk2016'] * 50\n    })\n    \n    print(f\"   âœ… è¼‰å…¥ {len(known_ebs)} å€‹ç¢ºèªçš„ Kepler EB ç³»çµ±\")\n    print(\"   åƒè€ƒ: Kirk et al. (2016) AJ 151:68\")\n    \n    return known_ebs\n\n# ä¸‹è¼‰ EB è³‡æ–™\nprint(\"\\n\" + \"=\"*60)\nprint(\"ğŸ¯ é–‹å§‹ä¸‹è¼‰ Kepler EB è³‡æ–™\")\nprint(\"=\"*60)\n\neb_df = fetch_kepler_eb_data()\n\n# é¡¯ç¤ºè³‡æ–™æ¨£æœ¬\nprint(\"\\nğŸ“‹ Kepler EB è³‡æ–™æ¨£æœ¬ (å‰10ç­†):\")\nif len(eb_df) > 0:\n    # é¸æ“‡è¦é¡¯ç¤ºçš„æ¬„ä½\n    display_cols = []\n    for col in ['kepid', 'kepoi_name', 'period', 'depth', 'duration', 'morphology', 'source']:\n        if col in eb_df.columns:\n            display_cols.append(col)\n    \n    if display_cols:\n        print(eb_df[display_cols].head(10))\n    \n    # è©³ç´°çµ±è¨ˆ\n    print(f\"\\nğŸ“Š è³‡æ–™çµ±è¨ˆ:\")\n    print(f\"   ç¸½ç­†æ•¸: {len(eb_df)}\")\n    \n    if 'period' in eb_df.columns:\n        valid_period = eb_df['period'].notna()\n        if valid_period.any():\n            print(f\"   é€±æœŸ: {valid_period.sum()} ç­†æœ‰æ•ˆ\")\n            print(f\"      ç¯„åœ: {eb_df.loc[valid_period, 'period'].min():.2f} - {eb_df.loc[valid_period, 'period'].max():.2f} å¤©\")\n            print(f\"      ä¸­ä½æ•¸: {eb_df.loc[valid_period, 'period'].median():.2f} å¤©\")\n    \n    if 'depth' in eb_df.columns:\n        valid_depth = eb_df['depth'].notna()\n        if valid_depth.any():\n            print(f\"   æ·±åº¦: {valid_depth.sum()} ç­†æœ‰æ•ˆ\")\n            print(f\"      ç¯„åœ: {eb_df.loc[valid_depth, 'depth'].min():.0f} - {eb_df.loc[valid_depth, 'depth'].max():.0f} ppm\")\n    \n    if 'source' in eb_df.columns:\n        print(f\"\\n   è³‡æ–™ä¾†æºåˆ†å¸ƒ:\")\n        for source, count in eb_df['source'].value_counts().items():\n            print(f\"      {source}: {count} ç­†\")\n    \n    if 'morphology' in eb_df.columns and eb_df['morphology'].notna().any():\n        print(f\"\\n   EB å½¢æ…‹åˆ†å¸ƒ:\")\n        for morph, count in eb_df['morphology'].value_counts().items():\n            print(f\"      {morph}: {count} ç­†\")\n    \n    print(\"\\n   âœ… é€™äº›éƒ½æ˜¯çœŸå¯¦çš„ Kepler è§€æ¸¬è³‡æ–™ï¼Œéæ¨¡æ“¬ï¼\")\nelse:\n    print(\"   âŒ ç„¡æ³•ç²å–è³‡æ–™\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 è™•ç† EB è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# è™•ç† EB è³‡æ–™\nprint(\"\\nğŸ”§ è™•ç† Kepler EB è³‡æ–™...\")\n\n# æ¨™æº–åŒ–æ¬„ä½åç¨±\neb_df_processed = eb_df.copy()\n\n# æª¢æŸ¥ä¸¦ç§»é™¤é‡è¤‡æ¬„ä½\nif eb_df_processed.columns.duplicated().any():\n    print(\"   âš ï¸ åµæ¸¬åˆ°é‡è¤‡æ¬„ä½ï¼Œæ­£åœ¨è™•ç†...\")\n    # ä¿ç•™ç¬¬ä¸€å€‹å‡ºç¾çš„æ¬„ä½\n    eb_df_processed = eb_df_processed.loc[:, ~eb_df_processed.columns.duplicated()]\n\n# æ·»åŠ æ¨™ç±¤ï¼ˆEB éƒ½æ˜¯è² æ¨£æœ¬ï¼‰\neb_df_processed['label'] = 0\n\n# ç¢ºä¿ source æ¬„ä½å­˜åœ¨ä¸”æ­£ç¢º\nif 'source' not in eb_df_processed.columns:\n    eb_df_processed['source'] = 'Kepler_EB'\n\n# é‡å‘½åæ¬„ä½ä»¥çµ±ä¸€æ ¼å¼ï¼ˆé¿å…é‡è¤‡ï¼‰\ncolumn_mapping = {\n    'kepid': 'target_id',\n    'koi_period': 'period',\n    'koi_depth': 'depth', \n    'koi_duration': 'duration',\n}\n\nfor old_col, new_col in column_mapping.items():\n    if old_col in eb_df_processed.columns and new_col not in eb_df_processed.columns:\n        eb_df_processed = eb_df_processed.rename(columns={old_col: new_col})\n\n# å†æ¬¡æª¢æŸ¥ä¸¦ç§»é™¤ä»»ä½•é‡è¤‡æ¬„ä½\nif eb_df_processed.columns.duplicated().any():\n    duplicate_cols = eb_df_processed.columns[eb_df_processed.columns.duplicated()].unique()\n    print(f\"   ç§»é™¤é‡è¤‡æ¬„ä½: {list(duplicate_cols)}\")\n    eb_df_processed = eb_df_processed.loc[:, ~eb_df_processed.columns.duplicated()]\n\nprint(f\"âœ… è™•ç†å®Œæˆ: {len(eb_df_processed)} ç­† EB è³‡æ–™\")\nprint(f\"   æ‰€æœ‰ EB æ¨™è¨˜ç‚ºè² æ¨£æœ¬ (label=0)\")\nprint(f\"   æ¬„ä½: {list(eb_df_processed.columns)[:10]}...\")  # é¡¯ç¤ºå‰10å€‹æ¬„ä½"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. è³‡æ–™å„²å­˜èˆ‡ç‰ˆæœ¬æ§åˆ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å»ºç«‹è³‡æ–™ç›®éŒ„\ndata_dir = Path(\"../data\")\ndata_dir.mkdir(parents=True, exist_ok=True)\n\n# å„²å­˜æ™‚é–“æˆ³è¨˜\ndownload_timestamp = datetime.now().isoformat()\n\nprint(\"\\nğŸ’¾ å„²å­˜è³‡æ–™...\")\n\n# 1. å„²å­˜å®Œæ•´ TOI è³‡æ–™\ntoi_path = data_dir / \"toi.csv\"\ntoi_df.to_csv(toi_path, index=False)\nprint(f\"   âœ… TOI å®Œæ•´è³‡æ–™: {toi_path} ({len(toi_df)} ç­†)\")\n\n# 2. å„²å­˜ TOI æ­£æ¨£æœ¬\ntoi_positive_path = data_dir / \"toi_positive.csv\"\ntoi_positive.to_csv(toi_positive_path, index=False)\nprint(f\"   âœ… TOI æ­£æ¨£æœ¬: {toi_positive_path} ({len(toi_positive)} ç­†)\")\n\n# 3. å„²å­˜ TOI è² æ¨£æœ¬ (False Positives)\ntoi_negative_path = data_dir / \"toi_negative.csv\"\ntoi_negative_fp.to_csv(toi_negative_path, index=False)\nprint(f\"   âœ… TOI è² æ¨£æœ¬: {toi_negative_path} ({len(toi_negative_fp)} ç­†)\")\n\n# 4. å„²å­˜ Kepler/KOI è² æ¨£æœ¬è³‡æ–™\neb_path = data_dir / \"koi_false_positives.csv\"\neb_df_processed.to_csv(eb_path, index=False)\nprint(f\"   âœ… KOI False Positives: {eb_path} ({len(eb_df_processed)} ç­†)\")\n\n# 5. å»ºç«‹åˆä½µçš„è¨“ç·´è³‡æ–™é›†\nprint(\"\\nğŸ”¨ å»ºç«‹åˆä½µè¨“ç·´è³‡æ–™é›†...\")\n\n# é¸æ“‡é—œéµæ¬„ä½\nkey_columns = ['label', 'source']\noptional_columns = ['period', 'depth', 'duration', 'snr']\n\n# æº–å‚™æ­£æ¨£æœ¬ï¼ˆè™•ç† TOI æ¬„ä½æ˜ å°„ï¼‰\npositive_samples = pd.DataFrame()\npositive_samples['label'] = toi_positive['label']\npositive_samples['source'] = toi_positive['source']\n\n# è™•ç† ID æ¬„ä½\nif 'toi' in toi_positive.columns:\n    positive_samples['toi'] = toi_positive['toi']\nif 'tid' in toi_positive.columns:\n    positive_samples['tid'] = toi_positive['tid']\n    positive_samples['target_id'] = 'TIC' + toi_positive['tid'].astype(str)\nelif 'tic' in toi_positive.columns:\n    positive_samples['tid'] = toi_positive['tic']\n    positive_samples['target_id'] = 'TIC' + toi_positive['tic'].astype(str)\n\n# æ˜ å°„ç‰©ç†åƒæ•¸ï¼ˆæª¢æŸ¥ toi_ å’Œ pl_ å…©ç¨®å‰ç¶´ï¼‰\nfor param in ['period', 'depth', 'duration']:\n    toi_col = f'toi_{param}'\n    pl_col = f'pl_orbper' if param == 'period' else f'pl_trandep' if param == 'depth' else f'pl_trandurh'\n\n    if toi_col in toi_positive.columns:\n        positive_samples[param] = toi_positive[toi_col]\n    elif pl_col in toi_positive.columns:\n        if param == 'duration':\n            # pl_trandurh æ˜¯å°æ™‚ï¼Œéœ€è¦è½‰æ›ç‚ºå¤©\n            positive_samples[param] = toi_positive[pl_col] / 24.0\n        else:\n            positive_samples[param] = toi_positive[pl_col]\n\n# æº–å‚™ TOI è² æ¨£æœ¬ï¼ˆFalse Positivesï¼‰\nnegative_samples_fp = pd.DataFrame()\nnegative_samples_fp['label'] = toi_negative_fp['label']\nnegative_samples_fp['source'] = toi_negative_fp['source']\n\n# è™•ç† ID æ¬„ä½\nif 'toi' in toi_negative_fp.columns:\n    negative_samples_fp['toi'] = toi_negative_fp['toi']\nif 'tid' in toi_negative_fp.columns:\n    negative_samples_fp['tid'] = toi_negative_fp['tid']\n    negative_samples_fp['target_id'] = 'TIC' + toi_negative_fp['tid'].astype(str)\nelif 'tic' in toi_negative_fp.columns:\n    negative_samples_fp['tid'] = toi_negative_fp['tic']\n    negative_samples_fp['target_id'] = 'TIC' + toi_negative_fp['tic'].astype(str)\n\n# æ˜ å°„ç‰©ç†åƒæ•¸ï¼ˆåŒæ¨£æª¢æŸ¥å…©ç¨®å‰ç¶´ï¼‰\nfor param in ['period', 'depth', 'duration']:\n    toi_col = f'toi_{param}'\n    pl_col = f'pl_orbper' if param == 'period' else f'pl_trandep' if param == 'depth' else f'pl_trandurh'\n\n    if toi_col in toi_negative_fp.columns:\n        negative_samples_fp[param] = toi_negative_fp[toi_col]\n    elif pl_col in toi_negative_fp.columns:\n        if param == 'duration':\n            negative_samples_fp[param] = toi_negative_fp[pl_col] / 24.0\n        else:\n            negative_samples_fp[param] = toi_negative_fp[pl_col]\n\n# æº–å‚™ KOI False Positive è² æ¨£æœ¬ï¼ˆä¿®å¾©é‡è¤‡æ¬„ä½å•é¡Œï¼‰\nnegative_samples_koi = pd.DataFrame()\nnegative_samples_koi['label'] = eb_df_processed['label'].values  # ä½¿ç”¨ .values é¿å…ç´¢å¼•å•é¡Œ\nnegative_samples_koi['source'] = eb_df_processed['source'].values\n\n# è™•ç† KOI ID\nif 'kepid' in eb_df_processed.columns:\n    negative_samples_koi['kepid'] = eb_df_processed['kepid'].values\n    negative_samples_koi['target_id'] = 'KIC' + pd.Series(eb_df_processed['kepid'].values).astype(str)\nelif 'target_id' in eb_df_processed.columns:\n    # æª¢æŸ¥æ˜¯å¦æœ‰é‡è¤‡çš„ target_id æ¬„ä½\n    if eb_df_processed['target_id'].ndim > 1:\n        # å¦‚æœæ˜¯ DataFrameï¼Œå–ç¬¬ä¸€æ¬„\n        negative_samples_koi['target_id'] = eb_df_processed['target_id'].iloc[:, 0].values\n    else:\n        negative_samples_koi['target_id'] = eb_df_processed['target_id'].values\nelse:\n    negative_samples_koi['target_id'] = 'KOI' + pd.Series(range(len(eb_df_processed))).astype(str)\n\n# æ˜ å°„ KOI ç‰©ç†åƒæ•¸ï¼ˆå®‰å…¨è™•ç†å¯èƒ½çš„é‡è¤‡æ¬„ä½ï¼‰\nfor param in ['period', 'depth', 'duration']:\n    if param in eb_df_processed.columns:\n        # æª¢æŸ¥æ¬„ä½æ˜¯å¦é‡è¤‡\n        col_data = eb_df_processed[param]\n        if isinstance(col_data, pd.DataFrame):\n            # å¦‚æœè¿”å› DataFrameï¼ˆæœ‰é‡è¤‡æ¬„ä½ï¼‰ï¼Œå–ç¬¬ä¸€æ¬„\n            negative_samples_koi[param] = col_data.iloc[:, 0].values\n        else:\n            # æ­£å¸¸çš„ Series\n            negative_samples_koi[param] = col_data.values\n\n# åˆä½µæ‰€æœ‰æ¨£æœ¬\nprint(\"\\n   åˆä½µè³‡æ–™é›†çµ±è¨ˆ:\")\nprint(f\"   - TOI æ­£æ¨£æœ¬: {len(positive_samples)} ç­†\")\nprint(f\"   - TOI è² æ¨£æœ¬ (FP): {len(negative_samples_fp)} ç­†\")\nprint(f\"   - KOI è² æ¨£æœ¬: {len(negative_samples_koi)} ç­†\")\n\nall_samples = pd.concat([\n    positive_samples,\n    negative_samples_fp,\n    negative_samples_koi\n], ignore_index=True)\n\n# ç§»é™¤å…¨ NaN çš„æ¬„ä½\nall_samples = all_samples.dropna(axis=1, how='all')\n\n# å„²å­˜åˆä½µè³‡æ–™é›†\ncombined_path = data_dir / \"supervised_dataset.csv\"\nall_samples.to_csv(combined_path, index=False)\nprint(f\"\\nâœ… åˆä½µè³‡æ–™é›†: {combined_path}\")\nprint(f\"   ç¸½æ¨£æœ¬æ•¸: {len(all_samples)} ç­†\")\nprint(f\"   æ­£æ¨£æœ¬: {(all_samples['label'] == 1).sum()} ç­†\")\nprint(f\"   è² æ¨£æœ¬: {(all_samples['label'] == 0).sum()} ç­†\")\n\n# è³‡æ–™å“è³ªå ±å‘Š\nprint(\"\\nğŸ“Š è³‡æ–™å®Œæ•´æ€§:\")\nfor col in ['period', 'depth', 'duration']:\n    if col in all_samples.columns:\n        valid = all_samples[col].notna().sum()\n        print(f\"   {col}: {valid}/{len(all_samples)} ({valid/len(all_samples)*100:.1f}%)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. è³‡æ–™ä¾†æºæ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å»ºç«‹è³‡æ–™ä¾†æºæ–‡ä»¶\nprovenance = {\n    \"download_timestamp\": download_timestamp,\n    \"data_sources\": {\n        \"toi\": {\n            \"source\": \"NASA Exoplanet Archive TOI Table\",\n            \"url\": \"https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=TOI\",\n            \"api_endpoint\": \"https://exoplanetarchive.ipac.caltech.edu/TAP\",\n            \"access_method\": \"astroquery.ipac.nexsci.nasa_exoplanet_archive\",\n            \"n_records\": len(toi_df),\n            \"n_positive\": len(toi_positive),\n            \"n_negative_fp\": len(toi_negative_fp),\n            \"column_mapping\": {\n                \"toi_period\": \"pl_orbper (days)\",\n                \"toi_depth\": \"pl_trandep (ppm)\",\n                \"toi_duration\": \"pl_trandurh (hours, converted to days)\",\n                \"toi_prad\": \"pl_rade (Earth radii)\"\n            },\n            \"columns_available\": list(toi_df.columns)[:20]  # åªåˆ—å‡ºå‰20å€‹æ¬„ä½\n        },\n        \"koi_false_positives\": {\n            \"source\": \"NASA Exoplanet Archive KOI Cumulative Table\",\n            \"url\": \"https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=cumulative\",\n            \"query\": \"WHERE koi_disposition='FALSE POSITIVE'\",\n            \"description\": \"KOI False Positives including eclipsing binaries\",\n            \"n_records\": len(eb_df_processed),\n            \"fallback_source\": \"Kirk et al. (2016) Kepler EB Catalog\",\n            \"columns\": list(eb_df_processed.columns)\n        },\n        \"combined_dataset\": {\n            \"file\": \"supervised_dataset.csv\",\n            \"n_total\": len(all_samples),\n            \"n_positive\": int((all_samples['label'] == 1).sum()),\n            \"n_negative\": int((all_samples['label'] == 0).sum()),\n            \"balance_ratio\": float((all_samples['label'] == 1).sum() / len(all_samples)),\n            \"sources\": {k: int(v) for k, v in all_samples['source'].value_counts().to_dict().items()}\n        }\n    },\n    \"known_issues\": [\n        \"TOI table uses pl_* prefix for physical parameters, not toi_*\",\n        \"pl_trandurh is in hours, requires conversion to days\",\n        \"Villanova EB catalog is inaccessible as of 2025\",\n        \"Many TOI entries have missing physical parameters\",\n        \"Using KOI False Positives as substitute for EB catalog\"\n    ],\n    \"column_definitions\": {\n        \"tfopwg_disp\": \"TFOPWG disposition (PC/CP/KP/FP/APC/FA)\",\n        \"PC\": \"Planet Candidate\",\n        \"CP\": \"Confirmed Planet\",\n        \"KP\": \"Known Planet\",\n        \"FP\": \"False Positive\",\n        \"APC\": \"Ambiguous Planet Candidate\",\n        \"FA\": \"False Alarm\",\n        \"pl_orbper\": \"Planetary orbital period in days\",\n        \"pl_trandep\": \"Transit depth in ppm\",\n        \"pl_trandurh\": \"Transit duration in hours\"\n    },\n    \"references\": [\n        \"NASA Exoplanet Archive: https://exoplanetarchive.ipac.caltech.edu/\",\n        \"TOI Column Definitions: https://exoplanetarchive.ipac.caltech.edu/docs/API_TOI_columns.html\",\n        \"Kirk et al. (2016) AJ 151:68 - Kepler Eclipsing Binary Catalog\",\n        \"Astroquery Documentation: https://astroquery.readthedocs.io/\"\n    ]\n}\n\n# å„²å­˜è³‡æ–™ä¾†æºæ–‡ä»¶\nprovenance_path = data_dir / \"data_provenance.json\"\nwith open(provenance_path, 'w') as f:\n    json.dump(provenance, f, indent=2, default=str)\n\nprint(\"\\nğŸ“ è³‡æ–™ä¾†æºæ–‡ä»¶å·²å»ºç«‹: data/data_provenance.json\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. è³‡æ–™æ‘˜è¦å ±å‘Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*60)\nprint(\"ğŸ“Š è³‡æ–™ä¸‹è¼‰æ‘˜è¦å ±å‘Š\")\nprint(\"=\"*60)\n\nprint(f\"\"\"\nğŸ“… ä¸‹è¼‰æ™‚é–“: {download_timestamp}\n\nğŸ¯ TOI (TESS Objects of Interest) çœŸå¯¦è³‡æ–™:\n   â€¢ ç¸½ç­†æ•¸: {len(toi_df):,}\n   â€¢ æ­£æ¨£æœ¬ (PC/CP/KP): {len(toi_positive):,}\n   â€¢ è² æ¨£æœ¬ (FP): {len(toi_negative_fp):,}\n   â€¢ è³‡æ–™ä¾†æº: NASA Exoplanet Archive (TAP Service)\n   â€¢ æ¬„ä½æ˜ å°„: pl_orbper â†’ toi_period, pl_trandep â†’ toi_depth\n   â€¢ å–®ä½è½‰æ›: pl_trandurh (å°æ™‚) â†’ toi_duration (å¤©)\n\nğŸŒŸ KOI False Positives (æ›¿ä»£ Kepler EB):\n   â€¢ ç¸½ç­†æ•¸: {len(eb_df_processed):,}\n   â€¢ å…¨éƒ¨æ¨™è¨˜ç‚ºè² æ¨£æœ¬ (åŒ…å« eclipsing binaries)\n   â€¢ è³‡æ–™ä¾†æº: NASA Archive KOI Cumulative Table\n   â€¢ æŸ¥è©¢æ¢ä»¶: koi_disposition = 'FALSE POSITIVE'\n   â€¢ å‚™ç”¨ä¾†æº: Kirk et al. (2016) ç¢ºèªçš„ EB ç³»çµ±\n\nğŸ“¦ åˆä½µè¨“ç·´è³‡æ–™é›†:\n   â€¢ ç¸½æ¨£æœ¬æ•¸: {len(all_samples):,}\n   â€¢ æ­£æ¨£æœ¬: {(all_samples['label'] == 1).sum():,} ({(all_samples['label'] == 1).sum()/len(all_samples)*100:.1f}%)\n   â€¢ è² æ¨£æœ¬: {(all_samples['label'] == 0).sum():,} ({(all_samples['label'] == 0).sum()/len(all_samples)*100:.1f}%)\n\n   è³‡æ–™ä¾†æºåˆ†å¸ƒ:\n\"\"\")\n\n# é¡¯ç¤ºè³‡æ–™ä¾†æºåˆ†å¸ƒ\nif 'source' in all_samples.columns:\n    source_counts = all_samples['source'].value_counts()\n    for source, count in source_counts.items():\n        print(f\"   â€¢ {source}: {count:,} ç­†\")\n\nprint(f\"\"\"\n\nğŸ’¾ è¼¸å‡ºæª”æ¡ˆ:\n   â€¢ data/toi.csv - å®Œæ•´ TOI è³‡æ–™ (å« pl_* åŸå§‹æ¬„ä½)\n   â€¢ data/toi_positive.csv - TOI æ­£æ¨£æœ¬ (PC/CP/KP)\n   â€¢ data/toi_negative.csv - TOI è² æ¨£æœ¬ (FP)\n   â€¢ data/koi_false_positives.csv - KOI False Positives (æ›¿ä»£ EB)\n   â€¢ data/supervised_dataset.csv - åˆä½µè¨“ç·´è³‡æ–™é›†\n   â€¢ data/data_provenance.json - è©³ç´°è³‡æ–™ä¾†æºæ–‡ä»¶\n\nâš ï¸ é‡è¦ç™¼ç¾èˆ‡è§£æ±ºæ–¹æ¡ˆ:\n   1. TOI ä½¿ç”¨ pl_* å‰ç¶´è€Œé toi_* (å·²æ˜ å°„è™•ç†)\n   2. pl_trandurh å–®ä½æ˜¯å°æ™‚éœ€è½‰æ› (å·²è™•ç† /24)\n   3. Villanova EB ç›®éŒ„ç„¡æ³•å­˜å– (æ”¹ç”¨ KOI FP)\n   4. éƒ¨åˆ† TOI ç¼ºå°‘ç‰©ç†åƒæ•¸ (éœ€å¾å…‰æ›²ç·šè¨ˆç®—)\n\nğŸ“Š è³‡æ–™å“è³ªè©•ä¼°:\n\"\"\")\n\n# é¡¯ç¤ºè³‡æ–™å®Œæ•´æ€§\nfor col in ['period', 'depth', 'duration']:\n    if col in all_samples.columns:\n        valid_count = all_samples[col].notna().sum()\n        valid_pct = valid_count / len(all_samples) * 100\n        print(f\"   â€¢ {col}: {valid_count:,}/{len(all_samples):,} ({valid_pct:.1f}%) æœ‰æ•ˆå€¼\")\n\nprint(f\"\"\"\n\nğŸš€ ä¸‹ä¸€æ­¥å»ºè­°:\n   1. åŸ·è¡Œ 02_bls_baseline.ipynb è¨ˆç®— BLS/TLS ç‰¹å¾µ\n   2. è‹¥ç‰©ç†åƒæ•¸ä¸è¶³ï¼Œå¾å…‰æ›²ç·šç›´æ¥è¨ˆç®—\n   3. è€ƒæ…®è³‡æ–™å¢å¼·æˆ– SMOTE å¹³è¡¡æ­£è² æ¨£æœ¬\n   4. é©—è­‰è³‡æ–™å“è³ªå¾Œå†è¨“ç·´æ¨¡å‹\n\nâœ… çœŸå¯¦è³‡æ–™ä¸‹è¼‰å®Œæˆï¼æ‰€æœ‰è³‡æ–™ä¾†è‡ª NASA å®˜æ–¹è³‡æ–™åº«ï¼Œç„¡æ¨¡æ“¬è³‡æ–™ï¼\"\"\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 7. è³‡æ–™æŒä¹…åŒ–å„²å­˜ - é¸æ“‡æœ€é©åˆä½ çš„æ–¹å¼\n\nè³‡æ–™ä¸‹è¼‰å®Œæˆå¾Œï¼Œé¸æ“‡ä»¥ä¸‹**å…¶ä¸­ä¸€ç¨®**æ–¹å¼ä¾†æŒä¹…åŒ–å„²å­˜ï¼š\n\n### ğŸ“Š ä¸‰ç¨®å„²å­˜æ–¹å¼æ¯”è¼ƒ\n\n| æ–¹å¼ | é©ç”¨æƒ…å¢ƒ | å„ªé» | ç¼ºé» |\n|-----|---------|------|------|\n| ğŸ¯ **GitHub Push** | Colab + æƒ³è¦ç‰ˆæœ¬æ§åˆ¶ | å®‰å…¨ã€å¯åˆ†äº«ã€æœ‰ç‰ˆæœ¬æ­·å² | éœ€è¦ Token |\n| ğŸ“‚ **Google Drive** | Colab + å€‹äººä½¿ç”¨ | ç°¡å–®ã€æ°¸ä¹…å„²å­˜ | åªæœ‰è‡ªå·±èƒ½å­˜å– |  \n| âœ… **æœ¬åœ°é©—è­‰** | æª¢æŸ¥è³‡æ–™å®Œæ•´æ€§ | å¿«é€Ÿé©—è­‰ | ä¸æŒä¹…åŒ– |\n\n### ğŸ’¡ å»ºè­°é¸æ“‡ï¼š\n- **ğŸš€ æ¨è–¦**ï¼šé¸æ“‡ **GitHub Push**ï¼ˆé¸é … Aï¼‰- é©åˆåˆ†äº«å’Œå”ä½œ\n- **ğŸ”’ ç§äºº**ï¼šé¸æ“‡ **Google Drive**ï¼ˆé¸é … Bï¼‰- é©åˆå€‹äººå„²å­˜  \n- **ğŸ” æª¢æŸ¥**ï¼šé¸æ“‡ **é©—è­‰**ï¼ˆé¸é … Cï¼‰- é©åˆç¢ºèªè³‡æ–™å“è³ª\n\n---\n\n**æ¥ä¸‹ä¾†åŸ·è¡Œå°æ‡‰çš„é¸é … cell å³å¯ï¼**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ğŸ” GitHub Token è¼¸å…¥ç•Œé¢\n\"\"\"\nå®‰å…¨ä¸”ç”¨æˆ¶å‹å¥½çš„ GitHub Token è¼¸å…¥æ–¹å¼\né©ç”¨æ–¼ Google Colab ç’°å¢ƒ\n\"\"\"\n\n# æª¢æŸ¥æ˜¯å¦åœ¨ Colab ç’°å¢ƒ\ntry:\n    from google.colab import drive\n    IN_COLAB = True\nexcept ImportError:\n    IN_COLAB = False\n\ndef get_github_token():\n    \"\"\"å®‰å…¨åœ°ç²å– GitHub Personal Access Token\"\"\"\n    try:\n        import getpass\n        print(\"ğŸ” GitHub Token è¼¸å…¥\")\n        print(\"=\" * 40)\n        print(\"ğŸ“ å¦‚ä½•ç²å– GitHub Personal Access Token:\")\n        print(\"   1. å‰å¾€ https://github.com/settings/tokens/new\")\n        print(\"   2. Token name: å¡«å…¥ 'Space Apps Exoplanet Data Upload'\")\n        print(\"   3. Expiration: é¸æ“‡ '30 days' æˆ– 'Custom'\")\n        print(\"   4. å‹¾é¸æ¬Šé™: 'repo' (Full control of private repositories)\")\n        print(\"   5. é»æ“Š 'Generate token' ä¸¦è¤‡è£½\")\n        print(\"\")\n        print(\"âš ï¸ é‡è¦: Token ä»¥ 'ghp_' é–‹é ­ï¼Œå…± 40 å­—ç¬¦\")\n        print(\"ğŸ”’ è¼¸å…¥æ™‚æ–‡å­—æœƒè¢«éš±è—ä»¥ä¿è­·ä½ çš„ Token\")\n        print(\"\")\n\n        # å®‰å…¨è¼¸å…¥ï¼ˆéš±è—å­—ç¬¦ï¼‰\n        token = getpass.getpass(\"ğŸ”‘ è«‹è²¼ä¸Šä½ çš„ GitHub Token: \")\n\n        # é©—è­‰ token æ ¼å¼\n        if not token:\n            print(\"âŒ Token ä¸èƒ½ç‚ºç©º\")\n            return None\n        elif not token.startswith('ghp_'):\n            print(\"âš ï¸ è­¦å‘Š: Token æ‡‰è©²ä»¥ 'ghp_' é–‹é ­\")\n            confirm = input(\"æ˜¯å¦ç¹¼çºŒä½¿ç”¨æ­¤ token? (y/N): \")\n            if confirm.lower() != 'y':\n                return None\n        elif len(token) != 40:\n            print(\"âš ï¸ è­¦å‘Š: GitHub Token é€šå¸¸æ˜¯ 40 å­—ç¬¦é•·\")\n            confirm = input(\"æ˜¯å¦ç¹¼çºŒä½¿ç”¨æ­¤ token? (y/N): \")\n            if confirm.lower() != 'y':\n                return None\n\n        print(\"âœ… Token æ ¼å¼æª¢æŸ¥é€šé\")\n        return token\n\n    except KeyboardInterrupt:\n        print(\"\\nâŒ ç”¨æˆ¶å–æ¶ˆè¼¸å…¥\")\n        return None\n    except ImportError:\n        print(\"âš ï¸ getpass ä¸å¯ç”¨ï¼Œä½¿ç”¨æ˜æ–‡è¼¸å…¥\")\n        token = input(\"ğŸ”‘ è«‹è²¼ä¸Šä½ çš„ GitHub Token: \")\n        if token and len(token) > 20:\n            return token\n        else:\n            print(\"âŒ Token ç„¡æ•ˆ\")\n            return None\n\ndef setup_git_lfs_fixed():\n    \"\"\"è¨­å®š Git LFS ä¾†è™•ç†å¤§æª”æ¡ˆ (ä¿®å¾©ç‰ˆæœ¬)\"\"\"\n    import subprocess\n    \n    try:\n        # æª¢æŸ¥ç•¶å‰ç’°å¢ƒ\n        print(f\"ğŸ” æª¢æŸ¥ç’°å¢ƒ: {'Google Colab' if IN_COLAB else 'æœ¬åœ°ç’°å¢ƒ'}\")\n        \n        # åœ¨ Colab ä¸­å®‰è£ git-lfs\n        if IN_COLAB:\n            print(\"ğŸ“¦ åœ¨ Colab ä¸­å®‰è£ Git LFS...\")\n            subprocess.run(['apt-get', 'update'], check=True, capture_output=True)\n            subprocess.run(['apt-get', 'install', '-y', 'git-lfs'], check=True, capture_output=True)\n            subprocess.run(['git', 'lfs', 'install'], check=True)\n            print(\"   âœ… Git LFS å·²å®‰è£\")\n        else:\n            # æœ¬åœ°ç’°å¢ƒæª¢æŸ¥ git-lfs\n            result = subprocess.run(['git', 'lfs', 'version'], capture_output=True, text=True)\n            if result.returncode == 0:\n                print(\"   âœ… Git LFS å·²å®‰è£\")\n            else:\n                print(\"   âš ï¸ è«‹å…ˆå®‰è£ Git LFS: https://git-lfs.github.com/\")\n                return False\n        \n        # è¿½è¹¤ CSV æª”æ¡ˆ\n        subprocess.run(['git', 'lfs', 'track', '*.csv'], cwd='..', check=True)\n        print(\"   âœ… å·²è¨­å®šè¿½è¹¤ CSV æª”æ¡ˆ\")\n        \n        return True\n    except Exception as e:\n        print(f\"   âš ï¸ Git LFS è¨­å®šå¤±æ•—: {e}\")\n        return False\n\ndef quick_push_to_github():\n    \"\"\"ä¸€éµæ¨é€åˆ° GitHub - åŒ…å«å®Œæ•´æµç¨‹\"\"\"\n    print(\"ğŸš€ ä¸€éµæ¨é€åˆ° GitHub\")\n    print(\"=\" * 50)\n\n    # æ­¥é©Ÿ 1: ç²å– token\n    print(\"\\nğŸ“‹ æ­¥é©Ÿ 1/4: ç²å– GitHub Token\")\n    token = get_github_token()\n    if not token:\n        print(\"âŒ ç„¡æ³•ç²å–æœ‰æ•ˆçš„ GitHub Token\")\n        print(\"ğŸ’¡ æç¤º: è«‹ç¢ºä¿ Token æ­£ç¢ºä¸¦æœ‰ 'repo' æ¬Šé™\")\n        return False\n\n    # æ­¥é©Ÿ 2: è¨­å®š Git LFS\n    print(\"\\nğŸ“‹ æ­¥é©Ÿ 2/4: è¨­å®š Git LFS\")\n    if not setup_git_lfs_fixed():\n        print(\"âŒ Git LFS è¨­å®šå¤±æ•—\")\n        return False\n\n    # æ­¥é©Ÿ 3: æ¨é€è³‡æ–™\n    print(\"\\nğŸ“‹ æ­¥é©Ÿ 3/4: æ¨é€è³‡æ–™åˆ° GitHub\")\n    # é€™è£¡éœ€è¦å°å…¥ push_to_github å‡½æ•¸ï¼Œæˆ–è€…åœ¨åŒä¸€ cell ä¸­å®šç¾©\n    try:\n        success = push_to_github(token=token)\n    except NameError:\n        print(\"âš ï¸ push_to_github å‡½æ•¸æœªå®šç¾©ï¼Œè«‹å…ˆåŸ·è¡ŒåŒ…å«è©²å‡½æ•¸çš„ cell\")\n        return False\n\n    # æ­¥é©Ÿ 4: å®Œæˆ\n    print(\"\\nğŸ“‹ æ­¥é©Ÿ 4/4: å®Œæˆ\")\n    if success:\n        print(\"\\nğŸ‰ è³‡æ–™æ¨é€æˆåŠŸï¼\")\n        print(\"\\nğŸ“ ä¸‹ä¸€æ­¥:\")\n        print(\"   1. å‰å¾€ä½ çš„ GitHub å€‰åº«ç¢ºèªæª”æ¡ˆ\")\n        print(\"   2. æª¢æŸ¥ data/ ç›®éŒ„ä¸‹çš„ CSV æª”æ¡ˆ\")\n        print(\"   3. å¯ä»¥é–‹å§‹åŸ·è¡Œ 02_bls_baseline.ipynb\")\n        print(f\"   4. å€‰åº«é€£çµ: https://github.com/exoplanet-spaceapps/exoplanet-starter\")\n        return True\n    else:\n        print(\"âŒ æ¨é€å¤±æ•—ï¼Œè«‹æª¢æŸ¥éŒ¯èª¤è¨Šæ¯\")\n        print(\"ğŸ’¡ å¸¸è¦‹å•é¡Œ:\")\n        print(\"   - Token æ¬Šé™ä¸è¶³ (éœ€è¦ 'repo' æ¬Šé™)\")\n        print(\"   - ç¶²è·¯é€£æ¥å•é¡Œ\")\n        print(\"   - å€‰åº«æ¬Šé™å•é¡Œ\")\n        return False\n\n# é¡¯ç¤ºä½¿ç”¨èªªæ˜\nprint(\"ğŸ’¡ GitHub Token è¼¸å…¥ç•Œé¢å·²è¼‰å…¥ï¼\")\nprint(f\"ğŸŒ ç•¶å‰ç’°å¢ƒ: {'Google Colab' if IN_COLAB else 'æœ¬åœ°ç’°å¢ƒ'}\")\nprint(\"\")\nprint(\"ğŸ¯ å¿«é€Ÿä½¿ç”¨:\")\nprint(\"   quick_push_to_github()    # ä¸€éµå®Œæˆæ‰€æœ‰æ­¥é©Ÿ\")\nprint(\"\")\nprint(\"ğŸ”§ åˆ†æ­¥ä½¿ç”¨:\")\nprint(\"   1. token = get_github_token()    # å®‰å…¨è¼¸å…¥ Token\")\nprint(\"   2. setup_git_lfs_fixed()         # è¨­å®š Git LFS (ä¿®å¾©ç‰ˆ)\")\nprint(\"   3. push_to_github(token=token)   # æ¨é€è³‡æ–™ (éœ€è¦å…ˆåŸ·è¡Œå®šç¾©è©²å‡½æ•¸çš„ cell)\")\nprint(\"\")\nprint(\"âš ï¸ æ³¨æ„: åœ¨ Colab ä¸­åŸ·è¡Œæ™‚ï¼ŒToken è¼¸å…¥æœƒè¢«éš±è—ä¿è­·\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### é¸é … Aï¼šğŸ¯ æ¨é€åˆ° GitHubï¼ˆæ¨è–¦ï¼‰\n\n**é©åˆ**: æƒ³è¦åˆ†äº«è³‡æ–™ã€ç‰ˆæœ¬æ§åˆ¶ã€åœ˜éšŠå”ä½œ\n\n**ç‰¹é»**: \n- âœ… å®‰å…¨çš„ Token è¼¸å…¥ï¼ˆéš±è—å­—ç¬¦ï¼‰\n- âœ… è‡ªå‹•è™•ç†å¤§æª”æ¡ˆï¼ˆGit LFSï¼‰ \n- âœ… å®Œæ•´çš„éŒ¯èª¤è™•ç†å’ŒæŒ‡å°\n- âœ… ä¸€éµå®Œæˆæ‰€æœ‰æ­¥é©Ÿ\n\n**ä½¿ç”¨æ–¹å¼**: åŸ·è¡Œä¸‹æ–¹ cell ä¸­çš„ `quick_push_to_github()`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# é¸é … Bï¼šğŸ“‚ å„²å­˜åˆ° Google Drive\n\"\"\"\né©åˆå€‹äººä½¿ç”¨ï¼Œè³‡æ–™æ°¸ä¹…å„²å­˜åœ¨ä½ çš„ Google Drive\n\"\"\"\n\n# æª¢æŸ¥æ˜¯å¦åœ¨ Colab ç’°å¢ƒ\ntry:\n    from google.colab import drive\n    IN_COLAB = True\nexcept ImportError:\n    IN_COLAB = False\n    print(\"âš ï¸ ä¸åœ¨ Google Colab ç’°å¢ƒï¼Œè·³é Drive æ›è¼‰\")\n\ndef save_to_google_drive():\n    \"\"\"ä¸€éµå„²å­˜è³‡æ–™åˆ° Google Drive\"\"\"\n    if not IN_COLAB:\n        print(\"âŒ æ­¤åŠŸèƒ½åƒ…é©ç”¨æ–¼ Google Colab ç’°å¢ƒ\")\n        print(\"ğŸ’¡ æœ¬åœ°ç’°å¢ƒè«‹ä½¿ç”¨ GitHub Push æˆ–ç›´æ¥è¤‡è£½ data/ ç›®éŒ„\")\n        return False\n    \n    try:\n        import shutil\n        from pathlib import Path\n        from datetime import datetime\n        \n        print(\"ğŸ”§ æ­¥é©Ÿ 1/3: æ›è¼‰ Google Drive\")\n        drive.mount('/content/drive')\n        \n        print(\"ğŸ”§ æ­¥é©Ÿ 2/3: å»ºç«‹å°ˆæ¡ˆç›®éŒ„\")\n        # å»ºç«‹å°ˆæ¡ˆç›®éŒ„\n        project_dir = Path('/content/drive/MyDrive/spaceapps-exoplanet')\n        project_dir.mkdir(parents=True, exist_ok=True)\n        \n        # å»ºç«‹è³‡æ–™ç›®éŒ„\n        drive_data_dir = project_dir / 'data'\n        drive_data_dir.mkdir(parents=True, exist_ok=True)\n        \n        # å»ºç«‹æ—¥æœŸç‰ˆæœ¬ç›®éŒ„\n        date_str = datetime.now().strftime('%Y%m%d_%H%M')\n        version_dir = drive_data_dir / f'v_{date_str}'\n        version_dir.mkdir(parents=True, exist_ok=True)\n        \n        print(\"ğŸ”§ æ­¥é©Ÿ 3/3: è¤‡è£½è³‡æ–™æª”æ¡ˆ\")\n        # è¤‡è£½æ‰€æœ‰è³‡æ–™æª”æ¡ˆ\n        data_files = ['toi.csv', 'toi_positive.csv', 'toi_negative.csv', \n                     'koi_false_positives.csv', 'supervised_dataset.csv', 'data_provenance.json']\n        \n        copied_files = []\n        for filename in data_files:\n            src = Path('../data') / filename\n            if src.exists():\n                dst = version_dir / filename\n                shutil.copy2(src, dst)\n                size_mb = dst.stat().st_size / 1024 / 1024\n                copied_files.append(f\"{filename} ({size_mb:.1f}MB)\")\n                print(f\"   âœ… {filename}\")\n        \n        # å»ºç«‹æœ€æ–°ç‰ˆæœ¬é€£çµ\n        latest_link = drive_data_dir / 'latest'\n        if latest_link.exists():\n            latest_link.unlink()\n        try:\n            latest_link.symlink_to(version_dir)\n        except:\n            # å¦‚æœ symlink å¤±æ•—ï¼Œè¤‡è£½åˆ° latest ç›®éŒ„\n            latest_dir = drive_data_dir / 'latest'\n            if latest_dir.exists():\n                shutil.rmtree(latest_dir)\n            shutil.copytree(version_dir, latest_dir)\n        \n        print(f\"\\nğŸ‰ è³‡æ–™å·²å„²å­˜åˆ° Google Drive!\")\n        print(f\"ğŸ“‚ ä½ç½®: {version_dir}\")\n        print(f\"ğŸ“Š æª”æ¡ˆ: {len(copied_files)} å€‹\")\n        \n        total_size = sum(f.stat().st_size for f in version_dir.glob('*'))\n        print(f\"ğŸ’¾ ç¸½å¤§å°: {total_size / 1024 / 1024:.1f} MB\")\n        \n        print(f\"\\nğŸ“ ä¸‹ä¸€æ­¥:\")\n        print(f\"   1. è³‡æ–™å·²æ°¸ä¹…å„²å­˜åœ¨ä½ çš„ Google Drive\")\n        print(f\"   2. å¯ä»¥åœ¨ Drive ä¸­ spaceapps-exoplanet/data/ æ‰¾åˆ°\")\n        print(f\"   3. åŸ·è¡Œ 02_bls_baseline.ipynb é€²è¡Œåˆ†æ\")\n        \n        return True\n        \n    except Exception as e:\n        print(f\"âŒ å„²å­˜å¤±æ•—: {e}\")\n        return False\n\n# ä½¿ç”¨èªªæ˜\nif IN_COLAB:\n    print(\"ğŸ“‚ Google Drive å„²å­˜åŠŸèƒ½å·²è¼‰å…¥ï¼\")\n    print(\"\")\n    print(\"ğŸ¯ ä¸€éµå„²å­˜:\")\n    print(\"   save_to_google_drive()\")\n    print(\"\")\n    print(\"ğŸ’¡ ç‰¹é»:\")\n    print(\"   - æ°¸ä¹…å„²å­˜åœ¨ä½ çš„ Google Drive\")\n    print(\"   - è‡ªå‹•å»ºç«‹æ—¥æœŸç‰ˆæœ¬ç›®éŒ„\")\n    print(\"   - åŒ…å«æ‰€æœ‰ä¸‹è¼‰çš„è³‡æ–™æª”æ¡ˆ\")\n    print(\"   - é©åˆå€‹äººä½¿ç”¨å’Œå‚™ä»½\")\nelse:\n    print(\"ğŸ’» æœ¬åœ°ç’°å¢ƒ - Google Drive åŠŸèƒ½ä¸å¯ç”¨\")\n    print(\"å»ºè­°ä½¿ç”¨ GitHub Push æˆ–ç›´æ¥ç®¡ç† data/ ç›®éŒ„\")\n\n# å¿«é€ŸåŸ·è¡Œï¼ˆå–æ¶ˆè¨»è§£ä½¿ç”¨ï¼‰\n# save_to_google_drive()  # å–æ¶ˆè¨»è§£ä¾†å„²å­˜åˆ° Google Drive",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# å®Œæ•´çš„ GitHub æ¨é€åŠŸèƒ½å¯¦ä½œ\n\"\"\"\nåŒ…å«æ‰€æœ‰å¿…è¦å‡½æ•¸çš„å®Œæ•´å¯¦ä½œ\n\"\"\"\nimport subprocess\nimport os\nfrom pathlib import Path\n\n# æª¢æŸ¥æ˜¯å¦åœ¨ Colab ç’°å¢ƒ\ntry:\n    from google.colab import drive\n    IN_COLAB = True\nexcept ImportError:\n    IN_COLAB = False\n\ndef get_github_token():\n    \"\"\"å®‰å…¨åœ°ç²å– GitHub Personal Access Token\"\"\"\n    try:\n        import getpass\n        print(\"ğŸ” GitHub Token è¼¸å…¥\")\n        print(\"=\" * 40)\n        print(\"ğŸ“ å¦‚ä½•ç²å– GitHub Personal Access Token:\")\n        print(\"   1. å‰å¾€ https://github.com/settings/tokens/new\")\n        print(\"   2. Token name: å¡«å…¥ 'Space Apps Exoplanet Data Upload'\")\n        print(\"   3. Expiration: é¸æ“‡ '30 days' æˆ– 'Custom'\")\n        print(\"   4. å‹¾é¸æ¬Šé™: 'repo' (Full control of private repositories)\")\n        print(\"   5. é»æ“Š 'Generate token' ä¸¦è¤‡è£½\")\n        print(\"\")\n        print(\"âš ï¸ é‡è¦: Token ä»¥ 'ghp_' é–‹é ­ï¼Œå…± 40 å­—ç¬¦\")\n        print(\"ğŸ”’ è¼¸å…¥æ™‚æ–‡å­—æœƒè¢«éš±è—ä»¥ä¿è­·ä½ çš„ Token\")\n        print(\"\")\n\n        # å®‰å…¨è¼¸å…¥ï¼ˆéš±è—å­—ç¬¦ï¼‰\n        token = getpass.getpass(\"ğŸ”‘ è«‹è²¼ä¸Šä½ çš„ GitHub Token: \")\n\n        # é©—è­‰ token æ ¼å¼\n        if not token:\n            print(\"âŒ Token ä¸èƒ½ç‚ºç©º\")\n            return None\n        elif not token.startswith('ghp_'):\n            print(\"âš ï¸ è­¦å‘Š: Token æ‡‰è©²ä»¥ 'ghp_' é–‹é ­\")\n            confirm = input(\"æ˜¯å¦ç¹¼çºŒä½¿ç”¨æ­¤ token? (y/N): \")\n            if confirm.lower() != 'y':\n                return None\n        elif len(token) != 40:\n            print(\"âš ï¸ è­¦å‘Š: GitHub Token é€šå¸¸æ˜¯ 40 å­—ç¬¦é•·\")\n            confirm = input(\"æ˜¯å¦ç¹¼çºŒä½¿ç”¨æ­¤ token? (y/N): \")\n            if confirm.lower() != 'y':\n                return None\n\n        print(\"âœ… Token æ ¼å¼æª¢æŸ¥é€šé\")\n        return token\n\n    except KeyboardInterrupt:\n        print(\"\\nâŒ ç”¨æˆ¶å–æ¶ˆè¼¸å…¥\")\n        return None\n    except ImportError:\n        print(\"âš ï¸ getpass ä¸å¯ç”¨ï¼Œä½¿ç”¨æ˜æ–‡è¼¸å…¥\")\n        token = input(\"ğŸ”‘ è«‹è²¼ä¸Šä½ çš„ GitHub Token: \")\n        if token and len(token) > 20:\n            return token\n        else:\n            print(\"âŒ Token ç„¡æ•ˆ\")\n            return None\n\ndef setup_git_lfs():\n    \"\"\"è¨­å®š Git LFS ä¾†è™•ç†å¤§æª”æ¡ˆ\"\"\"\n    try:\n        # æª¢æŸ¥ç•¶å‰ç’°å¢ƒ\n        print(f\"ğŸ” æª¢æŸ¥ç’°å¢ƒ: {'Google Colab' if IN_COLAB else 'æœ¬åœ°ç’°å¢ƒ'}\")\n        \n        # åœ¨ Colab ä¸­å®‰è£ git-lfs\n        if IN_COLAB:\n            print(\"ğŸ“¦ åœ¨ Colab ä¸­å®‰è£ Git LFS...\")\n            subprocess.run(['apt-get', 'update'], check=True, capture_output=True)\n            subprocess.run(['apt-get', 'install', '-y', 'git-lfs'], check=True, capture_output=True)\n            subprocess.run(['git', 'lfs', 'install'], check=True)\n            print(\"   âœ… Git LFS å·²å®‰è£\")\n        else:\n            # æœ¬åœ°ç’°å¢ƒæª¢æŸ¥ git-lfs\n            result = subprocess.run(['git', 'lfs', 'version'], capture_output=True, text=True)\n            if result.returncode == 0:\n                print(\"   âœ… Git LFS å·²å®‰è£\")\n            else:\n                print(\"   âš ï¸ è«‹å…ˆå®‰è£ Git LFS: https://git-lfs.github.com/\")\n                return False\n        \n        # è¿½è¹¤ CSV æª”æ¡ˆ\n        subprocess.run(['git', 'lfs', 'track', '*.csv'], cwd='..', check=True)\n        print(\"   âœ… å·²è¨­å®šè¿½è¹¤ CSV æª”æ¡ˆ\")\n        \n        return True\n    except Exception as e:\n        print(f\"   âš ï¸ Git LFS è¨­å®šå¤±æ•—: {e}\")\n        return False\n\ndef push_to_github(token, commit_message=None):\n    \"\"\"æ¨é€è³‡æ–™åˆ° GitHub å€‰åº«\"\"\"\n    if not commit_message:\n        commit_message = \"data: update NASA exoplanet data\\n\\n- TOI data from NASA Exoplanet Archive\\n- KOI False Positives for negative samples\\n- Complete supervised training dataset\\n- Data provenance and quality documentation\\n\\nğŸ¤– Generated with [Claude Code](https://claude.com/claude-code)\\n\\nCo-Authored-By: hctsai1006 <39769660@cuni.cz>\"\n    \n    try:\n        repo_dir = Path('..').resolve()\n        print(f\"ğŸ”§ Working directory: {repo_dir}\")\n        \n        # æª¢æŸ¥ Git å€‰åº«\n        result = subprocess.run(['git', 'status'], cwd=repo_dir, capture_output=True, text=True)\n        if result.returncode != 0:\n            print(\"âŒ ä¸åœ¨ Git å€‰åº«ä¸­æˆ– Git æœªåˆå§‹åŒ–\")\n            print(\"ğŸ’¡ è«‹å…ˆåŸ·è¡Œ: git init\")\n            return False\n        \n        # æª¢æŸ¥æ˜¯å¦æœ‰é ç«¯å€‰åº«\n        result = subprocess.run(['git', 'remote', 'get-url', 'origin'], cwd=repo_dir, capture_output=True, text=True)\n        if result.returncode != 0:\n            print(\"âš ï¸ æœªæ‰¾åˆ° origin é ç«¯å€‰åº«\")\n            print(\"ğŸ’¡ è«‹å…ˆè¨­å®š: git remote add origin <your-repo-url>\")\n            return False\n        \n        repo_url = result.stdout.strip()\n        print(f\"ğŸ“¡ ç›®æ¨™å€‰åº«: {repo_url}\")\n        \n        # è¨­å®šèªè­‰ URL\n        if 'github.com' in repo_url:\n            if repo_url.startswith('https://'):\n                auth_url = repo_url.replace('https://github.com/', f'https://{token}@github.com/')\n            else:\n                auth_url = f'https://{token}@github.com/{repo_url.split(\"/\")[-2]}/{repo_url.split(\"/\")[-1]}'\n                if not auth_url.endswith('.git'):\n                    auth_url += '.git'\n        else:\n            print(\"âŒ ä¸æ”¯æ´é GitHub å€‰åº«\")\n            return False\n        \n        print(\"ğŸ“‹ æ­¥é©Ÿ 1/4: æ·»åŠ æª”æ¡ˆåˆ° Git\")\n        \n        # æª¢æŸ¥ data ç›®éŒ„\n        data_dir = repo_dir / 'data'\n        if not data_dir.exists():\n            print(\"âŒ data ç›®éŒ„ä¸å­˜åœ¨\")\n            return False\n        \n        # æ·»åŠ  .gitattributes æª”æ¡ˆ\n        gitattributes = repo_dir / '.gitattributes'\n        with open(gitattributes, 'w') as f:\n            f.write('*.csv filter=lfs diff=lfs merge=lfs -text\\n')\n            f.write('*.json filter=lfs diff=lfs merge=lfs -text\\n')\n        \n        # æ·»åŠ æ‰€æœ‰æª”æ¡ˆ\n        files_to_add = [\n            '.gitattributes',\n            'data/',\n            'notebooks/',\n            'README.md',\n            'requirements.txt'\n        ]\n        \n        for file_pattern in files_to_add:\n            file_path = repo_dir / file_pattern\n            if file_path.exists():\n                result = subprocess.run(['git', 'add', file_pattern], cwd=repo_dir, capture_output=True, text=True)\n                if result.returncode == 0:\n                    print(f\"   âœ… å·²æ·»åŠ : {file_pattern}\")\n                else:\n                    print(f\"   âš ï¸ æ·»åŠ å¤±æ•—: {file_pattern}\")\n        \n        print(\"ğŸ“‹ æ­¥é©Ÿ 2/4: æäº¤è®Šæ›´\")\n        \n        # æª¢æŸ¥æ˜¯å¦æœ‰è®Šæ›´éœ€è¦æäº¤\n        result = subprocess.run(['git', 'status', '--porcelain'], cwd=repo_dir, capture_output=True, text=True)\n        if not result.stdout.strip():\n            print(\"   â„¹ï¸ æ²’æœ‰è®Šæ›´éœ€è¦æäº¤\")\n            return True\n        \n        # æäº¤è®Šæ›´\n        result = subprocess.run(['git', 'commit', '-m', commit_message], cwd=repo_dir, capture_output=True, text=True)\n        if result.returncode == 0:\n            print(\"   âœ… æäº¤æˆåŠŸ\")\n        else:\n            print(f\"   âš ï¸ æäº¤å¤±æ•—: {result.stderr}\")\n            # å¯èƒ½æ˜¯æ²’æœ‰è®Šæ›´ï¼Œç¹¼çºŒæ¨é€\n        \n        print(\"ğŸ“‹ æ­¥é©Ÿ 3/4: æ¨é€åˆ° GitHub\")\n        \n        # æ¨é€åˆ°é ç«¯\n        result = subprocess.run(['git', 'push', auth_url.replace(token, '*****'), 'HEAD:main'], \n                              cwd=repo_dir, capture_output=True, text=True)\n        \n        if result.returncode == 0:\n            print(\"   âœ… æ¨é€æˆåŠŸï¼\")\n            \n            print(\"ğŸ“‹ æ­¥é©Ÿ 4/4: é©—è­‰\")\n            # æª¢æŸ¥æ¨é€ç‹€æ…‹\n            result = subprocess.run(['git', 'status'], cwd=repo_dir, capture_output=True, text=True)\n            if 'up to date' in result.stdout or 'up-to-date' in result.stdout:\n                print(\"   âœ… å€‰åº«ç‹€æ…‹åŒæ­¥\")\n            \n            return True\n        else:\n            print(f\"   âŒ æ¨é€å¤±æ•—: {result.stderr}\")\n            # å˜—è©¦è§£æ±ºå¸¸è¦‹å•é¡Œ\n            if 'fetch first' in result.stderr or 'pull' in result.stderr:\n                print(\"   ğŸ”§ å˜—è©¦è§£æ±ºè¡çª...\")\n                subprocess.run(['git', 'pull', '--rebase'], cwd=repo_dir)\n                result = subprocess.run(['git', 'push', auth_url.replace(token, '*****'), 'HEAD:main'], \n                                      cwd=repo_dir, capture_output=True, text=True)\n                if result.returncode == 0:\n                    print(\"   âœ… è§£æ±ºè¡çªå¾Œæ¨é€æˆåŠŸï¼\")\n                    return True\n            return False\n    \n    except Exception as e:\n        print(f\"âŒ æ¨é€éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}\")\n        return False\n\ndef quick_push_to_github():\n    \"\"\"ä¸€éµæ¨é€åˆ° GitHub - åŒ…å«å®Œæ•´æµç¨‹\"\"\"\n    print(\"ğŸš€ ä¸€éµæ¨é€åˆ° GitHub\")\n    print(\"=\" * 50)\n\n    # æ­¥é©Ÿ 1: ç²å– token\n    print(\"\\nğŸ“‹ æ­¥é©Ÿ 1/4: ç²å– GitHub Token\")\n    token = get_github_token()\n    if not token:\n        print(\"âŒ ç„¡æ³•ç²å–æœ‰æ•ˆçš„ GitHub Token\")\n        print(\"ğŸ’¡ æç¤º: è«‹ç¢ºä¿ Token æ­£ç¢ºä¸¦æœ‰ 'repo' æ¬Šé™\")\n        return False\n\n    # æ­¥é©Ÿ 2: è¨­å®š Git LFS\n    print(\"\\nğŸ“‹ æ­¥é©Ÿ 2/4: è¨­å®š Git LFS\")\n    if not setup_git_lfs():\n        print(\"âŒ Git LFS è¨­å®šå¤±æ•—\")\n        return False\n\n    # æ­¥é©Ÿ 3: æ¨é€è³‡æ–™\n    print(\"\\nğŸ“‹ æ­¥é©Ÿ 3/4: æ¨é€è³‡æ–™åˆ° GitHub\")\n    success = push_to_github(token=token)\n\n    # æ­¥é©Ÿ 4: å®Œæˆ\n    print(\"\\nğŸ“‹ æ­¥é©Ÿ 4/4: å®Œæˆ\")\n    if success:\n        print(\"\\nğŸ‰ è³‡æ–™æ¨é€æˆåŠŸï¼\")\n        print(\"\\nğŸ“ ä¸‹ä¸€æ­¥:\")\n        print(\"   1. å‰å¾€ä½ çš„ GitHub å€‰åº«ç¢ºèªæª”æ¡ˆ\")\n        print(\"   2. æª¢æŸ¥ data/ ç›®éŒ„ä¸‹çš„ CSV æª”æ¡ˆ\")\n        print(\"   3. å¯ä»¥é–‹å§‹åŸ·è¡Œ 02_bls_baseline.ipynb\")\n        print(f\"   4. å€‰åº«é€£çµ: https://github.com/exoplanet-spaceapps/exoplanet-starter\")\n        return True\n    else:\n        print(\"âŒ æ¨é€å¤±æ•—ï¼Œè«‹æª¢æŸ¥éŒ¯èª¤è¨Šæ¯\")\n        print(\"ğŸ’¡ å¸¸è¦‹å•é¡Œ:\")\n        print(\"   - Token æ¬Šé™ä¸è¶³ (éœ€è¦ 'repo' æ¬Šé™)\")\n        print(\"   - ç¶²è·¯é€£æ¥å•é¡Œ\")\n        print(\"   - å€‰åº«æ¬Šé™å•é¡Œ\")\n        return False\n\n# é¡¯ç¤ºä½¿ç”¨èªªæ˜\nprint(\"ğŸ’¡ å®Œæ•´çš„ GitHub æ¨é€åŠŸèƒ½å·²è¼‰å…¥ï¼\")\nprint(f\"ğŸŒ ç•¶å‰ç’°å¢ƒ: {'Google Colab' if IN_COLAB else 'æœ¬åœ°ç’°å¢ƒ'}\")\nprint(\"\")\nprint(\"ğŸ¯ å¿«é€Ÿä½¿ç”¨:\")\nprint(\"   quick_push_to_github()    # ä¸€éµå®Œæˆæ‰€æœ‰æ­¥é©Ÿ\")\nprint(\"\")\nprint(\"ğŸ”§ åˆ†æ­¥ä½¿ç”¨:\")\nprint(\"   1. token = get_github_token()    # å®‰å…¨è¼¸å…¥ Token\")\nprint(\"   2. setup_git_lfs()               # è¨­å®š Git LFS\")\nprint(\"   3. push_to_github(token=token)   # æ¨é€è³‡æ–™\")\nprint(\"\")\nprint(\"âš ï¸ æ³¨æ„: åœ¨ Colab ä¸­åŸ·è¡Œæ™‚ï¼ŒToken è¼¸å…¥æœƒè¢«éš±è—ä¿è­·\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹ï¼šä¸€éµæ¨é€åˆ° GitHub\n\"\"\"\nåŸ·è¡Œæ­¤ cell ä¾†ä¸€éµæ¨é€è³‡æ–™åˆ° GitHub\nåŒ…å«å®‰å…¨çš„ Token è¼¸å…¥ç•Œé¢\n\"\"\"\n\n# é¸é … 1: ä¸€éµæ¨é€ (æ¨è–¦)\nprint(\"ğŸ¯ æ–¹å¼ 1: ä¸€éµæ¨é€ (æ¨è–¦)\")\nprint(\"   åŸ·è¡Œ: quick_push_to_github()\")\nprint(\"   - åŒ…å«å®‰å…¨çš„ Token è¼¸å…¥\")  \nprint(\"   - è‡ªå‹•è¨­å®š Git LFS\")\nprint(\"   - å®Œæ•´çš„éŒ¯èª¤è™•ç†\")\nprint(\"\")\n\n# é¸é … 2: åˆ†æ­¥åŸ·è¡Œ\nprint(\"ğŸ”§ æ–¹å¼ 2: åˆ†æ­¥åŸ·è¡Œ\")\nprint(\"   1. token = get_github_token()    # å®‰å…¨è¼¸å…¥\")\nprint(\"   2. setup_git_lfs()               # è¨­å®š LFS\")\nprint(\"   3. push_to_github(token=token)   # æ¨é€\")\nprint(\"\")\n\n# é¸é … 3: æ‰‹å‹•æŒ‡å®š Token (ä¸æ¨è–¦åœ¨ Colab ä¸­)\nprint(\"âš ï¸ æ–¹å¼ 3: æ‰‹å‹•æŒ‡å®š (ä¸æ¨è–¦)\")\nprint(\"   push_to_github(token='ghp_ä½ çš„token')\")\nprint(\"   - Token æœƒä»¥æ˜æ–‡é¡¯ç¤ºåœ¨ç¨‹å¼ç¢¼ä¸­\")\nprint(\"   - æœ‰å®‰å…¨é¢¨éšª\")\nprint(\"\")\n\nprint(\"=\"*50)\nprint(\"ğŸš€ è¦é–‹å§‹æ¨é€ï¼Œè«‹åŸ·è¡Œä»¥ä¸‹ä»»ä¸€å‘½ä»¤ï¼š\")\nprint(\"\")\nprint(\"quick_push_to_github()  # ä¸€éµå®Œæˆæ‰€æœ‰æ­¥é©Ÿ\")\nprint(\"\")\nprint(\"# æˆ–è€…åˆ†æ­¥åŸ·è¡Œ:\")\nprint(\"# token = get_github_token()\")\nprint(\"# setup_git_lfs()\")  \nprint(\"# push_to_github(token=token)\")\nprint(\"=\"*50)\n\n# æä¾›å¿«é€ŸåŸ·è¡ŒæŒ‰éˆ•çš„èªªæ˜\nprint(\"\")\nprint(\"ğŸ’¡ åœ¨ Colab ä¸­:\")\nprint(\"   1. å–æ¶ˆè¨»è§£ä¸‹æ–¹ä»»ä¸€è¡Œ\")\nprint(\"   2. åŸ·è¡Œæ­¤ cell\")\nprint(\"   3. è·Ÿéš¨æç¤ºè¼¸å…¥ GitHub Token\")\nprint(\"\")\n\n# å¿«é€ŸåŸ·è¡Œé¸é …ï¼ˆè¨»è§£æ‰ï¼Œç”¨æˆ¶å¯ä»¥å–æ¶ˆè¨»è§£ï¼‰\n# quick_push_to_github()  # å–æ¶ˆè¨»è§£æ­¤è¡Œä¾†ä¸€éµæ¨é€\n\n# æˆ–è€…åˆ†æ­¥åŸ·è¡Œï¼ˆè¨»è§£æ‰ï¼‰\n# token = get_github_token()\n# if token:\n#     setup_git_lfs()\n#     push_to_github(token=token)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### é¸é … Cï¼šâœ… é©—è­‰è³‡æ–™å®Œæ•´æ€§\n\n**é©åˆ**: æª¢æŸ¥è³‡æ–™å“è³ªã€ç¢ºèªä¸‹è¼‰æˆåŠŸ\n**ä½¿ç”¨**: åŸ·è¡Œ `verify_data_integrity()` é€²è¡Œå¿«é€Ÿé©—è­‰",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# é¸é … Cï¼šé©—è­‰è³‡æ–™å®Œæ•´æ€§\n\"\"\"\né©—è­‰ä¸‹è¼‰çš„è³‡æ–™æ˜¯å¦æ­£ç¢ºå„²å­˜\n\"\"\"\nfrom pathlib import Path\nimport pandas as pd\nimport json\n\ndef verify_data_integrity():\n    \"\"\"é©—è­‰æ‰€æœ‰è³‡æ–™æª”æ¡ˆçš„å®Œæ•´æ€§\"\"\"\n    print(\"ğŸ” é©—è­‰è³‡æ–™å®Œæ•´æ€§...\")\n    \n    data_dir = Path('../data')\n    \n    # æª¢æŸ¥å¿…è¦æª”æ¡ˆ\n    required_files = {\n        'toi.csv': 'TOI å®Œæ•´è³‡æ–™',\n        'toi_positive.csv': 'TOI æ­£æ¨£æœ¬',\n        'toi_negative.csv': 'TOI è² æ¨£æœ¬',\n        'koi_false_positives.csv': 'KOI False Positives',\n        'supervised_dataset.csv': 'åˆä½µè¨“ç·´è³‡æ–™é›†',\n        'data_provenance.json': 'è³‡æ–™ä¾†æºæ–‡ä»¶'\n    }\n    \n    missing_files = []\n    file_info = []\n    \n    for filename, description in required_files.items():\n        file_path = data_dir / filename\n        if file_path.exists():\n            size_mb = file_path.stat().st_size / 1024 / 1024\n            \n            if filename.endswith('.csv'):\n                try:\n                    df = pd.read_csv(file_path)\n                    rows = len(df)\n                    cols = len(df.columns)\n                    file_info.append({\n                        'file': filename,\n                        'desc': description,\n                        'rows': rows,\n                        'cols': cols,\n                        'size_mb': size_mb\n                    })\n                    print(f\"   âœ… {filename}: {rows:,} ç­†, {cols} æ¬„ä½, {size_mb:.2f} MB\")\n                except Exception as e:\n                    print(f\"   âŒ {filename}: è®€å–å¤±æ•— - {e}\")\n            else:\n                file_info.append({\n                    'file': filename,\n                    'desc': description,\n                    'size_mb': size_mb\n                })\n                print(f\"   âœ… {filename}: {size_mb:.2f} MB\")\n        else:\n            missing_files.append(filename)\n            print(f\"   âŒ {filename}: æª”æ¡ˆä¸å­˜åœ¨\")\n    \n    # è¼‰å…¥ä¸¦é©—è­‰åˆä½µè³‡æ–™é›†\n    if (data_dir / 'supervised_dataset.csv').exists():\n        print(\"\\nğŸ“Š åˆä½µè³‡æ–™é›†åˆ†æ:\")\n        combined_df = pd.read_csv(data_dir / 'supervised_dataset.csv')\n        \n        # æ¨™ç±¤åˆ†å¸ƒ\n        label_counts = combined_df['label'].value_counts()\n        print(f\"   æ­£æ¨£æœ¬ (label=1): {label_counts.get(1, 0):,} ç­†\")\n        print(f\"   è² æ¨£æœ¬ (label=0): {label_counts.get(0, 0):,} ç­†\")\n        print(f\"   å¹³è¡¡åº¦: {label_counts.get(1, 0) / len(combined_df) * 100:.1f}% vs {label_counts.get(0, 0) / len(combined_df) * 100:.1f}%\")\n        \n        # è³‡æ–™ä¾†æºåˆ†å¸ƒ\n        if 'source' in combined_df.columns:\n            print(\"\\n   è³‡æ–™ä¾†æº:\")\n            for source, count in combined_df['source'].value_counts().items():\n                print(f\"   - {source}: {count:,} ç­†\")\n        \n        # è³‡æ–™å®Œæ•´æ€§\n        print(\"\\n   ç‰©ç†åƒæ•¸å®Œæ•´æ€§:\")\n        for col in ['period', 'depth', 'duration']:\n            if col in combined_df.columns:\n                valid = combined_df[col].notna().sum()\n                pct = valid / len(combined_df) * 100\n                print(f\"   - {col}: {pct:.1f}% å®Œæ•´\")\n    \n    # ç¸½çµ\n    if missing_files:\n        print(f\"\\nâš ï¸ ç¼ºå°‘ {len(missing_files)} å€‹æª”æ¡ˆ\")\n        return False\n    else:\n        print(\"\\nâœ… æ‰€æœ‰è³‡æ–™æª”æ¡ˆå®Œæ•´ç„¡ç¼ºï¼\")\n        return True\n\n# åŸ·è¡Œé©—è­‰\nis_valid = verify_data_integrity()\n\nif is_valid:\n    print(\"\\nğŸ‰ è³‡æ–™æº–å‚™å°±ç·’ï¼Œå¯ä»¥é€²è¡Œä¸‹ä¸€æ­¥åˆ†æï¼\")\n    print(\"   å»ºè­°åŸ·è¡Œ 02_bls_baseline.ipynb\")\nelse:\n    print(\"\\nâš ï¸ è«‹é‡æ–°åŸ·è¡Œä¸Šæ–¹çš„è³‡æ–™ä¸‹è¼‰ç¨‹å¼ç¢¼\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}