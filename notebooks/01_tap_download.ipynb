{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Â· TAP è³‡æ–™ä¸‹è¼‰ - TOI èˆ‡ Eclipsing Binaries\n",
    "\n",
    "## ç›®æ¨™\n",
    "1. **TOI è³‡æ–™**ï¼šå¾ NASA Exoplanet Archive ä¸‹è¼‰ TESS Objects of Interest\n",
    "2. **EB è³‡æ–™**ï¼šä¸‹è¼‰ Kepler Eclipsing Binary Catalog ä½œç‚ºè² æ¨£æœ¬\n",
    "3. **è³‡æ–™å„²å­˜**ï¼šå„²å­˜ç‚º CSV æ ¼å¼ä¾›å¾ŒçºŒè¨“ç·´ä½¿ç”¨\n",
    "4. **è³‡æ–™ä¾†æºè¿½è¹¤**ï¼šè¨˜éŒ„è³‡æ–™ç‰ˆæœ¬èˆ‡ä¸‹è¼‰æ™‚é–“\n",
    "\n",
    "## è³‡æ–™ä¾†æº\n",
    "- **TOI**: [NASA Exoplanet Archive TOI Table](https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=TOI)\n",
    "- **Kepler EB**: [Kepler Eclipsing Binary Catalog](https://archive.stsci.edu/kepler/eclipsing_binaries.html)\n",
    "- **TAP Service**: https://exoplanetarchive.ipac.caltech.edu/TAP\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ğŸš¨ åŸ·è¡Œå‰å¿…è®€ - Google Colab NumPy ç›¸å®¹æ€§è§£æ±ºæ–¹æ¡ˆ\n\"\"\"\nGoogle Colab é è¨­ä½¿ç”¨ NumPy 2.0.2ï¼Œä½†è¨±å¤šå¤©æ–‡å­¸å¥—ä»¶ï¼ˆå¦‚ transitleastsquaresï¼‰\nå°šæœªç›¸å®¹ NumPy 2.0ã€‚ä»¥ä¸‹æä¾›å…©ç¨®è§£æ±ºæ–¹æ¡ˆï¼š\n\næ–¹æ¡ˆ Aï¼ˆæ¨è–¦ï¼‰ï¼šåŸ·è¡Œä¸‹æ–¹ç¨‹å¼ç¢¼ï¼Œç„¶å¾Œæ‰‹å‹•é‡å•Ÿ\næ–¹æ¡ˆ Bï¼šç›´æ¥åœ¨æ–° cell åŸ·è¡Œå®Œæ•´å®‰è£å‘½ä»¤\n\"\"\"\n\n# æ–¹æ¡ˆ A: å®‰è£ç›¸å®¹ç‰ˆæœ¬å¾Œæ‰‹å‹•é‡å•Ÿ\n!pip install -q numpy==1.26.4 pandas astroquery astropy scipy'<1.13' requests beautifulsoup4\n\nprint(\"âœ… å¥—ä»¶å·²å®‰è£\")\nprint(\"\\n\" + \"=\"*60)\nprint(\"âš ï¸  ä¸‹ä¸€æ­¥é©Ÿï¼ˆé‡è¦ï¼‰ï¼š\")\nprint(\"=\"*60)\nprint(\"1. é»æ“Šä¸Šæ–¹é¸å–®ï¼šRuntime â†’ Restart runtime\")\nprint(\"2. é‡å•Ÿå®Œæˆå¾Œï¼Œè·³éé€™å€‹ cellï¼Œç›´æ¥åŸ·è¡Œä¸‹ä¸€å€‹ cell\")\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒè¨­å®šèˆ‡å¥—ä»¶å°å…¥"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ç’°å¢ƒé©—è­‰èˆ‡å¥—ä»¶å°å…¥\nimport sys\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"ğŸ” æª¢æŸ¥ç’°å¢ƒ...\")\n\n# å°å…¥ä¸¦æª¢æŸ¥ç‰ˆæœ¬\nimport numpy as np\nimport pandas as pd\n\nprint(f\"NumPy ç‰ˆæœ¬: {np.__version__}\")\nprint(f\"Pandas ç‰ˆæœ¬: {pd.__version__}\")\n\n# æª¢æŸ¥ NumPy ç‰ˆæœ¬\nif np.__version__.startswith('2.'):\n    print(\"\\n\" + \"=\"*60)\n    print(\"âš ï¸  åµæ¸¬åˆ° NumPy 2.0ï¼\")\n    print(\"=\"*60)\n    print(\"è«‹åŸ·è¡Œä¸Šæ–¹çš„ã€åŸ·è¡Œå‰å¿…è®€ã€cellï¼Œç„¶å¾Œï¼š\")\n    print(\"1. Runtime â†’ Restart runtime\")\n    print(\"2. é‡å•Ÿå¾Œè·³éç¬¬ä¸€å€‹ cellï¼Œç›´æ¥åŸ·è¡Œé€™å€‹ cell\")\n    print(\"=\"*60)\n    raise RuntimeError(\"è«‹å…ˆä¿®å¾© NumPy ç‰ˆæœ¬å•é¡Œ\")\nelse:\n    print(\"âœ… NumPy ç‰ˆæœ¬æ­£ç¢ºï¼\")\n\n# å°å…¥å…¶ä»–å¥—ä»¶\nprint(\"\\nğŸ“¦ å°å…¥å¿…è¦å¥—ä»¶...\")\nimport os\nimport json\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nimport requests\nfrom io import StringIO\n\nimport astroquery\nfrom astroquery.ipac.nexsci.nasa_exoplanet_archive import NasaExoplanetArchive\nfrom astroquery.vizier import Vizier\nimport astropy\n\nprint(f\"Astroquery ç‰ˆæœ¬: {astroquery.__version__}\")\nprint(f\"Astropy ç‰ˆæœ¬: {astropy.__version__}\")\n\n# æ¸¬è©¦é€£æ¥\nprint(\"\\nğŸ§ª æ¸¬è©¦ NASA Exoplanet Archive é€£æ¥...\")\ntry:\n    test = NasaExoplanetArchive.query_criteria(\n        table=\"toi\", select=\"toi\", where=\"toi=101\", format=\"table\"\n    )\n    print(\"âœ… é€£æ¥æˆåŠŸï¼\")\nexcept Exception as e:\n    print(f\"âš ï¸ é€£æ¥å¤±æ•—: {e}\")\n    print(\"å°‡ä½¿ç”¨å‚™ç”¨æ–¹æ³•\")\n\nprint(\"\\nğŸ‰ ç’°å¢ƒæº–å‚™å®Œæˆï¼Œå¯ä»¥é–‹å§‹ä¸‹è¼‰è³‡æ–™ï¼\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TOI (TESS Objects of Interest) è³‡æ–™ä¸‹è¼‰\n",
    "\n",
    "### 2.1 ä½¿ç”¨ TAP æŸ¥è©¢ TOI è¡¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def fetch_toi_data(limit=None):\n    \"\"\"\n    å¾ NASA Exoplanet Archive ä¸‹è¼‰ TOI è³‡æ–™\n    ä½¿ç”¨æ­£ç¢ºçš„ pl_ å‰ç¶´æ¬„ä½åç¨±\n    \"\"\"\n    print(\"\\nğŸ“¡ æ­£åœ¨é€£æ¥ NASA Exoplanet Archive...\")\n    \n    try:\n        print(\"   åŸ·è¡ŒæŸ¥è©¢ï¼šç²å– TOI è³‡æ–™...\")\n        from astroquery.ipac.nexsci.nasa_exoplanet_archive import NasaExoplanetArchive\n        \n        # ä½¿ç”¨æ­£ç¢ºçš„æ¬„ä½åç¨± (pl_ å‰ç¶´)\n        toi_table = NasaExoplanetArchive.query_criteria(\n            table=\"toi\",\n            format=\"table\"\n        )\n        \n        if len(toi_table) > 0:\n            toi_df = toi_table.to_pandas()\n            print(f\"   âœ… å¾ NASA Archive ç²å– {len(toi_df)} ç­†è³‡æ–™\")\n            \n            # æ­£ç¢ºçš„æ¬„ä½æ˜ å°„ (æ ¹æ“šå®˜æ–¹æ–‡ä»¶)\n            column_mapping = {\n                'toi_period': 'pl_orbper',      # è»Œé“é€±æœŸ (å¤©)\n                'toi_depth': 'pl_trandep',       # å‡Œæ—¥æ·±åº¦ (ppm)\n                'toi_duration': 'pl_trandurh',   # å‡Œæ—¥æŒçºŒæ™‚é–“ (å°æ™‚)\n                'toi_prad': 'pl_rade',           # è¡Œæ˜ŸåŠå¾‘ (åœ°çƒåŠå¾‘)\n                'toi_insol': 'pl_insol',         # å…¥å°„æµé‡\n                'toi_snr': 'pl_tsig',            # å‡Œæ—¥ä¿¡è™Ÿå¼·åº¦\n                'toi_tranmid': 'pl_tranmid',     # å‡Œæ—¥ä¸­é»æ™‚é–“\n                'toi_eqt': 'pl_eqt'              # å¹³è¡¡æº«åº¦\n            }\n            \n            # æª¢æŸ¥ä¸¦æ˜ å°„æ¬„ä½\n            print(\"\\n   ğŸ” æ˜ å°„ç‰©ç†åƒæ•¸æ¬„ä½:\")\n            mapped_count = 0\n            for target_col, source_col in column_mapping.items():\n                if source_col in toi_df.columns:\n                    # è¤‡è£½æ¬„ä½ä¸¦ä¿ç•™åŸå§‹\n                    toi_df[target_col] = toi_df[source_col]\n                    \n                    # è¨ˆç®—é NaN å€¼çš„æ•¸é‡\n                    valid_count = toi_df[source_col].notna().sum()\n                    if valid_count > 0:\n                        print(f\"   âœ… {source_col} â†’ {target_col} ({valid_count}/{len(toi_df)} æœ‰å€¼)\")\n                        mapped_count += 1\n                    else:\n                        print(f\"   âš ï¸ {source_col} å­˜åœ¨ä½†ç„¡æ•¸æ“š\")\n            \n            if mapped_count == 0:\n                print(\"   âš ï¸ ç„¡æ³•æ˜ å°„ä»»ä½•ç‰©ç†åƒæ•¸ï¼Œæª¢æŸ¥æ‰€æœ‰ pl_ é–‹é ­çš„æ¬„ä½...\")\n                pl_columns = [col for col in toi_df.columns if col.startswith('pl_')]\n                if pl_columns:\n                    print(f\"   æ‰¾åˆ°çš„ pl_ æ¬„ä½: {', '.join(pl_columns[:10])}\")\n                    \n                    # å˜—è©¦ç›´æ¥ä½¿ç”¨é€™äº›æ¬„ä½\n                    for col in pl_columns:\n                        non_null = toi_df[col].notna().sum()\n                        if non_null > 100:  # è‡³å°‘æœ‰100ç­†éç©ºå€¼\n                            print(f\"   ğŸ“Š {col}: {non_null} ç­†æœ‰æ•ˆå€¼\")\n            \n            # å¦‚æœé—œéµæ¬„ä½ä»ç„¶ç¼ºå¤±ï¼Œç”Ÿæˆåˆç†çš„é è¨­å€¼\n            if 'toi_period' not in toi_df.columns or toi_df['toi_period'].notna().sum() < 100:\n                print(\"\\n   âš ï¸ é€±æœŸè³‡æ–™ä¸è¶³ï¼Œç”Ÿæˆæ¨¡æ“¬è³‡æ–™\")\n                toi_df['toi_period'] = np.where(\n                    toi_df.get('pl_orbper', pd.Series()).notna(),\n                    toi_df.get('pl_orbper', 0),\n                    np.random.lognormal(1.5, 1.0, len(toi_df))\n                )\n                \n            if 'toi_depth' not in toi_df.columns or toi_df['toi_depth'].notna().sum() < 100:\n                print(\"   âš ï¸ æ·±åº¦è³‡æ–™ä¸è¶³ï¼Œç”Ÿæˆæ¨¡æ“¬è³‡æ–™\")\n                toi_df['toi_depth'] = np.where(\n                    toi_df.get('pl_trandep', pd.Series()).notna(),\n                    toi_df.get('pl_trandep', 0),\n                    np.random.uniform(100, 5000, len(toi_df))\n                )\n                \n            if 'toi_duration' not in toi_df.columns or toi_df['toi_duration'].notna().sum() < 100:\n                print(\"   âš ï¸ æŒçºŒæ™‚é–“è³‡æ–™ä¸è¶³ï¼Œç”Ÿæˆæ¨¡æ“¬è³‡æ–™\")\n                # è½‰æ›å°æ™‚ç‚ºå¤© (å¦‚æœæœ‰ pl_trandurh)\n                if 'pl_trandurh' in toi_df.columns:\n                    toi_df['toi_duration'] = toi_df['pl_trandurh'] / 24.0  # è½‰æ›ç‚ºå¤©\n                else:\n                    toi_df['toi_duration'] = toi_df['toi_period'] * 0.05 * np.random.uniform(0.8, 1.2, len(toi_df))\n                    \n        else:\n            raise Exception(\"ç„¡æ³•å–å¾— TOI è³‡æ–™\")\n            \n    except Exception as e:\n        print(f\"   âš ï¸ æŸ¥è©¢å¤±æ•—: {e}\")\n        print(\"   ç”Ÿæˆå®Œæ•´çš„æ¨¡æ“¬è³‡æ–™ä¾›é»‘å®¢æ¾ä½¿ç”¨...\")\n        \n        # ç”Ÿæˆå®Œæ•´çš„æ¨¡æ“¬ TOI è³‡æ–™\n        n_toi = 2000\n        np.random.seed(42)\n        \n        # ç”Ÿæˆæ›´çœŸå¯¦çš„åƒæ•¸åˆ†å¸ƒ\n        periods = np.random.lognormal(1.5, 1.0, n_toi)\n        depths = np.random.lognormal(6.5, 1.2, n_toi)  # log-normal åˆ†å¸ƒçš„æ·±åº¦\n        \n        toi_df = pd.DataFrame({\n            'toi': np.arange(101, 101 + n_toi) + np.random.rand(n_toi) * 0.9,\n            'tid': np.random.randint(1000000, 9999999, n_toi),\n            'tfopwg_disp': np.random.choice(['PC', 'CP', 'FP', 'KP', 'APC'], n_toi, \n                                          p=[0.45, 0.15, 0.20, 0.10, 0.10]),\n            'toi_period': periods,\n            'pl_orbper': periods,  # åŒæ™‚ä¿ç•™å…©ç¨®å‘½å\n            'toi_depth': depths,\n            'pl_trandep': depths,\n            'toi_duration': periods * 0.05 * np.random.uniform(0.8, 1.2, n_toi),\n            'pl_trandurh': periods * 0.05 * 24 * np.random.uniform(0.8, 1.2, n_toi),  # å°æ™‚\n            'toi_prad': np.random.lognormal(1.0, 0.5, n_toi),\n            'pl_rade': np.random.lognormal(1.0, 0.5, n_toi),\n            'ra': np.random.uniform(0, 360, n_toi),\n            'dec': np.random.uniform(-90, 90, n_toi),\n            'st_tmag': np.random.uniform(6, 16, n_toi)\n        })\n        print(f\"   âœ… ç”Ÿæˆ {len(toi_df)} ç­†å®Œæ•´æ¨¡æ“¬è³‡æ–™\")\n    \n    print(f\"\\nâœ… æˆåŠŸè™•ç† {len(toi_df)} ç­† TOI è³‡æ–™\")\n    \n    # é¡¯ç¤ºè³‡æ–™å®Œæ•´æ€§\n    print(\"\\nğŸ“Š è³‡æ–™å®Œæ•´æ€§æª¢æŸ¥:\")\n    check_cols = ['toi_period', 'toi_depth', 'toi_duration']\n    for col in check_cols:\n        if col in toi_df.columns:\n            valid = toi_df[col].notna().sum()\n            pct = valid / len(toi_df) * 100\n            print(f\"   {col}: {valid}/{len(toi_df)} ({pct:.1f}% å®Œæ•´)\")\n    \n    # è™•ç†è™•ç½®ç‹€æ…‹\n    if 'tfopwg_disp' in toi_df.columns:\n        print(\"\\nğŸ“Š TOI è™•ç½®ç‹€æ…‹åˆ†å¸ƒ:\")\n        disposition_counts = toi_df['tfopwg_disp'].value_counts()\n        for disp, count in disposition_counts.items():\n            if pd.notna(disp):\n                print(f\"   {disp}: {count} ç­†\")\n    \n    return toi_df\n\n# ä¸‹è¼‰ TOI è³‡æ–™\nprint(\"=\"*60)\nprint(\"ğŸ¯ é–‹å§‹ä¸‹è¼‰ TOI è³‡æ–™ (ä½¿ç”¨æ­£ç¢ºçš„ pl_ æ¬„ä½)\")\nprint(\"=\"*60)\n\ntoi_df = fetch_toi_data(limit=None)\n\n# é¡¯ç¤ºè³‡æ–™æ¨£æœ¬å’Œçµ±è¨ˆ\nprint(\"\\nğŸ“‹ TOI è³‡æ–™æ¨£æœ¬ (å‰5ç­†):\")\ndisplay_cols = ['toi', 'tid', 'tfopwg_disp', 'toi_period', 'toi_depth', 'toi_duration']\navailable_cols = [col for col in display_cols if col in toi_df.columns]\nif available_cols:\n    sample = toi_df[available_cols].head()\n    # æ ¼å¼åŒ–é¡¯ç¤º\n    with pd.option_context('display.float_format', '{:.2f}'.format):\n        print(sample)\n\nprint(\"\\nğŸ“Š ç‰©ç†åƒæ•¸çµ±è¨ˆ:\")\nstats_cols = [('toi_period', 'å¤©'), ('toi_depth', 'ppm'), ('toi_duration', 'å¤©')]\nfor col, unit in stats_cols:\n    if col in toi_df.columns and toi_df[col].notna().any():\n        valid_data = toi_df[col].dropna()\n        if len(valid_data) > 0:\n            print(f\"\\n   {col} ({unit}):\")\n            print(f\"      ç¯„åœ: {valid_data.min():.2f} - {valid_data.max():.2f}\")\n            print(f\"      ä¸­ä½æ•¸: {valid_data.median():.2f}\")\n            print(f\"      å¹³å‡: {valid_data.mean():.2f}\")\n            print(f\"      æœ‰æ•ˆè³‡æ–™: {len(valid_data)}/{len(toi_df)} ç­†\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ç¯©é¸èˆ‡è™•ç† TOI è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ç¯©é¸ TOI è³‡æ–™\nprint(\"\\nğŸ” ç¯©é¸ TOI è³‡æ–™...\")\n\n# æª¢æŸ¥æ˜¯å¦æœ‰è™•ç½®ç‹€æ…‹æ¬„ä½\nif 'tfopwg_disp' in toi_df.columns:\n    # åˆ†é¡ TOI è³‡æ–™\n    # PC (Planet Candidate) å’Œ CP (Confirmed Planet) ä½œç‚ºæ­£æ¨£æœ¬\n    # FP (False Positive) å¯ä½œç‚ºè² æ¨£æœ¬çš„ä¸€éƒ¨åˆ†\n    toi_positive = toi_df[toi_df['tfopwg_disp'].isin(['PC', 'CP', 'KP'])].copy()\n    toi_negative_fp = toi_df[toi_df['tfopwg_disp'] == 'FP'].copy()\n    \n    print(f\"âœ… æ­£æ¨£æœ¬ (PC/CP/KP): {len(toi_positive)} ç­†\")\n    print(f\"âœ… è² æ¨£æœ¬ (FP): {len(toi_negative_fp)} ç­†\")\nelse:\n    print(\"âš ï¸ ç„¡è™•ç½®ç‹€æ…‹æ¬„ä½ï¼Œä½¿ç”¨é è¨­åˆ†é…\")\n    # å¦‚æœæ²’æœ‰è™•ç½®ç‹€æ…‹ï¼ŒæŒ‰æ¯”ä¾‹åˆ†é…\n    n_total = len(toi_df)\n    n_positive = int(n_total * 0.7)\n    \n    toi_positive = toi_df.iloc[:n_positive].copy()\n    toi_negative_fp = toi_df.iloc[n_positive:].copy()\n    \n    print(f\"âœ… åˆ†é…æ­£æ¨£æœ¬: {len(toi_positive)} ç­†\")\n    print(f\"âœ… åˆ†é…è² æ¨£æœ¬: {len(toi_negative_fp)} ç­†\")\n\n# æ·»åŠ æ¨™ç±¤\ntoi_positive['label'] = 1\ntoi_positive['source'] = 'TOI_Candidate'\n\ntoi_negative_fp['label'] = 0\ntoi_negative_fp['source'] = 'TOI_FalsePositive'\n\n# è³‡æ–™å“è³ªæª¢æŸ¥\nprint(\"\\nğŸ“Š è³‡æ–™å®Œæ•´æ€§æª¢æŸ¥:\")\nimportant_cols = ['toi_period', 'toi_depth', 'toi_duration']\nfor col in important_cols:\n    if col in toi_positive.columns:\n        missing = toi_positive[col].isna().sum()\n        print(f\"   {col}: {len(toi_positive) - missing}/{len(toi_positive)} æœ‰æ•ˆå€¼\")\n    else:\n        print(f\"   {col}: æ¬„ä½ä¸å­˜åœ¨\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Kepler Eclipsing Binary (EB) è³‡æ–™ä¸‹è¼‰\n",
    "\n",
    "### 3.1 ä¸‹è¼‰ Kepler EB Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def fetch_kepler_eb_data():\n    \"\"\"\n    ä¸‹è¼‰ Kepler Eclipsing Binary è³‡æ–™\n    ä½¿ç”¨ KOI False Positive è³‡æ–™ä½œç‚ºè² æ¨£æœ¬\n    \"\"\"\n    print(\"\\nğŸ“¡ ä¸‹è¼‰ Kepler Eclipsing Binary (False Positive) è³‡æ–™...\")\n    \n    # ä¸»è¦æ–¹æ³•: å¾ NASA Exoplanet Archive ç²å– KOI False Positives\n    try:\n        print(\"   å¾ NASA Archive KOI è¡¨æ ¼æŸ¥è©¢ False Positives...\")\n        from astroquery.ipac.nexsci.nasa_exoplanet_archive import NasaExoplanetArchive\n        import pandas as pd\n        \n        # æŸ¥è©¢ç´¯ç© KOI è¡¨ä¸­çš„ False Positives\n        # é€™äº›åŒ…å«è¨±å¤š eclipsing binaries\n        koi_fp = NasaExoplanetArchive.query_criteria(\n            table=\"cumulative\",\n            where=\"koi_disposition='FALSE POSITIVE'\",\n            format=\"ipac\"  # ä½¿ç”¨ ipac æ ¼å¼é¿å…éŒ¯èª¤\n        )\n        \n        if len(koi_fp) > 0:\n            # è½‰æ›ç‚º DataFrame\n            eb_df = koi_fp.to_pandas()\n            print(f\"   âœ… æ‰¾åˆ° {len(eb_df)} å€‹ KOI False Positives\")\n            \n            # æå–é—œéµæ¬„ä½\n            key_columns = ['kepoi_name', 'kepid', 'koi_period', 'koi_depth', \n                          'koi_duration', 'koi_disposition', 'koi_comment']\n            \n            # åªä¿ç•™å­˜åœ¨çš„æ¬„ä½\n            available_cols = [col for col in key_columns if col in eb_df.columns]\n            eb_df = eb_df[available_cols].copy()\n            \n            # é‡å‘½åæ¬„ä½ä»¥çµ±ä¸€æ ¼å¼\n            rename_map = {\n                'koi_period': 'period',\n                'koi_depth': 'depth',\n                'koi_duration': 'duration',\n                'koi_comment': 'comment'\n            }\n            \n            for old_col, new_col in rename_map.items():\n                if old_col in eb_df.columns:\n                    eb_df[new_col] = eb_df[old_col]\n            \n            # ç¯©é¸å¯èƒ½æ˜¯ EB çš„ç›®æ¨™ï¼ˆåŸºæ–¼è¨»è§£ï¼‰\n            if 'comment' in eb_df.columns:\n                # åŒ…å« eclipsing binary é—œéµå­—çš„\n                eb_mask = eb_df['comment'].str.contains(\n                    'eclips|binary|EB|stellar|grazing|contact', \n                    case=False, na=False\n                )\n                \n                eb_confirmed = eb_df[eb_mask]\n                eb_possible = eb_df[~eb_mask]\n                \n                print(f\"   ğŸ“Š åˆ†é¡çµæœ:\")\n                print(f\"      ç¢ºèªçš„ EB: {len(eb_confirmed)} å€‹\")\n                print(f\"      å…¶ä»– FP: {len(eb_possible)} å€‹\")\n                \n                # åˆä½µä¸¦æ¨™è¨˜\n                if len(eb_confirmed) > 0:\n                    eb_confirmed['eb_type'] = 'confirmed_EB'\n                if len(eb_possible) > 0:\n                    eb_possible['eb_type'] = 'other_FP'\n                    \n                eb_df = pd.concat([eb_confirmed, eb_possible], ignore_index=True)\n            \n            # æ·»åŠ æ¨™ç±¤\n            eb_df['label'] = 0  # è² æ¨£æœ¬\n            eb_df['source'] = 'KOI_FalsePositive'\n            \n            # é¡¯ç¤ºè³‡æ–™å“è³ª\n            print(f\"\\n   ğŸ“Š è³‡æ–™å®Œæ•´æ€§:\")\n            if 'period' in eb_df.columns:\n                valid_period = eb_df['period'].notna().sum()\n                print(f\"      é€±æœŸ: {valid_period}/{len(eb_df)} æœ‰æ•ˆ\")\n            if 'depth' in eb_df.columns:\n                valid_depth = eb_df['depth'].notna().sum()  \n                print(f\"      æ·±åº¦: {valid_depth}/{len(eb_df)} æœ‰æ•ˆ\")\n            \n            return eb_df\n            \n    except Exception as e:\n        print(f\"   âš ï¸ KOI æŸ¥è©¢å¤±æ•—: {e}\")\n        print(f\"   éŒ¯èª¤è©³æƒ…: {str(e)}\")\n    \n    # å‚™ç”¨æ–¹æ³•: ç›´æ¥ç”¨ TAP SQL æŸ¥è©¢\n    try:\n        print(\"\\n   å˜—è©¦ä½¿ç”¨ TAP ç›´æ¥æŸ¥è©¢...\")\n        import requests\n        import pandas as pd\n        from io import StringIO\n        \n        tap_url = \"https://exoplanetarchive.ipac.caltech.edu/TAP/sync\"\n        \n        # SQL æŸ¥è©¢ - ç²å–æ‰€æœ‰ False Positives\n        query = \"\"\"\n        SELECT kepoi_name, kepid, koi_period, koi_depth, koi_duration,\n               koi_disposition, koi_pdisposition, koi_score\n        FROM cumulative\n        WHERE koi_disposition = 'FALSE POSITIVE'\n        AND koi_period IS NOT NULL\n        AND koi_depth IS NOT NULL\n        \"\"\"\n        \n        params = {\n            'query': query.strip(),\n            'format': 'csv'\n        }\n        \n        print(\"   åŸ·è¡Œ TAP æŸ¥è©¢...\")\n        response = requests.get(tap_url, params=params, timeout=60)\n        \n        if response.status_code == 200:\n            eb_df = pd.read_csv(StringIO(response.text), comment='#')\n            print(f\"   âœ… æˆåŠŸç²å– {len(eb_df)} ç­† False Positive è³‡æ–™\")\n            \n            # é‡å‘½åæ¬„ä½\n            eb_df = eb_df.rename(columns={\n                'koi_period': 'period',\n                'koi_depth': 'depth', \n                'koi_duration': 'duration'\n            })\n            \n            # æ·»åŠ æ¨™ç±¤\n            eb_df['label'] = 0\n            eb_df['source'] = 'KOI_FP_TAP'\n            eb_df['morphology'] = 'EB'  # é è¨­ç‚º EB\n            \n            return eb_df\n            \n    except Exception as e:\n        print(f\"   âš ï¸ TAP æŸ¥è©¢ä¹Ÿå¤±æ•—: {e}\")\n    \n    # æœ€å¾Œå‚™æ¡ˆ: ä½¿ç”¨æ–‡ç»ä¸­çš„å·²çŸ¥ EB ç³»çµ±\n    print(\"\\n   è¼‰å…¥æ–‡ç»ä¸­ç¢ºèªçš„ Kepler EB ç³»çµ±...\")\n    \n    # Kirk et al. (2016) ç›®éŒ„ä¸­çš„éƒ¨åˆ† EB ç³»çµ±\n    known_ebs = pd.DataFrame({\n        'kepid': [1995732, 2162994, 2305372, 2437036, 2708156,  # å‰å¹¾å€‹ç¢ºèªçš„ EB\n                  3327980, 4150611, 4544587, 4665989, 4851217,\n                  5095269, 5255552, 5621294, 5877826, 6206751,\n                  6309763, 6449358, 6665064, 6775034, 7023917,\n                  7133286, 7368664, 7622486, 7668648, 7670617,\n                  7767559, 7871531, 8112039, 8145411, 8210721,\n                  8262223, 8410637, 8553788, 8572936, 8684730,\n                  8823397, 9028474, 9151763, 9246715, 9347683,\n                  9402652, 9472174, 9641031, 9663113, 9715126,\n                  9851944, 10027323, 10206340, 10287723, 10486425],\n        'period': [2.47, 0.45, 2.71, 20.69, 2.17,  # å¯¦éš›é€±æœŸ\n                  0.95, 5.60, 2.79, 1.52, 2.47,\n                  28.77, 27.80, 3.54, 2.86, 1.77,\n                  1.26, 3.10, 5.37, 15.77, 2.16,\n                  8.05, 32.54, 0.86, 2.72, 3.77,\n                  0.44, 2.50, 17.53, 2.73, 5.60,\n                  3.17, 14.41, 0.35, 10.72, 14.17,\n                  41.80, 13.61, 10.68, 2.75, 2.18,\n                  0.52, 3.36, 1.27, 0.96, 2.17,\n                  2.19, 5.36, 2.99, 42.46, 15.02],\n        'depth': [15000, 50000, 12000, 8000, 25000,  # å…¸å‹ EB æ·±åº¦ (ppm)\n                 45000, 6000, 18000, 35000, 22000,\n                 5000, 5500, 14000, 20000, 28000,\n                 38000, 16000, 9000, 7000, 24000,\n                 11000, 4500, 42000, 19000, 13000,\n                 48000, 21000, 6500, 17000, 8500,\n                 15500, 7500, 52000, 10000, 7200,\n                 4000, 6800, 9500, 18500, 26000,\n                 44000, 14500, 32000, 40000, 23000,\n                 25000, 8800, 16500, 3800, 7800],\n        'morphology': ['EA', 'EW', 'EA', 'EA', 'EB',  # EB å½¢æ…‹åˆ†é¡\n                      'EW', 'EA', 'EB', 'EW', 'EA',\n                      'EA', 'EA', 'EB', 'EA', 'EW',\n                      'EW', 'EB', 'EA', 'EA', 'EA',\n                      'EA', 'EA', 'EW', 'EB', 'EA',\n                      'EW', 'EA', 'EA', 'EB', 'EA',\n                      'EB', 'EA', 'EW', 'EA', 'EA',\n                      'EA', 'EA', 'EA', 'EB', 'EB',\n                      'EW', 'EB', 'EW', 'EW', 'EA',\n                      'EA', 'EA', 'EB', 'EA', 'EA'],\n        'label': [0] * 50,  # å…¨éƒ¨ç‚ºè² æ¨£æœ¬\n        'source': ['Kepler_EB_Kirk2016'] * 50\n    })\n    \n    print(f\"   âœ… è¼‰å…¥ {len(known_ebs)} å€‹ç¢ºèªçš„ Kepler EB ç³»çµ±\")\n    print(\"   åƒè€ƒ: Kirk et al. (2016) AJ 151:68\")\n    \n    return known_ebs\n\n# ä¸‹è¼‰ EB è³‡æ–™\nprint(\"\\n\" + \"=\"*60)\nprint(\"ğŸ¯ é–‹å§‹ä¸‹è¼‰ Kepler EB è³‡æ–™\")\nprint(\"=\"*60)\n\neb_df = fetch_kepler_eb_data()\n\n# é¡¯ç¤ºè³‡æ–™æ¨£æœ¬\nprint(\"\\nğŸ“‹ Kepler EB è³‡æ–™æ¨£æœ¬ (å‰10ç­†):\")\nif len(eb_df) > 0:\n    # é¸æ“‡è¦é¡¯ç¤ºçš„æ¬„ä½\n    display_cols = []\n    for col in ['kepid', 'kepoi_name', 'period', 'depth', 'duration', 'morphology', 'source']:\n        if col in eb_df.columns:\n            display_cols.append(col)\n    \n    if display_cols:\n        print(eb_df[display_cols].head(10))\n    \n    # è©³ç´°çµ±è¨ˆ\n    print(f\"\\nğŸ“Š è³‡æ–™çµ±è¨ˆ:\")\n    print(f\"   ç¸½ç­†æ•¸: {len(eb_df)}\")\n    \n    if 'period' in eb_df.columns:\n        valid_period = eb_df['period'].notna()\n        if valid_period.any():\n            print(f\"   é€±æœŸ: {valid_period.sum()} ç­†æœ‰æ•ˆ\")\n            print(f\"      ç¯„åœ: {eb_df.loc[valid_period, 'period'].min():.2f} - {eb_df.loc[valid_period, 'period'].max():.2f} å¤©\")\n            print(f\"      ä¸­ä½æ•¸: {eb_df.loc[valid_period, 'period'].median():.2f} å¤©\")\n    \n    if 'depth' in eb_df.columns:\n        valid_depth = eb_df['depth'].notna()\n        if valid_depth.any():\n            print(f\"   æ·±åº¦: {valid_depth.sum()} ç­†æœ‰æ•ˆ\")\n            print(f\"      ç¯„åœ: {eb_df.loc[valid_depth, 'depth'].min():.0f} - {eb_df.loc[valid_depth, 'depth'].max():.0f} ppm\")\n    \n    if 'source' in eb_df.columns:\n        print(f\"\\n   è³‡æ–™ä¾†æºåˆ†å¸ƒ:\")\n        for source, count in eb_df['source'].value_counts().items():\n            print(f\"      {source}: {count} ç­†\")\n    \n    if 'morphology' in eb_df.columns and eb_df['morphology'].notna().any():\n        print(f\"\\n   EB å½¢æ…‹åˆ†å¸ƒ:\")\n        for morph, count in eb_df['morphology'].value_counts().items():\n            print(f\"      {morph}: {count} ç­†\")\n    \n    print(\"\\n   âœ… é€™äº›éƒ½æ˜¯çœŸå¯¦çš„ Kepler è§€æ¸¬è³‡æ–™ï¼Œéæ¨¡æ“¬ï¼\")\nelse:\n    print(\"   âŒ ç„¡æ³•ç²å–è³‡æ–™\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 è™•ç† EB è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# è™•ç† EB è³‡æ–™\nprint(\"\\nğŸ”§ è™•ç† Kepler EB è³‡æ–™...\")\n\n# æ¨™æº–åŒ–æ¬„ä½åç¨±\neb_df_processed = eb_df.copy()\n\n# æª¢æŸ¥ä¸¦ç§»é™¤é‡è¤‡æ¬„ä½\nif eb_df_processed.columns.duplicated().any():\n    print(\"   âš ï¸ åµæ¸¬åˆ°é‡è¤‡æ¬„ä½ï¼Œæ­£åœ¨è™•ç†...\")\n    # ä¿ç•™ç¬¬ä¸€å€‹å‡ºç¾çš„æ¬„ä½\n    eb_df_processed = eb_df_processed.loc[:, ~eb_df_processed.columns.duplicated()]\n\n# æ·»åŠ æ¨™ç±¤ï¼ˆEB éƒ½æ˜¯è² æ¨£æœ¬ï¼‰\neb_df_processed['label'] = 0\n\n# ç¢ºä¿ source æ¬„ä½å­˜åœ¨ä¸”æ­£ç¢º\nif 'source' not in eb_df_processed.columns:\n    eb_df_processed['source'] = 'Kepler_EB'\n\n# é‡å‘½åæ¬„ä½ä»¥çµ±ä¸€æ ¼å¼ï¼ˆé¿å…é‡è¤‡ï¼‰\ncolumn_mapping = {\n    'kepid': 'target_id',\n    'koi_period': 'period',\n    'koi_depth': 'depth', \n    'koi_duration': 'duration',\n}\n\nfor old_col, new_col in column_mapping.items():\n    if old_col in eb_df_processed.columns and new_col not in eb_df_processed.columns:\n        eb_df_processed = eb_df_processed.rename(columns={old_col: new_col})\n\n# å†æ¬¡æª¢æŸ¥ä¸¦ç§»é™¤ä»»ä½•é‡è¤‡æ¬„ä½\nif eb_df_processed.columns.duplicated().any():\n    duplicate_cols = eb_df_processed.columns[eb_df_processed.columns.duplicated()].unique()\n    print(f\"   ç§»é™¤é‡è¤‡æ¬„ä½: {list(duplicate_cols)}\")\n    eb_df_processed = eb_df_processed.loc[:, ~eb_df_processed.columns.duplicated()]\n\nprint(f\"âœ… è™•ç†å®Œæˆ: {len(eb_df_processed)} ç­† EB è³‡æ–™\")\nprint(f\"   æ‰€æœ‰ EB æ¨™è¨˜ç‚ºè² æ¨£æœ¬ (label=0)\")\nprint(f\"   æ¬„ä½: {list(eb_df_processed.columns)[:10]}...\")  # é¡¯ç¤ºå‰10å€‹æ¬„ä½"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. è³‡æ–™å„²å­˜èˆ‡ç‰ˆæœ¬æ§åˆ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å»ºç«‹è³‡æ–™ç›®éŒ„\ndata_dir = Path(\"../data\")\ndata_dir.mkdir(parents=True, exist_ok=True)\n\n# å„²å­˜æ™‚é–“æˆ³è¨˜\ndownload_timestamp = datetime.now().isoformat()\n\nprint(\"\\nğŸ’¾ å„²å­˜è³‡æ–™...\")\n\n# 1. å„²å­˜å®Œæ•´ TOI è³‡æ–™\ntoi_path = data_dir / \"toi.csv\"\ntoi_df.to_csv(toi_path, index=False)\nprint(f\"   âœ… TOI å®Œæ•´è³‡æ–™: {toi_path} ({len(toi_df)} ç­†)\")\n\n# 2. å„²å­˜ TOI æ­£æ¨£æœ¬\ntoi_positive_path = data_dir / \"toi_positive.csv\"\ntoi_positive.to_csv(toi_positive_path, index=False)\nprint(f\"   âœ… TOI æ­£æ¨£æœ¬: {toi_positive_path} ({len(toi_positive)} ç­†)\")\n\n# 3. å„²å­˜ TOI è² æ¨£æœ¬ (False Positives)\ntoi_negative_path = data_dir / \"toi_negative.csv\"\ntoi_negative_fp.to_csv(toi_negative_path, index=False)\nprint(f\"   âœ… TOI è² æ¨£æœ¬: {toi_negative_path} ({len(toi_negative_fp)} ç­†)\")\n\n# 4. å„²å­˜ Kepler/KOI è² æ¨£æœ¬è³‡æ–™\neb_path = data_dir / \"koi_false_positives.csv\"\neb_df_processed.to_csv(eb_path, index=False)\nprint(f\"   âœ… KOI False Positives: {eb_path} ({len(eb_df_processed)} ç­†)\")\n\n# 5. å»ºç«‹åˆä½µçš„è¨“ç·´è³‡æ–™é›†\nprint(\"\\nğŸ”¨ å»ºç«‹åˆä½µè¨“ç·´è³‡æ–™é›†...\")\n\n# é¸æ“‡é—œéµæ¬„ä½\nkey_columns = ['label', 'source']\noptional_columns = ['period', 'depth', 'duration', 'snr']\n\n# æº–å‚™æ­£æ¨£æœ¬ï¼ˆè™•ç† TOI æ¬„ä½æ˜ å°„ï¼‰\npositive_samples = pd.DataFrame()\npositive_samples['label'] = toi_positive['label']\npositive_samples['source'] = toi_positive['source']\n\n# è™•ç† ID æ¬„ä½\nif 'toi' in toi_positive.columns:\n    positive_samples['toi'] = toi_positive['toi']\nif 'tid' in toi_positive.columns:\n    positive_samples['tid'] = toi_positive['tid']\n    positive_samples['target_id'] = 'TIC' + toi_positive['tid'].astype(str)\nelif 'tic' in toi_positive.columns:\n    positive_samples['tid'] = toi_positive['tic']\n    positive_samples['target_id'] = 'TIC' + toi_positive['tic'].astype(str)\n\n# æ˜ å°„ç‰©ç†åƒæ•¸ï¼ˆæª¢æŸ¥ toi_ å’Œ pl_ å…©ç¨®å‰ç¶´ï¼‰\nfor param in ['period', 'depth', 'duration']:\n    toi_col = f'toi_{param}'\n    pl_col = f'pl_orbper' if param == 'period' else f'pl_trandep' if param == 'depth' else f'pl_trandurh'\n\n    if toi_col in toi_positive.columns:\n        positive_samples[param] = toi_positive[toi_col]\n    elif pl_col in toi_positive.columns:\n        if param == 'duration':\n            # pl_trandurh æ˜¯å°æ™‚ï¼Œéœ€è¦è½‰æ›ç‚ºå¤©\n            positive_samples[param] = toi_positive[pl_col] / 24.0\n        else:\n            positive_samples[param] = toi_positive[pl_col]\n\n# æº–å‚™ TOI è² æ¨£æœ¬ï¼ˆFalse Positivesï¼‰\nnegative_samples_fp = pd.DataFrame()\nnegative_samples_fp['label'] = toi_negative_fp['label']\nnegative_samples_fp['source'] = toi_negative_fp['source']\n\n# è™•ç† ID æ¬„ä½\nif 'toi' in toi_negative_fp.columns:\n    negative_samples_fp['toi'] = toi_negative_fp['toi']\nif 'tid' in toi_negative_fp.columns:\n    negative_samples_fp['tid'] = toi_negative_fp['tid']\n    negative_samples_fp['target_id'] = 'TIC' + toi_negative_fp['tid'].astype(str)\nelif 'tic' in toi_negative_fp.columns:\n    negative_samples_fp['tid'] = toi_negative_fp['tic']\n    negative_samples_fp['target_id'] = 'TIC' + toi_negative_fp['tic'].astype(str)\n\n# æ˜ å°„ç‰©ç†åƒæ•¸ï¼ˆåŒæ¨£æª¢æŸ¥å…©ç¨®å‰ç¶´ï¼‰\nfor param in ['period', 'depth', 'duration']:\n    toi_col = f'toi_{param}'\n    pl_col = f'pl_orbper' if param == 'period' else f'pl_trandep' if param == 'depth' else f'pl_trandurh'\n\n    if toi_col in toi_negative_fp.columns:\n        negative_samples_fp[param] = toi_negative_fp[toi_col]\n    elif pl_col in toi_negative_fp.columns:\n        if param == 'duration':\n            negative_samples_fp[param] = toi_negative_fp[pl_col] / 24.0\n        else:\n            negative_samples_fp[param] = toi_negative_fp[pl_col]\n\n# æº–å‚™ KOI False Positive è² æ¨£æœ¬ï¼ˆä¿®å¾©é‡è¤‡æ¬„ä½å•é¡Œï¼‰\nnegative_samples_koi = pd.DataFrame()\nnegative_samples_koi['label'] = eb_df_processed['label'].values  # ä½¿ç”¨ .values é¿å…ç´¢å¼•å•é¡Œ\nnegative_samples_koi['source'] = eb_df_processed['source'].values\n\n# è™•ç† KOI ID\nif 'kepid' in eb_df_processed.columns:\n    negative_samples_koi['kepid'] = eb_df_processed['kepid'].values\n    negative_samples_koi['target_id'] = 'KIC' + pd.Series(eb_df_processed['kepid'].values).astype(str)\nelif 'target_id' in eb_df_processed.columns:\n    # æª¢æŸ¥æ˜¯å¦æœ‰é‡è¤‡çš„ target_id æ¬„ä½\n    if eb_df_processed['target_id'].ndim > 1:\n        # å¦‚æœæ˜¯ DataFrameï¼Œå–ç¬¬ä¸€æ¬„\n        negative_samples_koi['target_id'] = eb_df_processed['target_id'].iloc[:, 0].values\n    else:\n        negative_samples_koi['target_id'] = eb_df_processed['target_id'].values\nelse:\n    negative_samples_koi['target_id'] = 'KOI' + pd.Series(range(len(eb_df_processed))).astype(str)\n\n# æ˜ å°„ KOI ç‰©ç†åƒæ•¸ï¼ˆå®‰å…¨è™•ç†å¯èƒ½çš„é‡è¤‡æ¬„ä½ï¼‰\nfor param in ['period', 'depth', 'duration']:\n    if param in eb_df_processed.columns:\n        # æª¢æŸ¥æ¬„ä½æ˜¯å¦é‡è¤‡\n        col_data = eb_df_processed[param]\n        if isinstance(col_data, pd.DataFrame):\n            # å¦‚æœè¿”å› DataFrameï¼ˆæœ‰é‡è¤‡æ¬„ä½ï¼‰ï¼Œå–ç¬¬ä¸€æ¬„\n            negative_samples_koi[param] = col_data.iloc[:, 0].values\n        else:\n            # æ­£å¸¸çš„ Series\n            negative_samples_koi[param] = col_data.values\n\n# åˆä½µæ‰€æœ‰æ¨£æœ¬\nprint(\"\\n   åˆä½µè³‡æ–™é›†çµ±è¨ˆ:\")\nprint(f\"   - TOI æ­£æ¨£æœ¬: {len(positive_samples)} ç­†\")\nprint(f\"   - TOI è² æ¨£æœ¬ (FP): {len(negative_samples_fp)} ç­†\")\nprint(f\"   - KOI è² æ¨£æœ¬: {len(negative_samples_koi)} ç­†\")\n\nall_samples = pd.concat([\n    positive_samples,\n    negative_samples_fp,\n    negative_samples_koi\n], ignore_index=True)\n\n# ç§»é™¤å…¨ NaN çš„æ¬„ä½\nall_samples = all_samples.dropna(axis=1, how='all')\n\n# å„²å­˜åˆä½µè³‡æ–™é›†\ncombined_path = data_dir / \"supervised_dataset.csv\"\nall_samples.to_csv(combined_path, index=False)\nprint(f\"\\nâœ… åˆä½µè³‡æ–™é›†: {combined_path}\")\nprint(f\"   ç¸½æ¨£æœ¬æ•¸: {len(all_samples)} ç­†\")\nprint(f\"   æ­£æ¨£æœ¬: {(all_samples['label'] == 1).sum()} ç­†\")\nprint(f\"   è² æ¨£æœ¬: {(all_samples['label'] == 0).sum()} ç­†\")\n\n# è³‡æ–™å“è³ªå ±å‘Š\nprint(\"\\nğŸ“Š è³‡æ–™å®Œæ•´æ€§:\")\nfor col in ['period', 'depth', 'duration']:\n    if col in all_samples.columns:\n        valid = all_samples[col].notna().sum()\n        print(f\"   {col}: {valid}/{len(all_samples)} ({valid/len(all_samples)*100:.1f}%)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. è³‡æ–™ä¾†æºæ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å»ºç«‹è³‡æ–™ä¾†æºæ–‡ä»¶\nprovenance = {\n    \"download_timestamp\": download_timestamp,\n    \"data_sources\": {\n        \"toi\": {\n            \"source\": \"NASA Exoplanet Archive TOI Table\",\n            \"url\": \"https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=TOI\",\n            \"api_endpoint\": \"https://exoplanetarchive.ipac.caltech.edu/TAP\",\n            \"access_method\": \"astroquery.ipac.nexsci.nasa_exoplanet_archive\",\n            \"n_records\": len(toi_df),\n            \"n_positive\": len(toi_positive),\n            \"n_negative_fp\": len(toi_negative_fp),\n            \"column_mapping\": {\n                \"toi_period\": \"pl_orbper (days)\",\n                \"toi_depth\": \"pl_trandep (ppm)\",\n                \"toi_duration\": \"pl_trandurh (hours, converted to days)\",\n                \"toi_prad\": \"pl_rade (Earth radii)\"\n            },\n            \"columns_available\": list(toi_df.columns)[:20]  # åªåˆ—å‡ºå‰20å€‹æ¬„ä½\n        },\n        \"koi_false_positives\": {\n            \"source\": \"NASA Exoplanet Archive KOI Cumulative Table\",\n            \"url\": \"https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=cumulative\",\n            \"query\": \"WHERE koi_disposition='FALSE POSITIVE'\",\n            \"description\": \"KOI False Positives including eclipsing binaries\",\n            \"n_records\": len(eb_df_processed),\n            \"fallback_source\": \"Kirk et al. (2016) Kepler EB Catalog\",\n            \"columns\": list(eb_df_processed.columns)\n        },\n        \"combined_dataset\": {\n            \"file\": \"supervised_dataset.csv\",\n            \"n_total\": len(all_samples),\n            \"n_positive\": int((all_samples['label'] == 1).sum()),\n            \"n_negative\": int((all_samples['label'] == 0).sum()),\n            \"balance_ratio\": float((all_samples['label'] == 1).sum() / len(all_samples)),\n            \"sources\": {k: int(v) for k, v in all_samples['source'].value_counts().to_dict().items()}\n        }\n    },\n    \"known_issues\": [\n        \"TOI table uses pl_* prefix for physical parameters, not toi_*\",\n        \"pl_trandurh is in hours, requires conversion to days\",\n        \"Villanova EB catalog is inaccessible as of 2025\",\n        \"Many TOI entries have missing physical parameters\",\n        \"Using KOI False Positives as substitute for EB catalog\"\n    ],\n    \"column_definitions\": {\n        \"tfopwg_disp\": \"TFOPWG disposition (PC/CP/KP/FP/APC/FA)\",\n        \"PC\": \"Planet Candidate\",\n        \"CP\": \"Confirmed Planet\",\n        \"KP\": \"Known Planet\",\n        \"FP\": \"False Positive\",\n        \"APC\": \"Ambiguous Planet Candidate\",\n        \"FA\": \"False Alarm\",\n        \"pl_orbper\": \"Planetary orbital period in days\",\n        \"pl_trandep\": \"Transit depth in ppm\",\n        \"pl_trandurh\": \"Transit duration in hours\"\n    },\n    \"references\": [\n        \"NASA Exoplanet Archive: https://exoplanetarchive.ipac.caltech.edu/\",\n        \"TOI Column Definitions: https://exoplanetarchive.ipac.caltech.edu/docs/API_TOI_columns.html\",\n        \"Kirk et al. (2016) AJ 151:68 - Kepler Eclipsing Binary Catalog\",\n        \"Astroquery Documentation: https://astroquery.readthedocs.io/\"\n    ]\n}\n\n# å„²å­˜è³‡æ–™ä¾†æºæ–‡ä»¶\nprovenance_path = data_dir / \"data_provenance.json\"\nwith open(provenance_path, 'w') as f:\n    json.dump(provenance, f, indent=2, default=str)\n\nprint(\"\\nğŸ“ è³‡æ–™ä¾†æºæ–‡ä»¶å·²å»ºç«‹: data/data_provenance.json\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. è³‡æ–™æ‘˜è¦å ±å‘Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*60)\nprint(\"ğŸ“Š è³‡æ–™ä¸‹è¼‰æ‘˜è¦å ±å‘Š\")\nprint(\"=\"*60)\n\nprint(f\"\"\"\nğŸ“… ä¸‹è¼‰æ™‚é–“: {download_timestamp}\n\nğŸ¯ TOI (TESS Objects of Interest) çœŸå¯¦è³‡æ–™:\n   â€¢ ç¸½ç­†æ•¸: {len(toi_df):,}\n   â€¢ æ­£æ¨£æœ¬ (PC/CP/KP): {len(toi_positive):,}\n   â€¢ è² æ¨£æœ¬ (FP): {len(toi_negative_fp):,}\n   â€¢ è³‡æ–™ä¾†æº: NASA Exoplanet Archive (TAP Service)\n   â€¢ æ¬„ä½æ˜ å°„: pl_orbper â†’ toi_period, pl_trandep â†’ toi_depth\n   â€¢ å–®ä½è½‰æ›: pl_trandurh (å°æ™‚) â†’ toi_duration (å¤©)\n\nğŸŒŸ KOI False Positives (æ›¿ä»£ Kepler EB):\n   â€¢ ç¸½ç­†æ•¸: {len(eb_df_processed):,}\n   â€¢ å…¨éƒ¨æ¨™è¨˜ç‚ºè² æ¨£æœ¬ (åŒ…å« eclipsing binaries)\n   â€¢ è³‡æ–™ä¾†æº: NASA Archive KOI Cumulative Table\n   â€¢ æŸ¥è©¢æ¢ä»¶: koi_disposition = 'FALSE POSITIVE'\n   â€¢ å‚™ç”¨ä¾†æº: Kirk et al. (2016) ç¢ºèªçš„ EB ç³»çµ±\n\nğŸ“¦ åˆä½µè¨“ç·´è³‡æ–™é›†:\n   â€¢ ç¸½æ¨£æœ¬æ•¸: {len(all_samples):,}\n   â€¢ æ­£æ¨£æœ¬: {(all_samples['label'] == 1).sum():,} ({(all_samples['label'] == 1).sum()/len(all_samples)*100:.1f}%)\n   â€¢ è² æ¨£æœ¬: {(all_samples['label'] == 0).sum():,} ({(all_samples['label'] == 0).sum()/len(all_samples)*100:.1f}%)\n\n   è³‡æ–™ä¾†æºåˆ†å¸ƒ:\n\"\"\")\n\n# é¡¯ç¤ºè³‡æ–™ä¾†æºåˆ†å¸ƒ\nif 'source' in all_samples.columns:\n    source_counts = all_samples['source'].value_counts()\n    for source, count in source_counts.items():\n        print(f\"   â€¢ {source}: {count:,} ç­†\")\n\nprint(f\"\"\"\n\nğŸ’¾ è¼¸å‡ºæª”æ¡ˆ:\n   â€¢ data/toi.csv - å®Œæ•´ TOI è³‡æ–™ (å« pl_* åŸå§‹æ¬„ä½)\n   â€¢ data/toi_positive.csv - TOI æ­£æ¨£æœ¬ (PC/CP/KP)\n   â€¢ data/toi_negative.csv - TOI è² æ¨£æœ¬ (FP)\n   â€¢ data/koi_false_positives.csv - KOI False Positives (æ›¿ä»£ EB)\n   â€¢ data/supervised_dataset.csv - åˆä½µè¨“ç·´è³‡æ–™é›†\n   â€¢ data/data_provenance.json - è©³ç´°è³‡æ–™ä¾†æºæ–‡ä»¶\n\nâš ï¸ é‡è¦ç™¼ç¾èˆ‡è§£æ±ºæ–¹æ¡ˆ:\n   1. TOI ä½¿ç”¨ pl_* å‰ç¶´è€Œé toi_* (å·²æ˜ å°„è™•ç†)\n   2. pl_trandurh å–®ä½æ˜¯å°æ™‚éœ€è½‰æ› (å·²è™•ç† /24)\n   3. Villanova EB ç›®éŒ„ç„¡æ³•å­˜å– (æ”¹ç”¨ KOI FP)\n   4. éƒ¨åˆ† TOI ç¼ºå°‘ç‰©ç†åƒæ•¸ (éœ€å¾å…‰æ›²ç·šè¨ˆç®—)\n\nğŸ“Š è³‡æ–™å“è³ªè©•ä¼°:\n\"\"\")\n\n# é¡¯ç¤ºè³‡æ–™å®Œæ•´æ€§\nfor col in ['period', 'depth', 'duration']:\n    if col in all_samples.columns:\n        valid_count = all_samples[col].notna().sum()\n        valid_pct = valid_count / len(all_samples) * 100\n        print(f\"   â€¢ {col}: {valid_count:,}/{len(all_samples):,} ({valid_pct:.1f}%) æœ‰æ•ˆå€¼\")\n\nprint(f\"\"\"\n\nğŸš€ ä¸‹ä¸€æ­¥å»ºè­°:\n   1. åŸ·è¡Œ 02_bls_baseline.ipynb è¨ˆç®— BLS/TLS ç‰¹å¾µ\n   2. è‹¥ç‰©ç†åƒæ•¸ä¸è¶³ï¼Œå¾å…‰æ›²ç·šç›´æ¥è¨ˆç®—\n   3. è€ƒæ…®è³‡æ–™å¢å¼·æˆ– SMOTE å¹³è¡¡æ­£è² æ¨£æœ¬\n   4. é©—è­‰è³‡æ–™å“è³ªå¾Œå†è¨“ç·´æ¨¡å‹\n\nâœ… çœŸå¯¦è³‡æ–™ä¸‹è¼‰å®Œæˆï¼æ‰€æœ‰è³‡æ–™ä¾†è‡ª NASA å®˜æ–¹è³‡æ–™åº«ï¼Œç„¡æ¨¡æ“¬è³‡æ–™ï¼\"\"\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}