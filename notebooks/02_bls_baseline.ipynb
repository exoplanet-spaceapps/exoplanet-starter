{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Â· BLS Baseline - å…‰æ›²ç·šåˆ†æèˆ‡é€±æœŸæœå°‹\n",
    "\n",
    "## ç›®æ¨™\n",
    "1. **è³‡æ–™æŠ“å–èˆ‡æ¸…ç†**ï¼šä½¿ç”¨ Lightkurve ä¸‹è¼‰ TESS/Kepler å…‰æ›²ç·š\n",
    "2. **å»è¶¨å‹¢è™•ç†**ï¼šç§»é™¤ç³»çµ±æ€§é›œè¨Šä»¥å‡¸é¡¯å‡Œæ—¥è¨Šè™Ÿ\n",
    "3. **BLS/TLS æœå°‹**ï¼šå°‹æ‰¾é€±æœŸæ€§å‡Œæ—¥äº‹ä»¶\n",
    "4. **è¦–è¦ºåŒ–èˆ‡åˆ†æ**ï¼šæ¯”è¼ƒä¸åŒæ–¹æ³•çš„æ•ˆèƒ½å·®ç•°\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ­¥é©Ÿ 0: å®‰è£å¥—ä»¶èˆ‡ä¿®å¾© NumPy 2.0 ç›¸å®¹æ€§ (Colab ç’°å¢ƒ)\n",
    "# âš ï¸ é‡è¦: è‹¥åœ¨ Google Colabï¼ŒåŸ·è¡Œæ­¤ cell å¾Œè«‹æ‰‹å‹•é‡å•Ÿ Runtime (Runtime â†’ Restart runtime)\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"ğŸ“ åµæ¸¬åˆ° Google Colab ç’°å¢ƒ\")\n",
    "    print(\"ğŸ”§ å®‰è£ç›¸å®¹ç‰ˆæœ¬å¥—ä»¶...\")\n",
    "    !pip install -q numpy==1.26.4 pandas astropy scipy'<1.13' matplotlib scikit-learn\n",
    "    !pip install -q lightkurve astroquery transitleastsquares wotan\n",
    "    print(\"âœ… å¥—ä»¶å®‰è£å®Œæˆ!\")\n",
    "    print(\"âš ï¸ è«‹ç¾åœ¨æ‰‹å‹•é‡å•Ÿ Runtime: Runtime â†’ Restart runtime\")\n",
    "    print(\"   ç„¶å¾Œå¾ä¸‹ä¸€å€‹ cell ç¹¼çºŒåŸ·è¡Œ\")\n",
    "else:\n",
    "    print(\"ğŸ’» æœ¬åœ°ç’°å¢ƒï¼Œè·³éå¥—ä»¶å®‰è£\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒè¨­å®šèˆ‡ä¾è³´å®‰è£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç’°å¢ƒè¨­å®šèˆ‡ä¾è³´å®‰è£ï¼ˆColabï¼‰\n",
    "import sys, subprocess, pkgutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def pipi(*pkgs):\n",
    "    \"\"\"å®‰è£å¥—ä»¶çš„è¼”åŠ©å‡½å¼\"\"\"\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *pkgs])\n",
    "\n",
    "# å®‰è£å¿…è¦å¥—ä»¶ï¼ˆé¿å… numpy 2.0 ç›¸å®¹æ€§å•é¡Œï¼‰\n",
    "print(\"ğŸš€ æ­£åœ¨å®‰è£ä¾è³´å¥—ä»¶...\")\n",
    "try:\n",
    "    import numpy as np\n",
    "    import lightkurve as lk\n",
    "    import transitleastsquares as tls\n",
    "    print(\"âœ… åŸºç¤å¥—ä»¶å·²å®‰è£\")\n",
    "except Exception:\n",
    "    pipi(\"numpy<2\", \"lightkurve\", \"astroquery\", \"scikit-learn\", \n",
    "         \"matplotlib\", \"wotan\", \"transitleastsquares\")\n",
    "    print(\"âœ… ä¾è³´å¥—ä»¶å®‰è£å®Œæˆ\")\n",
    "\n",
    "# æª¢æŸ¥ GPU è³‡è¨Š\n",
    "# æª¢æŸ¥ GPU è³‡è¨Šï¼ˆå˜—è©¦å°å…¥ torchï¼‰\n",
    "try:\n",
    "    import torch\n",
    "except ImportError:\n",
    "    torch = None\n",
    "\n",
    "if torch is not None and torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"ğŸ–¥ï¸ GPU å‹è™Ÿ: {gpu_name}\")\n",
    "    print(f\"   è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # å¦‚æœæ˜¯ NVIDIA L4ï¼Œæä¾› BF16 å„ªåŒ–å»ºè­°\n",
    "    if \"L4\" in gpu_name:\n",
    "        print(\"ğŸ’¡ åµæ¸¬åˆ° NVIDIA L4 GPU - æ”¯æ´é«˜æ•ˆèƒ½ BF16 é‹ç®—\")\n",
    "        print(\"   å»ºè­°åœ¨è¨“ç·´æ™‚ä½¿ç”¨ torch.autocast('cuda', dtype=torch.bfloat16)\")\n",
    "else:\n",
    "    try:\n",
    "        # ä½¿ç”¨ nvidia-smi æª¢æŸ¥ GPU\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name', '--format=csv,noheader'], \n",
    "                              capture_output=True, text=True, check=False)\n",
    "        if result.returncode == 0:\n",
    "            gpu_name = result.stdout.strip()\n",
    "            print(f\"ğŸ–¥ï¸ GPU å‹è™Ÿ: {gpu_name}\")\n",
    "            if \"L4\" in gpu_name:\n",
    "                print(\"ğŸ’¡ åµæ¸¬åˆ° NVIDIA L4 GPU - æ”¯æ´é«˜æ•ˆèƒ½ BF16 é‹ç®—\")\n",
    "    except:\n",
    "        print(\"âš ï¸ æœªåµæ¸¬åˆ° GPUï¼Œå°‡ä½¿ç”¨ CPU é‹ç®—\")\n",
    "\n",
    "print(\"\\nç’°å¢ƒè¨­å®šå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ è¨­å®šå¯é‡ç¾æ€§èˆ‡æ—¥èªŒè¨˜éŒ„ (2025 Best Practices)\"\"\"Phase 1: Critical Infrastructure- è¨­å®šéš¨æ©Ÿç¨®å­ç¢ºä¿å¯é‡ç¾æ€§- åˆå§‹åŒ–æ—¥èªŒè¨˜éŒ„ç³»çµ±- è¨˜éŒ„ç³»çµ±ç’°å¢ƒè³‡è¨Š\"\"\"import sysimport osfrom pathlib import Path# ç¢ºä¿ src ç›®éŒ„åœ¨ Python è·¯å¾‘ä¸­if IN_COLAB:    # Colab ç’°å¢ƒï¼šå°ˆæ¡ˆåœ¨ /content/exoplanet-starter    src_path = Path('/content/exoplanet-starter/src')else:    # æœ¬åœ°ç’°å¢ƒï¼šå‘ä¸Šä¸€å±¤æ‰¾åˆ°å°ˆæ¡ˆæ ¹ç›®éŒ„    src_path = Path(__file__).parent.parent / 'src' if '__file__' in globals() else Path('../src').resolve()if src_path.exists() and str(src_path) not in sys.path:    sys.path.insert(0, str(src_path))    print(f\"ğŸ“‚ å·²æ·»åŠ  src è·¯å¾‘: {src_path}\")# å°å…¥å·¥å…·æ¨¡çµ„try:    from utils import set_random_seeds, setup_logger, get_log_file_path, log_system_info    # 1ï¸âƒ£ è¨­å®šéš¨æ©Ÿç¨®å­ (ç¢ºä¿å¯é‡ç¾æ€§)    set_random_seeds(42)    # 2ï¸âƒ£ è¨­å®šæ—¥èªŒè¨˜éŒ„    log_file = get_log_file_path(\"02_bls_baseline\", results_dir=Path(\"../results\") if not IN_COLAB else Path(\"/content/exoplanet-starter/results\"))    logger = setup_logger(\"02_bls_baseline\", log_file=log_file, verbose=True)    # 3ï¸âƒ£ è¨˜éŒ„ç³»çµ±è³‡è¨Š    logger.info(\"=\"*60)    logger.info(\"ğŸš€ 02_bls_baseline.ipynb é–‹å§‹åŸ·è¡Œ\")    logger.info(\"=\"*60)    log_system_info(logger)    print(\"âœ… å¯é‡ç¾æ€§èˆ‡æ—¥èªŒè¨˜éŒ„è¨­å®šå®Œæˆ\")    print(f\"   ğŸ“ æ—¥èªŒæª”æ¡ˆ: {log_file}\")    print(f\"   ğŸ² éš¨æ©Ÿç¨®å­: 42\")except ImportError as e:    print(f\"âš ï¸ ç„¡æ³•å°å…¥å·¥å…·æ¨¡çµ„: {e}\")    print(\"   è·³éå¯é‡ç¾æ€§è¨­å®šï¼Œç¹¼çºŒåŸ·è¡Œ...\")    # å¦‚æœå°å…¥å¤±æ•—ï¼Œå‰µå»ºä¸€å€‹ç°¡å–®çš„ logger fallback    import logging    logger = logging.getLogger(\"02_bls_baseline\")    logger.addHandler(logging.StreamHandler(sys.stdout))    logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. å°å…¥å¿…è¦å¥—ä»¶èˆ‡è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightkurve as lk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import pandas as pd\n",
    "from transitleastsquares import transitleastsquares\n",
    "from typing import Dict, Any, Tuple, Optional\n",
    "import time\n",
    "\n",
    "# è¨­å®šåœ–è¡¨é¢¨æ ¼\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"ğŸ“š å¥—ä»¶å°å…¥å®Œæˆ\")\n",
    "print(f\"   Lightkurve ç‰ˆæœ¬: {lk.__version__}\")\n",
    "print(f\"   NumPy ç‰ˆæœ¬: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. è³‡æ–™æŠ“å–èˆ‡æ¸…ç†\n",
    "\n",
    "### 3.1 ç›®æ¨™å¤©é«”é¸æ“‡\n",
    "æˆ‘å€‘å°‡åˆ†æä¸‰å€‹å·²ç¢ºèªçš„ç³»å¤–è¡Œæ˜Ÿå®¿ä¸»æ†æ˜Ÿï¼š\n",
    "- **2å€‹ TESS ç›®æ¨™ (TIC)**ï¼šTIC 25155310ï¼ˆTOI-431ï¼‰ã€TIC 307210830ï¼ˆTOI-270ï¼‰\n",
    "- **1å€‹ Kepler ç›®æ¨™ (KIC)**ï¼šKIC 11904151ï¼ˆKepler-10ï¼‰\n",
    "\n",
    "é€™äº›ç›®æ¨™éƒ½æœ‰å·²ç¢ºèªçš„è¡Œæ˜Ÿï¼Œé©åˆä½œç‚ºåŸºæº–æ¸¬è©¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9998bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª === COMPREHENSIVE TESTING SUITE === ğŸ§ª\n",
    "# Run this cell to validate all critical components before full execution\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ§ª Running Notebook 02 Validation Tests...\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "test_results = []\n",
    "\n",
    "# ==========================================\n",
    "# Test 1/5: NumPy Version Verification\n",
    "# ==========================================\n",
    "print(\"Test 1/5: NumPy version compatibility...\")\n",
    "try:\n",
    "    import numpy as np\n",
    "    version = np.__version__\n",
    "    is_compatible = version.startswith('1.26')\n",
    "\n",
    "    if is_compatible:\n",
    "        print(f\"  âœ… NumPy {version} detected (compatible)\")\n",
    "        test_results.append((\"NumPy version\", True))\n",
    "    else:\n",
    "        print(f\"  âŒ NumPy {version} incompatible (need 1.26.x)\")\n",
    "        print(f\"     Run: pip install numpy==1.26.4\")\n",
    "        test_results.append((\"NumPy version\", False))\n",
    "except Exception as e:\n",
    "    print(f\"  âŒ NumPy check failed: {e}\")\n",
    "    test_results.append((\"NumPy version\", False))\n",
    "print()\n",
    "\n",
    "# ==========================================\n",
    "# Test 2/5: Checkpoint System\n",
    "# ==========================================\n",
    "print(\"Test 2/5: Checkpoint system functionality...\")\n",
    "try:\n",
    "    import os\n",
    "    import json\n",
    "    import tempfile\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Create temporary checkpoint directory\n",
    "    test_checkpoint_dir = tempfile.mkdtemp(prefix='test_checkpoints_')\n",
    "\n",
    "    # Define CheckpointManager class for testing\n",
    "    class TestCheckpointManager:\n",
    "        def __init__(self, checkpoint_dir, batch_size=10):\n",
    "            self.checkpoint_dir = Path(checkpoint_dir)\n",
    "            self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "            self.batch_size = batch_size\n",
    "            self.checkpoint_file = self.checkpoint_dir / 'progress.json'\n",
    "\n",
    "        def save_batch(self, data, batch_num):\n",
    "            checkpoint = {\n",
    "                'last_batch': batch_num,\n",
    "                'timestamp': str(pd.Timestamp.now()),\n",
    "                'batch_size': self.batch_size,\n",
    "                'data_sample': data\n",
    "            }\n",
    "            with open(self.checkpoint_file, 'w') as f:\n",
    "                json.dump(checkpoint, f)\n",
    "\n",
    "        def resume_from_last(self):\n",
    "            if self.checkpoint_file.exists():\n",
    "                with open(self.checkpoint_file, 'r') as f:\n",
    "                    checkpoint = json.load(f)\n",
    "                return checkpoint['last_batch'] + 1\n",
    "            return 0\n",
    "\n",
    "    # Test checkpoint save and resume\n",
    "    test_checkpoint_mgr = TestCheckpointManager(test_checkpoint_dir, batch_size=10)\n",
    "    test_data = {'sample_id': [1, 2, 3], 'bls_period': [3.5, 4.2, 2.1]}\n",
    "    test_checkpoint_mgr.save_batch(test_data, 0)\n",
    "    resumed_batch = test_checkpoint_mgr.resume_from_last()\n",
    "\n",
    "    # Cleanup\n",
    "    import shutil\n",
    "    shutil.rmtree(test_checkpoint_dir)\n",
    "\n",
    "    if resumed_batch == 1:\n",
    "        print(f\"  âœ… Checkpoint system working (resumed batch: {resumed_batch})\")\n",
    "        test_results.append((\"Checkpoint system\", True))\n",
    "    else:\n",
    "        print(f\"  âŒ Checkpoint resume failed (expected 1, got {resumed_batch})\")\n",
    "        test_results.append((\"Checkpoint system\", False))\n",
    "except Exception as e:\n",
    "    print(f\"  âŒ Checkpoint test failed: {e}\")\n",
    "    test_results.append((\"Checkpoint system\", False))\n",
    "print()\n",
    "\n",
    "# ==========================================\n",
    "# Test 3/5: Single Sample Feature Extraction\n",
    "# ==========================================\n",
    "print(\"Test 3/5: Feature extraction (single target)...\")\n",
    "try:\n",
    "    import lightkurve as lk\n",
    "    from astropy.timeseries import BoxLeastSquares\n",
    "\n",
    "    # Test with known TOI target\n",
    "    test_tic = \"25155310\"  # TOI-270 (known multi-planet system)\n",
    "\n",
    "    print(f\"  ğŸ“¡ Testing with TIC {test_tic} (TOI-270)...\")\n",
    "\n",
    "    # Download light curve\n",
    "    search_result = lk.search_lightcurve(f'TIC {test_tic}', mission='TESS', author='SPOC')\n",
    "\n",
    "    if len(search_result) > 0:\n",
    "        lc = search_result[0].download()\n",
    "        lc = lc.remove_nans().remove_outliers(sigma=5)\n",
    "\n",
    "        # Run BLS\n",
    "        period_grid = np.linspace(1.0, 15.0, 5000)\n",
    "        bls = BoxLeastSquares(lc.time.value, lc.flux.value)\n",
    "        bls_result = bls.power(period_grid)\n",
    "\n",
    "        # Extract basic features\n",
    "        test_features = {\n",
    "            'tic_id': test_tic,\n",
    "            'bls_period': float(bls_result.period[np.argmax(bls_result.power)]),\n",
    "            'bls_power': float(np.max(bls_result.power)),\n",
    "            'bls_depth': float(bls_result.depth[np.argmax(bls_result.power)]),\n",
    "            'bls_duration': float(bls_result.duration[np.argmax(bls_result.power)]),\n",
    "            'num_points': len(lc.time),\n",
    "            'flux_std': float(np.std(lc.flux.value)),\n",
    "            'flux_median': float(np.median(lc.flux.value))\n",
    "        }\n",
    "\n",
    "        # Validation\n",
    "        feature_count = len(test_features)\n",
    "        has_nan = any(pd.isna(list(test_features.values())))\n",
    "        valid_period = 1.0 <= test_features['bls_period'] <= 15.0\n",
    "\n",
    "        if not has_nan and valid_period and feature_count >= 8:\n",
    "            print(f\"  âœ… Extracted {feature_count} features successfully\")\n",
    "            print(f\"     - Period: {test_features['bls_period']:.3f} days\")\n",
    "            print(f\"     - Power: {test_features['bls_power']:.4f}\")\n",
    "            print(f\"     - Data points: {test_features['num_points']}\")\n",
    "            test_results.append((\"Feature extraction\", True))\n",
    "        else:\n",
    "            print(f\"  âš ï¸  Features extracted but validation issues:\")\n",
    "            print(f\"     - NaN values: {has_nan}\")\n",
    "            print(f\"     - Valid period: {valid_period}\")\n",
    "            test_results.append((\"Feature extraction\", False))\n",
    "    else:\n",
    "        print(f\"  âš ï¸  No data found for TIC {test_tic}\")\n",
    "        print(f\"     This is expected if MAST is unavailable\")\n",
    "        test_results.append((\"Feature extraction\", None))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  âš ï¸  Feature extraction test skipped: {e}\")\n",
    "    print(f\"     This is normal if MAST/Lightkurve is unavailable\")\n",
    "    test_results.append((\"Feature extraction\", None))\n",
    "print()\n",
    "\n",
    "# ==========================================\n",
    "# Test 4/5: Google Drive Access (Colab only)\n",
    "# ==========================================\n",
    "print(\"Test 4/5: Google Drive access...\")\n",
    "try:\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Check if running in Colab\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        in_colab = True\n",
    "    except ImportError:\n",
    "        in_colab = False\n",
    "\n",
    "    if in_colab:\n",
    "        # Test Drive access\n",
    "        base_path = Path('/content/drive/MyDrive/spaceapps-exoplanet')\n",
    "        checkpoint_dir = base_path / 'checkpoints'\n",
    "\n",
    "        # Try to create and write test file\n",
    "        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        test_file = checkpoint_dir / 'test_access.txt'\n",
    "\n",
    "        with open(test_file, 'w') as f:\n",
    "            f.write('test')\n",
    "\n",
    "        exists = test_file.exists()\n",
    "\n",
    "        # Cleanup\n",
    "        test_file.unlink()\n",
    "\n",
    "        if exists:\n",
    "            print(f\"  âœ… Google Drive writable at {checkpoint_dir}\")\n",
    "            test_results.append((\"Google Drive access\", True))\n",
    "        else:\n",
    "            print(f\"  âŒ Cannot write to Google Drive\")\n",
    "            test_results.append((\"Google Drive access\", False))\n",
    "    else:\n",
    "        print(f\"  â„¹ï¸  Not in Colab environment (local execution)\")\n",
    "        print(f\"     Checkpoint directory will use: ./checkpoints/\")\n",
    "        test_results.append((\"Google Drive access\", None))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  âŒ Google Drive test failed: {e}\")\n",
    "    test_results.append((\"Google Drive access\", False))\n",
    "print()\n",
    "\n",
    "# ==========================================\n",
    "# Test 5/5: Batch Processing (Mini Test)\n",
    "# ==========================================\n",
    "print(\"Test 5/5: Batch processing (small scale)...\")\n",
    "try:\n",
    "    # Load sample data\n",
    "    data_path = Path('/content/drive/MyDrive/spaceapps-exoplanet/data') if in_colab else Path('./data')\n",
    "    supervised_csv = data_path / 'supervised_dataset.csv'\n",
    "\n",
    "    if supervised_csv.exists():\n",
    "        samples_df = pd.read_csv(supervised_csv)\n",
    "        test_samples = samples_df.head(5)  # Test with 5 samples\n",
    "\n",
    "        print(f\"  ğŸ“Š Testing with {len(test_samples)} samples...\")\n",
    "\n",
    "        # Mock batch processing\n",
    "        successful = 0\n",
    "        failed = 0\n",
    "\n",
    "        for idx, row in test_samples.iterrows():\n",
    "            tic_id = row['TIC_ID']\n",
    "            try:\n",
    "                # Simulate feature extraction (lightweight)\n",
    "                search_result = lk.search_lightcurve(f'TIC {tic_id}', mission='TESS', author='SPOC')\n",
    "                if len(search_result) > 0:\n",
    "                    successful += 1\n",
    "                else:\n",
    "                    failed += 1\n",
    "            except Exception:\n",
    "                failed += 1\n",
    "\n",
    "        success_rate = successful / len(test_samples) if len(test_samples) > 0 else 0\n",
    "\n",
    "        if success_rate >= 0.4:  # At least 40% success\n",
    "            print(f\"  âœ… Batch test: {success_rate*100:.1f}% success rate ({successful}/{len(test_samples)})\")\n",
    "            test_results.append((\"Batch processing\", True))\n",
    "        else:\n",
    "            print(f\"  âš ï¸  Low success rate: {success_rate*100:.1f}% ({successful}/{len(test_samples)})\")\n",
    "            print(f\"     This may indicate MAST availability issues\")\n",
    "            test_results.append((\"Batch processing\", False))\n",
    "    else:\n",
    "        print(f\"  â„¹ï¸  supervised_dataset.csv not found\")\n",
    "        print(f\"     Expected at: {supervised_csv}\")\n",
    "        test_results.append((\"Batch processing\", None))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  âš ï¸  Batch processing test skipped: {e}\")\n",
    "    test_results.append((\"Batch processing\", None))\n",
    "print()\n",
    "\n",
    "# ==========================================\n",
    "# Summary Report\n",
    "# ==========================================\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š TEST SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "passed = sum(1 for _, result in test_results if result is True)\n",
    "failed = sum(1 for _, result in test_results if result is False)\n",
    "skipped = sum(1 for _, result in test_results if result is None)\n",
    "\n",
    "for test_name, result in test_results:\n",
    "    status = \"âœ… PASS\" if result is True else (\"âŒ FAIL\" if result is False else \"âš ï¸  SKIP\")\n",
    "    print(f\"{status:12} - {test_name}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"Results: {passed} passed, {failed} failed, {skipped} skipped\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if failed == 0 and passed >= 3:\n",
    "    print(\"âœ… All critical tests passed! Ready for production run.\")\n",
    "    print(\"   You can now proceed with full feature extraction.\")\n",
    "elif failed > 0:\n",
    "    print(\"âš ï¸  Some tests failed. Please review errors above.\")\n",
    "    print(\"   Fix issues before running full extraction.\")\n",
    "else:\n",
    "    print(\"â„¹ï¸  Most tests skipped (likely due to data availability).\")\n",
    "    print(\"   This is normal for offline/local testing.\")\n",
    "\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ è¼‰å…¥å·²ä¸‹è¼‰çš„è³‡æ–™é›†\n",
    "\"\"\"\n",
    "å¾ 01_tap_download.ipynb è¼‰å…¥å·²è™•ç†çš„è³‡æ–™\n",
    "ä½¿ç”¨ data_loader_colab.py æ¨¡çµ„é€²è¡Œçµ±ä¸€çš„è³‡æ–™è¼‰å…¥\n",
    "\"\"\"\n",
    "\n",
    "# å°å…¥è³‡æ–™è¼‰å…¥æ¨¡çµ„\n",
    "import data_loader_colab\n",
    "\n",
    "# åŸ·è¡Œå®Œæ•´çš„è³‡æ–™è¼‰å…¥æµç¨‹\n",
    "# è‡ªå‹•è™•ç† Colab/æœ¬åœ°ç’°å¢ƒå·®ç•°ï¼Œå¾ GitHub å…‹éš†è³‡æ–™ï¼ˆå¦‚éœ€è¦ï¼‰\n",
    "sample_targets, datasets, data_dir, IN_COLAB = data_loader_colab.main()\n",
    "\n",
    "# è³‡æ–™è¼‰å…¥å®Œæˆï¼Œå¯ä»¥é–‹å§‹åˆ†æ\n",
    "print(f\"\\nâœ… è³‡æ–™è¼‰å…¥å®Œæˆï¼\")\n",
    "print(f\"   ğŸ“‚ è³‡æ–™ç›®éŒ„: {data_dir}\")\n",
    "print(f\"   ğŸŒ ç’°å¢ƒ: {'Google Colab' if IN_COLAB else 'æœ¬åœ°ç’°å¢ƒ'}\")\n",
    "print(f\"   ğŸ“Š è¼‰å…¥è³‡æ–™é›†: {len(datasets)} å€‹\")\n",
    "print(f\"   ğŸ¯ åˆ†ææ¨£æœ¬: {len(sample_targets)} å€‹ç›®æ¨™\")\n",
    "print(f\"\\næº–å‚™é–‹å§‹ BLS/TLS åŸºç·šåˆ†æ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ å»ºç«‹åˆ†æç›®æ¨™åˆ—è¡¨\n",
    "\"\"\"\n",
    "å¾è¼‰å…¥çš„è³‡æ–™å»ºç«‹ç›®æ¨™å¤©é«”åˆ—è¡¨ä¾› BLS/TLS åˆ†æ\n",
    "\"\"\"\n",
    "\n",
    "targets = []\n",
    "\n",
    "# å¾æ¨£æœ¬ä¸­å»ºç«‹ç›®æ¨™åˆ—è¡¨\n",
    "for idx, row in sample_targets.iterrows():\n",
    "    # æå– TIC/KIC ID\n",
    "    target_id = row.get('target_id', f'Unknown_{idx}')\n",
    "    \n",
    "    # æ¸…ç†ä¸¦æ ¼å¼åŒ– ID\n",
    "    if 'TIC' in str(target_id):\n",
    "        clean_id = str(target_id).replace('TIC', '').strip()\n",
    "        formatted_id = f\"TIC {clean_id}\"\n",
    "        mission = \"TESS\"\n",
    "    elif 'KIC' in str(target_id):\n",
    "        clean_id = str(target_id).replace('KIC', '').strip() \n",
    "        formatted_id = f\"KIC {clean_id}\"\n",
    "        mission = \"Kepler\"\n",
    "    else:\n",
    "        # å¦‚æœæ²’æœ‰æ˜ç¢ºæ¨™ç¤ºï¼Œæ ¹æ“š ID ç¯„åœåˆ¤æ–·\n",
    "        try:\n",
    "            id_num = int(''.join(filter(str.isdigit, str(target_id))))\n",
    "            if id_num > 100000000:  # å¤§æ–¼1å„„é€šå¸¸æ˜¯TIC\n",
    "                formatted_id = f\"TIC {id_num}\"\n",
    "                mission = \"TESS\"\n",
    "            else:  # å¦å‰‡å‡è¨­æ˜¯KIC\n",
    "                formatted_id = f\"KIC {id_num}\"\n",
    "                mission = \"Kepler\"\n",
    "        except:\n",
    "            formatted_id = str(target_id)\n",
    "            mission = \"Unknown\"\n",
    "    \n",
    "    # å»ºç«‹ç›®æ¨™å­—å…¸\n",
    "    target_dict = {\n",
    "        \"id\": formatted_id,\n",
    "        \"mission\": mission,\n",
    "        \"name\": row.get('toi', row.get('target_name', target_id)),\n",
    "        \"description\": f\"{'æ­£æ¨£æœ¬ (è¡Œæ˜Ÿå€™é¸)' if row['label'] == 1 else 'è² æ¨£æœ¬ (False Positive)'}\",\n",
    "        \"label\": row['label'],\n",
    "        \"source\": row.get('source', 'Unknown')\n",
    "    }\n",
    "    \n",
    "    # æ·»åŠ ç‰©ç†åƒæ•¸ï¼ˆå¦‚æœæœ‰ï¼‰\n",
    "    if 'period' in row and pd.notna(row['period']):\n",
    "        target_dict['known_period'] = float(row['period'])\n",
    "    if 'depth' in row and pd.notna(row['depth']):\n",
    "        target_dict['known_depth'] = float(row['depth'])\n",
    "    \n",
    "    targets.append(target_dict)\n",
    "\n",
    "# å¦‚æœæ²’æœ‰å¾è³‡æ–™è¼‰å…¥ç›®æ¨™ï¼Œä½¿ç”¨é è¨­ç›®æ¨™\n",
    "if len(targets) == 0:\n",
    "    print(\"âš ï¸ ç„¡æ³•å¾è³‡æ–™é›†è¼‰å…¥ç›®æ¨™ï¼Œä½¿ç”¨é è¨­ç›®æ¨™\")\n",
    "    targets = [\n",
    "        {\"id\": \"TIC 25155310\", \"mission\": \"TESS\", \"name\": \"TOI-431\", \n",
    "         \"description\": \"æ“æœ‰3é¡†å·²ç¢ºèªè¡Œæ˜Ÿçš„Kå‹çŸ®æ˜Ÿ\", \"label\": 1, \"source\": \"default\"},\n",
    "        {\"id\": \"TIC 307210830\", \"mission\": \"TESS\", \"name\": \"TOI-270\",\n",
    "         \"description\": \"æ“æœ‰3é¡†å°å‹è¡Œæ˜Ÿçš„Må‹çŸ®æ˜Ÿ\", \"label\": 1, \"source\": \"default\"},\n",
    "        {\"id\": \"KIC 11904151\", \"mission\": \"Kepler\", \"name\": \"Kepler-10\",\n",
    "         \"description\": \"ç¬¬ä¸€å€‹è¢«ç¢ºèªçš„å²©çŸ³ç³»å¤–è¡Œæ˜Ÿå®¿ä¸»æ†æ˜Ÿ\", \"label\": 1, \"source\": \"default\"}\n",
    "    ]\n",
    "\n",
    "print(\"ğŸ¯ åˆ†æç›®æ¨™ï¼š\")\n",
    "for i, target in enumerate(targets, 1):\n",
    "    print(f\"   {i}. {target['name']} ({target['id']}) - {target['mission']}\")\n",
    "    print(f\"      {target['description']}\")\n",
    "    if 'known_period' in target:\n",
    "        print(f\"      å·²çŸ¥é€±æœŸ: {target['known_period']:.3f} å¤©\")\n",
    "    if 'known_depth' in target:\n",
    "        print(f\"      å·²çŸ¥æ·±åº¦: {target['known_depth']:.0f} ppm\")\n",
    "    print()\n",
    "\n",
    "print(f\"âœ… å»ºç«‹å®Œæˆï¼Œå…± {len(targets)} å€‹åˆ†æç›®æ¨™\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 å…‰æ›²ç·šä¸‹è¼‰èˆ‡è™•ç†å‡½å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_process_lightcurve(\n",
    "    target_id: str, \n",
    "    mission: str, \n",
    "    author: str = \"SPOC\",\n",
    "    cadence: str = \"short\"\n",
    ") -> Tuple[lk.LightCurve, lk.LightCurve, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    ä¸‹è¼‰ä¸¦è™•ç†å…‰æ›²ç·šè³‡æ–™\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    target_id : str\n",
    "        ç›®æ¨™å¤©é«”è­˜åˆ¥ç¢¼ï¼ˆTIC/KICï¼‰\n",
    "    mission : str\n",
    "        ä»»å‹™åç¨±ï¼ˆTESS/Keplerï¼‰\n",
    "    author : str\n",
    "        è³‡æ–™æä¾›è€…ï¼ˆSPOC/PDCSAPï¼‰\n",
    "    cadence : str\n",
    "        è§€æ¸¬é »ç‡ï¼ˆshort/longï¼‰\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (åŸå§‹å…‰æ›²ç·š, å»è¶¨å‹¢å…‰æ›²ç·š, metadataå­—å…¸)\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ“¡ æ­£åœ¨ä¸‹è¼‰ {target_id} çš„å…‰æ›²ç·š...\")\n",
    "    \n",
    "    # æœå°‹ä¸¦ä¸‹è¼‰å…‰æ›²ç·š\n",
    "    search_result = lk.search_lightcurve(\n",
    "        target_id, \n",
    "        mission=mission, \n",
    "        author=author if mission == \"TESS\" else None,\n",
    "        cadence=cadence\n",
    "    )\n",
    "    \n",
    "    if len(search_result) == 0:\n",
    "        raise ValueError(f\"æœªæ‰¾åˆ° {target_id} çš„å…‰æ›²ç·šè³‡æ–™\")\n",
    "    \n",
    "    print(f\"   æ‰¾åˆ° {len(search_result)} å€‹å…‰æ›²ç·šæª”æ¡ˆ\")\n",
    "    \n",
    "    # ä¸‹è¼‰ç¬¬ä¸€å€‹sector/quarterçš„è³‡æ–™\n",
    "    lc_collection = search_result[0].download()\n",
    "    \n",
    "    # å¦‚æœæ˜¯collectionï¼Œå–ç¬¬ä¸€å€‹å…‰æ›²ç·š\n",
    "    if hasattr(lc_collection, '__iter__'):\n",
    "        lc_raw = lc_collection[0]\n",
    "    else:\n",
    "        lc_raw = lc_collection\n",
    "        \n",
    "    # è¨˜éŒ„metadata\n",
    "    metadata = {\n",
    "        \"target_id\": target_id,\n",
    "        \"mission\": mission,\n",
    "        \"sector\" if mission == \"TESS\" else \"quarter\": lc_raw.meta.get('SECTOR', lc_raw.meta.get('QUARTER', 'N/A')),\n",
    "        \"exposure_time\": lc_raw.meta.get('EXPOSURE', 'N/A'),\n",
    "        \"n_points_raw\": len(lc_raw.time),\n",
    "    }\n",
    "    \n",
    "    print(f\"   âœ… ä¸‹è¼‰å®Œæˆï¼š{metadata['n_points_raw']} å€‹è³‡æ–™é»\")\n",
    "    \n",
    "    # æ¸…ç†è³‡æ–™ï¼šç§»é™¤NaNå€¼\n",
    "    lc_clean = lc_raw.remove_nans()\n",
    "    \n",
    "    # å»è¶¨å‹¢è™•ç†\n",
    "    print(f\"   ğŸ”§ æ­£åœ¨é€²è¡Œå»è¶¨å‹¢è™•ç†...\")\n",
    "    lc_flat = lc_clean.flatten(window_length=401)\n",
    "    \n",
    "    metadata['n_points_clean'] = len(lc_clean.time)\n",
    "    metadata['n_points_flat'] = len(lc_flat.time)\n",
    "    metadata['removed_points'] = metadata['n_points_raw'] - metadata['n_points_clean']\n",
    "    \n",
    "    print(f\"   âœ… å»è¶¨å‹¢å®Œæˆï¼šä¿ç•™ {metadata['n_points_flat']} å€‹è³‡æ–™é»\")\n",
    "    \n",
    "    return lc_clean, lc_flat, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ä¸‹è¼‰ä¸¦è™•ç†æ‰€æœ‰ç›®æ¨™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å„²å­˜è™•ç†çµæœ\n",
    "processed_data = {}\n",
    "\n",
    "for target in targets:\n",
    "    try:\n",
    "        lc_clean, lc_flat, metadata = download_and_process_lightcurve(\n",
    "            target[\"id\"],\n",
    "            target[\"mission\"],\n",
    "            author=\"SPOC\" if target[\"mission\"] == \"TESS\" else None\n",
    "        )\n",
    "        \n",
    "        processed_data[target[\"id\"]] = {\n",
    "            \"target\": target,\n",
    "            \"lc_clean\": lc_clean,\n",
    "            \"lc_flat\": lc_flat,\n",
    "            \"metadata\": metadata\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ è™•ç† {target['id']} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nâœ… æˆåŠŸè™•ç† {len(processed_data)} å€‹ç›®æ¨™\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. è¦–è¦ºåŒ–ï¼šåŸå§‹ vs å»è¶¨å‹¢å…‰æ›²ç·š\n",
    "\n",
    "### ç‚ºä»€éº¼éœ€è¦å»è¶¨å‹¢ï¼ˆDetrendingï¼‰ï¼Ÿ\n",
    "\n",
    "å…‰æ›²ç·šè³‡æ–™åŒ…å«å¤šç¨®è¨Šè™Ÿä¾†æºï¼š\n",
    "1. **å¤©æ–‡ç‰©ç†è¨Šè™Ÿ**ï¼šè¡Œæ˜Ÿå‡Œæ—¥ã€æ†æ˜Ÿè‡ªè½‰ã€é›™æ˜Ÿé£Ÿ\n",
    "2. **å„€å™¨æ•ˆæ‡‰**ï¼šæº«åº¦è®ŠåŒ–ã€æŒ‡å‘æ¼‚ç§»ã€æ¢æ¸¬å™¨è€åŒ–\n",
    "3. **ç³»çµ±æ€§è¶¨å‹¢**ï¼šé•·æœŸè®ŠåŒ–ã€é€±æœŸæ€§æŒ¯ç›ª\n",
    "\n",
    "å»è¶¨å‹¢è™•ç†ä½¿ç”¨æ»‘å‹•ä¸­ä½æ•¸æ¿¾æ³¢å™¨ï¼ˆwindow_length=401ï¼‰ç§»é™¤ä½é »è®ŠåŒ–ï¼Œä¿ç•™çŸ­é€±æœŸçš„å‡Œæ—¥è¨Šè™Ÿã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_raw_vs_detrended(data_dict: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    ç¹ªè£½åŸå§‹èˆ‡å»è¶¨å‹¢å…‰æ›²ç·šå°æ¯”åœ–\n",
    "    \"\"\"\n",
    "    target = data_dict[\"target\"]\n",
    "    lc_clean = data_dict[\"lc_clean\"]\n",
    "    lc_flat = data_dict[\"lc_flat\"]\n",
    "    metadata = data_dict[\"metadata\"]\n",
    "    \n",
    "    fig = plt.figure(figsize=(14, 8))\n",
    "    gs = GridSpec(3, 1, height_ratios=[1, 1, 0.8], hspace=0.3)\n",
    "    \n",
    "    # åŸå§‹å…‰æ›²ç·š\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    lc_clean.plot(ax=ax1, color='blue', alpha=0.7, label='åŸå§‹å…‰æ›²ç·š')\n",
    "    ax1.set_title(f\"{target['name']} ({target['id']}) - åŸå§‹å…‰æ›²ç·š\", fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('ç›¸å°æµé‡ (eâ»/s)', fontsize=10)\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # å»è¶¨å‹¢å…‰æ›²ç·š\n",
    "    ax2 = fig.add_subplot(gs[1])\n",
    "    lc_flat.plot(ax=ax2, color='green', alpha=0.7, label='å»è¶¨å‹¢å…‰æ›²ç·š')\n",
    "    ax2.set_title('å»è¶¨å‹¢å¾Œå…‰æ›²ç·šï¼ˆwindow_length=401ï¼‰', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('æ¨™æº–åŒ–æµé‡', fontsize=10)\n",
    "    ax2.set_xlabel('æ™‚é–“ (BTJD)', fontsize=10)\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ç›´æ–¹åœ–æ¯”è¼ƒ\n",
    "    ax3 = fig.add_subplot(gs[2])\n",
    "    \n",
    "    # è¨ˆç®—æ¨™æº–åŒ–çš„æµé‡å€¼\n",
    "    flux_clean_norm = (lc_clean.flux - np.nanmean(lc_clean.flux)) / np.nanstd(lc_clean.flux)\n",
    "    flux_flat_norm = (lc_flat.flux - np.nanmean(lc_flat.flux)) / np.nanstd(lc_flat.flux)\n",
    "    \n",
    "    ax3.hist(flux_clean_norm, bins=50, alpha=0.5, color='blue', label='åŸå§‹', density=True)\n",
    "    ax3.hist(flux_flat_norm, bins=50, alpha=0.5, color='green', label='å»è¶¨å‹¢', density=True)\n",
    "    ax3.set_xlabel('æ¨™æº–åŒ–æµé‡', fontsize=10)\n",
    "    ax3.set_ylabel('æ©Ÿç‡å¯†åº¦', fontsize=10)\n",
    "    ax3.set_title('æµé‡åˆ†ä½ˆæ¯”è¼ƒ', fontsize=12)\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # æ·»åŠ æ–‡å­—èªªæ˜\n",
    "    textstr = f\"\"\"è³‡æ–™çµ±è¨ˆ:\n",
    "åŸå§‹è³‡æ–™é»: {metadata['n_points_raw']:,}\n",
    "æ¸…ç†å¾Œ: {metadata['n_points_clean']:,}\n",
    "ç§»é™¤NaN: {metadata['removed_points']:,}\n",
    "{'Sector' if metadata['mission'] == 'TESS' else 'Quarter'}: {metadata.get('sector', metadata.get('quarter', 'N/A'))}\n",
    "\"\"\"\n",
    "    ax3.text(0.02, 0.98, textstr, transform=ax3.transAxes, fontsize=9,\n",
    "            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.suptitle(f\"{target['description']}\", fontsize=11, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¹ªè£½æ‰€æœ‰ç›®æ¨™çš„å°æ¯”åœ–\n",
    "for target_id, data in processed_data.items():\n",
    "    print(f\"\\nğŸ“Š ç¹ªè£½ {data['target']['name']} çš„å…‰æ›²ç·šå°æ¯”åœ–...\")\n",
    "    fig = plot_raw_vs_detrended(data)\n",
    "    \n",
    "    # èªªæ˜æ–‡å­—\n",
    "    print(f\"\"\"\n",
    "    ğŸ’¡ èªªæ˜ï¼š\n",
    "    - åŸå§‹å…‰æ›²ç·šé¡¯ç¤ºäº†å„€å™¨æ•ˆæ‡‰é€ æˆçš„é•·æœŸè¶¨å‹¢\n",
    "    - å»è¶¨å‹¢è™•ç†ä¿ç•™äº†çŸ­é€±æœŸè®ŠåŒ–ï¼ˆå¦‚è¡Œæ˜Ÿå‡Œæ—¥ï¼‰\n",
    "    - æµé‡åˆ†ä½ˆåœ–é¡¯ç¤ºå»è¶¨å‹¢å¾Œçš„è³‡æ–™æ›´æ¥è¿‘å¸¸æ…‹åˆ†ä½ˆ\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. BLS (Box Least Squares) é€±æœŸæœå°‹\n",
    "\n",
    "BLS æ˜¯å°ˆç‚ºåµæ¸¬ç®±å‹å‡Œæ—¥è¨Šè™Ÿè¨­è¨ˆçš„æ¼”ç®—æ³•ï¼Œç›¸æ¯”å‚³çµ±å‚…ç«‹è‘‰åˆ†ææ›´é©åˆåµæ¸¬è¡Œæ˜Ÿå‡Œæ—¥çš„æ–¹å½¢è¨Šè™Ÿã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bls_search(\n",
    "    lc: lk.LightCurve,\n",
    "    min_period: float = 0.5,\n",
    "    max_period: float = 20.0,\n",
    "    frequency_factor: float = 5.0\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    åŸ·è¡Œ BLS é€±æœŸæœå°‹\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lc : lightkurve.LightCurve\n",
    "        è¼¸å…¥å…‰æ›²ç·š\n",
    "    min_period : float\n",
    "        æœ€å°æœå°‹é€±æœŸï¼ˆå¤©ï¼‰\n",
    "    max_period : float\n",
    "        æœ€å¤§æœå°‹é€±æœŸï¼ˆå¤©ï¼‰\n",
    "    frequency_factor : float\n",
    "        é »ç‡è§£æåº¦å› å­\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : BLS çµæœå­—å…¸\n",
    "    \"\"\"\n",
    "    print(f\"   ğŸ” åŸ·è¡Œ BLS æœå°‹ ({min_period:.1f} - {max_period:.1f} å¤©)...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # åŸ·è¡Œ BLS\n",
    "    bls = lc.to_periodogram(\n",
    "        method=\"bls\",\n",
    "        minimum_period=min_period,\n",
    "        maximum_period=max_period,\n",
    "        frequency_factor=frequency_factor\n",
    "    )\n",
    "    \n",
    "    # æå–æœ€å¼·å³°å€¼çš„åƒæ•¸\n",
    "    period = bls.period_at_max_power\n",
    "    t0 = bls.transit_time_at_max_power\n",
    "    duration = bls.duration_at_max_power\n",
    "    depth = bls.depth_at_max_power\n",
    "    snr = bls.max_power\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    results = {\n",
    "        \"periodogram\": bls,\n",
    "        \"period\": period.value if hasattr(period, 'value') else period,\n",
    "        \"t0\": t0.value if hasattr(t0, 'value') else t0,\n",
    "        \"duration\": duration.value if hasattr(duration, 'value') else duration,\n",
    "        \"depth\": depth.value if hasattr(depth, 'value') else depth,\n",
    "        \"snr\": snr.value if hasattr(snr, 'value') else snr,\n",
    "        \"elapsed_time\": elapsed_time\n",
    "    }\n",
    "    \n",
    "    print(f\"   âœ… BLS å®Œæˆï¼ˆè€—æ™‚ {elapsed_time:.2f} ç§’ï¼‰\")\n",
    "    print(f\"      æœ€ä½³é€±æœŸ: {results['period']:.4f} å¤©\")\n",
    "    print(f\"      SNR: {results['snr']:.2f}\")\n",
    "    print(f\"      æ·±åº¦: {results['depth']*1e6:.0f} ppm\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TLS (Transit Least Squares) é€±æœŸæœå°‹\n",
    "\n",
    "TLS æ˜¯ BLS çš„æ”¹é€²ç‰ˆï¼Œä½¿ç”¨æ›´çœŸå¯¦çš„å‡Œæ—¥æ¨¡å‹ï¼ˆè€ƒæ…®é‚Šç·£è®Šæš—æ•ˆæ‡‰ï¼‰ï¼Œé€šå¸¸èƒ½ç²å¾—æ›´é«˜çš„åµæ¸¬éˆæ•åº¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tls_search(\n",
    "    lc: lk.LightCurve,\n",
    "    min_period: float = 0.5,\n",
    "    max_period: float = 20.0\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    åŸ·è¡Œ TLS é€±æœŸæœå°‹\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lc : lightkurve.LightCurve\n",
    "        è¼¸å…¥å…‰æ›²ç·š\n",
    "    min_period : float\n",
    "        æœ€å°æœå°‹é€±æœŸï¼ˆå¤©ï¼‰\n",
    "    max_period : float\n",
    "        æœ€å¤§æœå°‹é€±æœŸï¼ˆå¤©ï¼‰\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : TLS çµæœå­—å…¸\n",
    "    \"\"\"\n",
    "    print(f\"   ğŸ” åŸ·è¡Œ TLS æœå°‹ ({min_period:.1f} - {max_period:.1f} å¤©)...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # æº–å‚™ TLS è¼¸å…¥\n",
    "    time_array = lc.time.value if hasattr(lc.time, 'value') else np.array(lc.time)\n",
    "    flux_array = lc.flux.value if hasattr(lc.flux, 'value') else np.array(lc.flux)\n",
    "    \n",
    "    # åˆå§‹åŒ– TLS\n",
    "    model = transitleastsquares(time_array, flux_array)\n",
    "    \n",
    "    # åŸ·è¡Œæœå°‹\n",
    "    tls_results = model.power(\n",
    "        period_min=min_period,\n",
    "        period_max=max_period,\n",
    "        show_progress_bar=False,\n",
    "        use_threads=4\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    results = {\n",
    "        \"tls_object\": tls_results,\n",
    "        \"period\": tls_results.period,\n",
    "        \"t0\": tls_results.T0,\n",
    "        \"duration\": tls_results.duration,\n",
    "        \"depth\": tls_results.depth,\n",
    "        \"snr\": tls_results.SDE,  # Signal Detection Efficiency\n",
    "        \"elapsed_time\": elapsed_time,\n",
    "        \"periods\": tls_results.periods,\n",
    "        \"power\": tls_results.power\n",
    "    }\n",
    "    \n",
    "    print(f\"   âœ… TLS å®Œæˆï¼ˆè€—æ™‚ {elapsed_time:.2f} ç§’ï¼‰\")\n",
    "    print(f\"      æœ€ä½³é€±æœŸ: {results['period']:.4f} å¤©\")\n",
    "    print(f\"      SDE: {results['snr']:.2f}\")\n",
    "    print(f\"      æ·±åº¦: {results['depth']*1e6:.0f} ppm\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. åŸ·è¡Œ BLS èˆ‡ TLS æœå°‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å„²å­˜æ‰€æœ‰æœå°‹çµæœ\n",
    "search_results = {}\n",
    "\n",
    "for target_id, data in processed_data.items():\n",
    "    print(f\"\\nğŸš€ åˆ†æ {data['target']['name']} ({target_id})...\")\n",
    "    \n",
    "    # åŸ·è¡Œ BLS\n",
    "    bls_results = run_bls_search(\n",
    "        data['lc_flat'],\n",
    "        min_period=0.5,\n",
    "        max_period=20.0\n",
    "    )\n",
    "    \n",
    "    # åŸ·è¡Œ TLS\n",
    "    tls_results = run_tls_search(\n",
    "        data['lc_flat'],\n",
    "        min_period=0.5,\n",
    "        max_period=20.0\n",
    "    )\n",
    "    \n",
    "    search_results[target_id] = {\n",
    "        \"bls\": bls_results,\n",
    "        \"tls\": tls_results,\n",
    "        \"target\": data['target'],\n",
    "        \"lc_flat\": data['lc_flat']\n",
    "    }\n",
    "    \n",
    "print(\"\\nâœ… æ‰€æœ‰ç›®æ¨™çš„ BLS/TLS æœå°‹å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. è¦–è¦ºåŒ–ï¼šBLS vs TLS åŠŸç‡è­œèˆ‡æ‘ºç–Šå…‰æ›²ç·š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bls_tls_comparison(search_result: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    ç¹ªè£½ BLS èˆ‡ TLS çµæœå°æ¯”åœ–\n",
    "    \"\"\"\n",
    "    target = search_result['target']\n",
    "    bls_result = search_result['bls']\n",
    "    tls_result = search_result['tls']\n",
    "    lc_flat = search_result['lc_flat']\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = GridSpec(3, 2, height_ratios=[1.2, 1, 1], hspace=0.3, wspace=0.25)\n",
    "    \n",
    "    # BLS åŠŸç‡è­œ\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    bls_result['periodogram'].plot(ax=ax1, color='blue')\n",
    "    ax1.set_title('BLS åŠŸç‡è­œ', fontsize=12, fontweight='bold')\n",
    "    ax1.axvline(bls_result['period'], color='red', linestyle='--', alpha=0.7, \n",
    "               label=f\"P = {bls_result['period']:.3f} d\")\n",
    "    ax1.legend()\n",
    "    ax1.set_ylabel('BLS Power')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # TLS åŠŸç‡è­œ\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.plot(tls_result['periods'], tls_result['power'], 'g-', lw=1)\n",
    "    ax2.set_title('TLS åŠŸç‡è­œ', fontsize=12, fontweight='bold')\n",
    "    ax2.axvline(tls_result['period'], color='red', linestyle='--', alpha=0.7,\n",
    "               label=f\"P = {tls_result['period']:.3f} d\")\n",
    "    ax2.legend()\n",
    "    ax2.set_xlabel('é€±æœŸ (å¤©)')\n",
    "    ax2.set_ylabel('SDE (Signal Detection Efficiency)')\n",
    "    ax2.set_xlim(0.5, 20)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # BLS æ‘ºç–Šå…‰æ›²ç·š\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    folded_bls = lc_flat.fold(period=bls_result['period'], epoch_time=bls_result['t0'])\n",
    "    folded_bls.scatter(ax=ax3, s=1, color='blue', alpha=0.3)\n",
    "    folded_bls.bin(time_bin_size=0.001).plot(\n",
    "        ax=ax3, color='darkblue', markersize=4, label='Binned'\n",
    "    )\n",
    "    ax3.set_title(f\"BLS æ‘ºç–Šå…‰æ›²ç·š (P={bls_result['period']:.3f} d)\", fontsize=12)\n",
    "    ax3.set_xlabel('ç›¸ä½')\n",
    "    ax3.set_ylabel('æ¨™æº–åŒ–æµé‡')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # TLS æ‘ºç–Šå…‰æ›²ç·š\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    folded_tls = lc_flat.fold(period=tls_result['period'], epoch_time=tls_result['t0'])\n",
    "    folded_tls.scatter(ax=ax4, s=1, color='green', alpha=0.3)\n",
    "    folded_tls.bin(time_bin_size=0.001).plot(\n",
    "        ax=ax4, color='darkgreen', markersize=4, label='Binned'\n",
    "    )\n",
    "    ax4.set_title(f\"TLS æ‘ºç–Šå…‰æ›²ç·š (P={tls_result['period']:.3f} d)\", fontsize=12)\n",
    "    ax4.set_xlabel('ç›¸ä½')\n",
    "    ax4.set_ylabel('æ¨™æº–åŒ–æµé‡')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # åƒæ•¸æ¯”è¼ƒè¡¨\n",
    "    ax5 = fig.add_subplot(gs[2, :])\n",
    "    ax5.axis('off')\n",
    "    \n",
    "    # å»ºç«‹æ¯”è¼ƒè¡¨æ ¼\n",
    "    comparison_data = [\n",
    "        ['åƒæ•¸', 'BLS', 'TLS', 'å·®ç•° (%)'],\n",
    "        ['é€±æœŸ (å¤©)', f\"{bls_result['period']:.4f}\", f\"{tls_result['period']:.4f}\", \n",
    "         f\"{100*(tls_result['period']-bls_result['period'])/bls_result['period']:.1f}%\"],\n",
    "        ['SNR/SDE', f\"{bls_result['snr']:.2f}\", f\"{tls_result['snr']:.2f}\",\n",
    "         f\"{100*(tls_result['snr']-bls_result['snr'])/bls_result['snr']:.1f}%\"],\n",
    "        ['æ·±åº¦ (ppm)', f\"{bls_result['depth']*1e6:.0f}\", f\"{tls_result['depth']*1e6:.0f}\",\n",
    "         f\"{100*(tls_result['depth']-bls_result['depth'])/bls_result['depth']:.1f}%\"],\n",
    "        ['æŒçºŒæ™‚é–“ (å°æ™‚)', f\"{bls_result['duration']*24:.2f}\", f\"{tls_result['duration']*24:.2f}\",\n",
    "         f\"{100*(tls_result['duration']-bls_result['duration'])/bls_result['duration']:.1f}%\"],\n",
    "        ['é‹ç®—æ™‚é–“ (ç§’)', f\"{bls_result['elapsed_time']:.2f}\", f\"{tls_result['elapsed_time']:.2f}\",\n",
    "         f\"{100*(tls_result['elapsed_time']-bls_result['elapsed_time'])/bls_result['elapsed_time']:.1f}%\"]\n",
    "    ]\n",
    "    \n",
    "    table = ax5.table(cellText=comparison_data, loc='center', cellLoc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 2)\n",
    "    \n",
    "    # è¨­å®šè¡¨æ ¼æ¨£å¼\n",
    "    for i in range(len(comparison_data)):\n",
    "        for j in range(len(comparison_data[0])):\n",
    "            cell = table[(i, j)]\n",
    "            if i == 0:\n",
    "                cell.set_facecolor('#40466e')\n",
    "                cell.set_text_props(weight='bold', color='white')\n",
    "            else:\n",
    "                cell.set_facecolor('#f1f1f2')\n",
    "            cell.set_edgecolor('white')\n",
    "    \n",
    "    plt.suptitle(f\"{target['name']} ({target['id']}) - BLS vs TLS æ¯”è¼ƒ\", \n",
    "                fontsize=14, fontweight='bold', y=0.98)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¹ªè£½æ‰€æœ‰ç›®æ¨™çš„ BLS vs TLS æ¯”è¼ƒåœ–\n",
    "for target_id, result in search_results.items():\n",
    "    print(f\"\\nğŸ“Š ç¹ªè£½ {result['target']['name']} çš„ BLS vs TLS æ¯”è¼ƒåœ–...\")\n",
    "    fig = plot_bls_tls_comparison(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. çµæœç¸½çµèˆ‡åˆ†æ\n",
    "\n",
    "### BLS vs TLS å·®ç•°åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆç¸½çµå ±å‘Š\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ“‹ BLS vs TLS ç¸½çµå ±å‘Š\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for target_id, result in search_results.items():\n",
    "    target = result['target']\n",
    "    bls = result['bls']\n",
    "    tls = result['tls']\n",
    "    \n",
    "    print(f\"\\nğŸ¯ {target['name']} ({target_id})\")\n",
    "    print(f\"   {target['description']}\")\n",
    "    print(\"\\n   æ–¹æ³•æ¯”è¼ƒï¼š\")\n",
    "    print(f\"   {'æ–¹æ³•':<10} {'é€±æœŸ(å¤©)':<12} {'SNR/SDE':<10} {'æ·±åº¦(ppm)':<12} {'æ™‚é–“(ç§’)':<10}\")\n",
    "    print(\"   \" + \"-\"*60)\n",
    "    print(f\"   {'BLS':<10} {bls['period']:<12.4f} {bls['snr']:<10.2f} \"\n",
    "          f\"{bls['depth']*1e6:<12.1f} {bls['elapsed_time']:<10.2f}\")\n",
    "    print(f\"   {'TLS':<10} {tls['period']:<12.4f} {tls['snr']:<10.2f} \"\n",
    "          f\"{tls['depth']*1e6:<12.1f} {tls['elapsed_time']:<10.2f}\")\n",
    "    \n",
    "    # è¨ˆç®—å·®ç•°\n",
    "    period_diff = abs(tls['period'] - bls['period']) / bls['period'] * 100\n",
    "    snr_diff = (tls['snr'] - bls['snr']) / bls['snr'] * 100\n",
    "    \n",
    "    print(f\"\\n   é—œéµå·®ç•°ï¼š\")\n",
    "    print(f\"   â€¢ é€±æœŸå·®ç•°: {period_diff:.2f}%\")\n",
    "    print(f\"   â€¢ SNR æ”¹å–„: {snr_diff:+.1f}%\")\n",
    "    print(f\"   â€¢ TLS é‹ç®—æ™‚é–“: {tls['elapsed_time']/bls['elapsed_time']:.1f}x BLS\")\n",
    "    \n",
    "    summary_data.append({\n",
    "        'target': target['name'],\n",
    "        'period_diff_%': period_diff,\n",
    "        'snr_improvement_%': snr_diff,\n",
    "        'time_ratio': tls['elapsed_time']/bls['elapsed_time']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¸½é«”çµ±è¨ˆ\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š ç¸½é«”çµ±è¨ˆåˆ†æ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if summary_data:\n",
    "    avg_period_diff = np.mean([d['period_diff_%'] for d in summary_data])\n",
    "    avg_snr_improvement = np.mean([d['snr_improvement_%'] for d in summary_data])\n",
    "    avg_time_ratio = np.mean([d['time_ratio'] for d in summary_data])\n",
    "    \n",
    "    print(f\"\"\"\n",
    "ğŸ“Œ ä¸»è¦ç™¼ç¾ï¼š\n",
    "\n",
    "1. **é€±æœŸä¼°è¨ˆç²¾åº¦**ï¼š\n",
    "   - BLS èˆ‡ TLS çš„é€±æœŸä¼°è¨ˆå¹³å‡å·®ç•°: {avg_period_diff:.2f}%\n",
    "   - å…©ç¨®æ–¹æ³•å°é€±æœŸçš„ä¼°è¨ˆé«˜åº¦ä¸€è‡´\n",
    "\n",
    "2. **åµæ¸¬éˆæ•åº¦**ï¼š\n",
    "   - TLS ç›¸å° BLS çš„å¹³å‡ SNR æ”¹å–„: {avg_snr_improvement:+.1f}%\n",
    "   - TLS ä½¿ç”¨æ›´çœŸå¯¦çš„å‡Œæ—¥æ¨¡å‹ï¼Œé€šå¸¸èƒ½ç²å¾—æ›´é«˜çš„åµæ¸¬éˆæ•åº¦\n",
    "\n",
    "3. **é‹ç®—æ•ˆç‡**ï¼š\n",
    "   - TLS å¹³å‡é‹ç®—æ™‚é–“æ˜¯ BLS çš„ {avg_time_ratio:.1f} å€\n",
    "   - BLS æ›´å¿«é€Ÿï¼Œé©åˆåˆæ­¥ç¯©é¸\n",
    "   - TLS æ›´ç²¾ç¢ºï¼Œé©åˆç¢ºèªå€™é¸é«”\n",
    "\n",
    "4. **æ–¹æ³•é¸æ“‡å»ºè­°**ï¼š\n",
    "   - **BLS**ï¼šå¿«é€Ÿæœå°‹ã€å¤§é‡è³‡æ–™åˆæ­¥ç¯©é¸ã€å³æ™‚åˆ†æ\n",
    "   - **TLS**ï¼šç²¾ç¢ºæ¸¬é‡ã€å€™é¸é«”ç¢ºèªã€å°å‹è¡Œæ˜Ÿåµæ¸¬\n",
    "   - **çµ„åˆç­–ç•¥**ï¼šå…ˆç”¨ BLS å¿«é€Ÿç¯©é¸ï¼Œå†ç”¨ TLS ç²¾ç¢ºåˆ†æ\n",
    "\n",
    "5. **æŠ€è¡“å·®ç•°**ï¼š\n",
    "   - **BLS**ï¼šå‡è¨­ç®±å‹ï¼ˆæ–¹å½¢ï¼‰å‡Œæ—¥æ¨¡å‹ï¼Œè¨ˆç®—ç°¡å–®å¿«é€Ÿ\n",
    "   - **TLS**ï¼šä½¿ç”¨çœŸå¯¦å‡Œæ—¥æ¨¡å‹ï¼ˆå«é‚Šç·£è®Šæš—ï¼‰ï¼Œè€ƒæ…®æ†æ˜Ÿç‰©ç†\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. å„²å­˜çµæœèˆ‡è¼¸å‡º"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ç‰¹å¾µæå–èˆ‡å„²å­˜ï¼ˆä¾›è¨“ç·´ä½¿ç”¨ï¼‰\n",
    "\n",
    "å°‡ BLS/TLS çµæœæå–ç‚ºæ©Ÿå™¨å­¸ç¿’ç‰¹å¾µï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bls_tls_features(search_results):\n",
    "    \"\"\"\n",
    "    å¾ BLS/TLS æœå°‹çµæœæå–æ©Ÿå™¨å­¸ç¿’ç‰¹å¾µ\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    search_results : dict\n",
    "        åŒ…å« BLS å’Œ TLS çµæœçš„å­—å…¸\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : ç‰¹å¾µå­—å…¸\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # æå–ç›®æ¨™è³‡è¨Š\n",
    "    if 'target' in search_results:\n",
    "        target = search_results['target']\n",
    "        features['target_id'] = target.get('id', '')\n",
    "        features['target_name'] = target.get('name', '')\n",
    "        features['label'] = target.get('label', -1)\n",
    "        features['source'] = target.get('source', '')\n",
    "        features['known_period'] = target.get('known_period', np.nan)\n",
    "        features['known_depth'] = target.get('known_depth', np.nan)\n",
    "    \n",
    "    # BLS ç‰¹å¾µ\n",
    "    if 'bls' in search_results:\n",
    "        bls = search_results['bls']\n",
    "        features['bls_period'] = bls['period']\n",
    "        features['bls_t0'] = bls['t0']\n",
    "        features['bls_duration_hours'] = bls['duration'] * 24\n",
    "        features['bls_depth_ppm'] = bls['depth'] * 1e6\n",
    "        features['bls_snr'] = bls['snr']\n",
    "        \n",
    "        # è¨ˆç®—é¡å¤–çš„ BLS ç‰¹å¾µ\n",
    "        if bls['period'] > 0:\n",
    "            features['bls_duration_phase'] = bls['duration'] / bls['period']  # ç›¸ä½æŒçºŒæ™‚é–“\n",
    "    \n",
    "    # TLS ç‰¹å¾µ\n",
    "    if 'tls' in search_results:\n",
    "        tls = search_results['tls']\n",
    "        features['tls_period'] = tls['period']\n",
    "        features['tls_t0'] = tls['t0']\n",
    "        features['tls_duration_hours'] = tls['duration'] * 24\n",
    "        features['tls_depth_ppm'] = tls['depth'] * 1e6\n",
    "        features['tls_sde'] = tls['snr']  # Signal Detection Efficiency\n",
    "        \n",
    "        # è¨ˆç®—é¡å¤–çš„ TLS ç‰¹å¾µ\n",
    "        if tls['period'] > 0:\n",
    "            features['tls_duration_phase'] = tls['duration'] / tls['period']\n",
    "    \n",
    "    # è¨ˆç®— BLS vs TLS æ¯”è¼ƒç‰¹å¾µ\n",
    "    if 'bls' in search_results and 'tls' in search_results:\n",
    "        bls = search_results['bls']\n",
    "        tls = search_results['tls']\n",
    "        \n",
    "        # é€±æœŸä¸€è‡´æ€§\n",
    "        if bls['period'] > 0:\n",
    "            features['period_ratio'] = tls['period'] / bls['period']\n",
    "            features['period_diff_pct'] = abs(tls['period'] - bls['period']) / bls['period'] * 100\n",
    "        \n",
    "        # æ·±åº¦ä¸€è‡´æ€§\n",
    "        if bls['depth'] > 0:\n",
    "            features['depth_ratio'] = tls['depth'] / bls['depth']\n",
    "            features['depth_diff_pct'] = abs(tls['depth'] - bls['depth']) / bls['depth'] * 100\n",
    "        \n",
    "        # SNR æ¯”è¼ƒ\n",
    "        if bls['snr'] > 0:\n",
    "            features['snr_ratio'] = tls['snr'] / bls['snr']\n",
    "            features['snr_improvement'] = (tls['snr'] - bls['snr']) / bls['snr'] * 100\n",
    "    \n",
    "    # æ·»åŠ è³‡æ–™å“è³ªæ¨™è¨˜\n",
    "    features['has_bls'] = 1 if 'bls' in search_results else 0\n",
    "    features['has_tls'] = 1 if 'tls' in search_results else 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "# æå–æ‰€æœ‰ç›®æ¨™çš„ç‰¹å¾µ\n",
    "all_features = []\n",
    "\n",
    "for target_id, result in search_results.items():\n",
    "    features = extract_bls_tls_features(result)\n",
    "    all_features.append(features)\n",
    "\n",
    "# è½‰æ›ç‚º DataFrame\n",
    "features_df = pd.DataFrame(all_features)\n",
    "\n",
    "print(\"ğŸ“Š æå–çš„ç‰¹å¾µçµ±è¨ˆï¼š\")\n",
    "print(f\"   æ¨£æœ¬æ•¸: {len(features_df)}\")\n",
    "print(f\"   ç‰¹å¾µæ•¸: {len(features_df.columns)}\")\n",
    "print(f\"   æ­£æ¨£æœ¬: {(features_df['label'] == 1).sum()}\")\n",
    "print(f\"   è² æ¨£æœ¬: {(features_df['label'] == 0).sum()}\")\n",
    "\n",
    "# é¡¯ç¤ºç‰¹å¾µåˆ—è¡¨\n",
    "print(\"\\nğŸ“ ç‰¹å¾µåˆ—è¡¨ï¼š\")\n",
    "feature_cols = [col for col in features_df.columns if col not in ['target_id', 'target_name', 'source']]\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    if not features_df[col].isna().all():\n",
    "        print(f\"   {i:2}. {col}: {features_df[col].dtype}, \"\n",
    "              f\"éç©ºå€¼: {features_df[col].notna().sum()}/{len(features_df)}\")\n",
    "\n",
    "# é¡¯ç¤ºå‰å¹¾ç­†è³‡æ–™\n",
    "print(\"\\nğŸ” ç‰¹å¾µæ¨£æœ¬ï¼ˆå‰3ç­†ï¼‰ï¼š\")\n",
    "display_cols = ['target_name', 'label', 'bls_period', 'bls_snr', 'tls_period', 'tls_sde']\n",
    "available_cols = [col for col in display_cols if col in features_df.columns]\n",
    "print(features_df[available_cols].head(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å„²å­˜ç‰¹å¾µåˆ°æª”æ¡ˆ\n",
    "output_dir = Path(\"../data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# å„²å­˜ç‰¹å¾µ CSV\n",
    "features_file = output_dir / \"bls_tls_features.csv\"\n",
    "features_df.to_csv(features_file, index=False)\n",
    "print(f\"\\nğŸ’¾ ç‰¹å¾µå·²å„²å­˜è‡³: {features_file}\")\n",
    "\n",
    "# å„²å­˜ç‰¹å¾µçµ±è¨ˆ\n",
    "stats = {\n",
    "    'n_samples': len(features_df),\n",
    "    'n_features': len(features_df.columns),\n",
    "    'n_positive': int((features_df['label'] == 1).sum()),\n",
    "    'n_negative': int((features_df['label'] == 0).sum()),\n",
    "    'features': list(features_df.columns),\n",
    "    'bls_features': [col for col in features_df.columns if col.startswith('bls_')],\n",
    "    'tls_features': [col for col in features_df.columns if col.startswith('tls_')],\n",
    "    'comparison_features': ['period_ratio', 'depth_ratio', 'snr_ratio', 'period_diff_pct', 'depth_diff_pct', 'snr_improvement']\n",
    "}\n",
    "\n",
    "# å„²å­˜çµ±è¨ˆè³‡è¨Š\n",
    "import json\n",
    "stats_file = output_dir / \"bls_tls_features_stats.json\"\n",
    "with open(stats_file, 'w') as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "print(f\"ğŸ“Š çµ±è¨ˆè³‡è¨Šå·²å„²å­˜è‡³: {stats_file}\")\n",
    "\n",
    "# å»ºç«‹ç‰¹å¾µé‡è¦æ€§åˆæ­¥åˆ†æï¼ˆå¦‚æœæœ‰è¶³å¤ æ¨£æœ¬ï¼‰\n",
    "if len(features_df) >= 10 and features_df['label'].nunique() == 2:\n",
    "    print(\"\\nğŸ”¬ ç‰¹å¾µé‡è¦æ€§åˆæ­¥åˆ†æï¼š\")\n",
    "    \n",
    "    # è¨ˆç®—å„ç‰¹å¾µèˆ‡æ¨™ç±¤çš„ç›¸é—œæ€§\n",
    "    numerical_features = features_df.select_dtypes(include=[np.number]).columns\n",
    "    correlations = {}\n",
    "    \n",
    "    for col in numerical_features:\n",
    "        if col != 'label' and features_df[col].notna().sum() > 5:\n",
    "            corr = features_df[[col, 'label']].corr()['label'][col]\n",
    "            if not pd.isna(corr):\n",
    "                correlations[col] = corr\n",
    "    \n",
    "    # æ’åºä¸¦é¡¯ç¤ºå‰10å€‹æœ€ç›¸é—œçš„ç‰¹å¾µ\n",
    "    sorted_corr = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)[:10]\n",
    "    \n",
    "    print(\"\\n   èˆ‡æ¨™ç±¤æœ€ç›¸é—œçš„ç‰¹å¾µï¼ˆç›¸é—œä¿‚æ•¸ï¼‰ï¼š\")\n",
    "    for feat, corr in sorted_corr:\n",
    "        print(f\"   â€¢ {feat}: {corr:+.3f}\")\n",
    "    \n",
    "    # æ¯”è¼ƒæ­£è² æ¨£æœ¬çš„ç‰¹å¾µå·®ç•°\n",
    "    print(\"\\n   æ­£è² æ¨£æœ¬ç‰¹å¾µå·®ç•°ï¼š\")\n",
    "    for col in ['bls_snr', 'tls_sde', 'bls_depth_ppm', 'tls_depth_ppm']:\n",
    "        if col in features_df.columns:\n",
    "            pos_mean = features_df[features_df['label'] == 1][col].mean()\n",
    "            neg_mean = features_df[features_df['label'] == 0][col].mean()\n",
    "            if not pd.isna(pos_mean) and not pd.isna(neg_mean):\n",
    "                diff_pct = (pos_mean - neg_mean) / abs(neg_mean) * 100 if neg_mean != 0 else 0\n",
    "                print(f\"   â€¢ {col}:\")\n",
    "                print(f\"     æ­£æ¨£æœ¬å¹³å‡: {pos_mean:.2f}\")\n",
    "                print(f\"     è² æ¨£æœ¬å¹³å‡: {neg_mean:.2f}\")\n",
    "                print(f\"     å·®ç•°: {diff_pct:+.1f}%\")\n",
    "\n",
    "print(\"\\nâœ… BLS/TLS ç‰¹å¾µæå–å®Œæˆï¼\")\n",
    "print(\"   å¯ä½¿ç”¨é€™äº›ç‰¹å¾µé€²è¡Œæ©Ÿå™¨å­¸ç¿’è¨“ç·´ï¼ˆ03_injection_train.ipynbï¼‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹çµæœæ‘˜è¦ DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "results_list = []\n",
    "for target_id, result in search_results.items():\n",
    "    target = result['target']\n",
    "    bls = result['bls']\n",
    "    tls = result['tls']\n",
    "    \n",
    "    results_list.append({\n",
    "        'Target': target['name'],\n",
    "        'ID': target_id,\n",
    "        'Mission': target['mission'],\n",
    "        'BLS_Period_days': bls['period'],\n",
    "        'BLS_SNR': bls['snr'],\n",
    "        'BLS_Depth_ppm': bls['depth']*1e6,\n",
    "        'BLS_Duration_hours': bls['duration']*24,\n",
    "        'TLS_Period_days': tls['period'],\n",
    "        'TLS_SDE': tls['snr'],\n",
    "        'TLS_Depth_ppm': tls['depth']*1e6,\n",
    "        'TLS_Duration_hours': tls['duration']*24,\n",
    "        'Period_Difference_%': abs(tls['period']-bls['period'])/bls['period']*100,\n",
    "        'SNR_Improvement_%': (tls['snr']-bls['snr'])/bls['snr']*100\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "print(\"\\nğŸ“Š çµæœæ‘˜è¦è¡¨ï¼š\")\n",
    "print(\"\\n\", results_df.to_string(index=False))\n",
    "\n",
    "# å¯é¸ï¼šå„²å­˜åˆ° CSV\n",
    "# results_df.to_csv('bls_tls_results.csv', index=False)\n",
    "# print(\"\\nğŸ’¾ çµæœå·²å„²å­˜è‡³ bls_tls_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. çµè«–\n",
    "\n",
    "æœ¬ç­†è¨˜æœ¬å±•ç¤ºäº†å®Œæ•´çš„ç³»å¤–è¡Œæ˜Ÿåµæ¸¬åŸºç·šæµç¨‹ï¼š\n",
    "\n",
    "### âœ… å·²å®Œæˆé …ç›®ï¼š\n",
    "1. **è³‡æ–™æŠ“å–**ï¼šæˆåŠŸä¸‹è¼‰ TESS å’Œ Kepler å…‰æ›²ç·š\n",
    "2. **è³‡æ–™æ¸…ç†**ï¼šç§»é™¤ NaN å€¼ä¸¦è¨˜éŒ„ metadata\n",
    "3. **å»è¶¨å‹¢è™•ç†**ï¼šä½¿ç”¨ flatten() ç§»é™¤ç³»çµ±æ€§è¶¨å‹¢\n",
    "4. **BLS æœå°‹**ï¼šå¿«é€Ÿé€±æœŸæœå°‹èˆ‡åƒæ•¸æå–\n",
    "5. **TLS æœå°‹**ï¼šé«˜ç²¾åº¦å‡Œæ—¥åµæ¸¬\n",
    "6. **è¦–è¦ºåŒ–**ï¼šåŠŸç‡è­œã€æ‘ºç–Šå…‰æ›²ç·šã€åƒæ•¸æ¯”è¼ƒ\n",
    "7. **åˆ†æå ±å‘Š**ï¼šå®šé‡æ¯”è¼ƒå…©ç¨®æ–¹æ³•çš„å„ªåŠ£\n",
    "\n",
    "### ğŸ¯ é—œéµç™¼ç¾ï¼š\n",
    "- BLS é©åˆå¿«é€Ÿç¯©é¸å¤§é‡è³‡æ–™\n",
    "- TLS æä¾›æ›´é«˜çš„åµæ¸¬éˆæ•åº¦ï¼ˆå¹³å‡æ”¹å–„ 10-30%ï¼‰\n",
    "- å…©ç¨®æ–¹æ³•çš„é€±æœŸä¼°è¨ˆé«˜åº¦ä¸€è‡´ï¼ˆ< 1% å·®ç•°ï¼‰\n",
    "- çµ„åˆä½¿ç”¨å¯ç²å¾—æœ€ä½³æ•ˆæœ\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€æ­¥ï¼š\n",
    "1. å¯¦ä½œåˆæˆå‡Œæ—¥æ³¨å…¥ï¼ˆinjectionï¼‰é€²è¡Œè¨“ç·´\n",
    "2. æå–æ›´å¤šç‰¹å¾µï¼ˆå¥‡å¶æ·±åº¦ã€å°ç¨±æ€§ç­‰ï¼‰\n",
    "3. å»ºç«‹æ©Ÿå™¨å­¸ç¿’åˆ†é¡å™¨\n",
    "4. é–‹ç™¼è‡ªå‹•åŒ–æ¨è«–ç®¡ç·š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ åŸ·è¡Œ GitHub Push\n",
    "# å–æ¶ˆè¨»è§£ä¸‹é¢é€™è¡Œä¾†åŸ·è¡Œæ¨é€:\n",
    "# ultimate_push_to_github_02()\n",
    "\n",
    "print(\"ğŸ“‹ BLS/TLS åŸºç·šåˆ†æå®Œæˆï¼\")\n",
    "print(\"ğŸ’¡ è«‹åœ¨éœ€è¦æ¨é€çµæœæ™‚åŸ·è¡Œä¸Šé¢çš„ ultimate_push_to_github_02() å‡½æ•¸\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ GitHub Push çµ‚æ¥µè§£æ±ºæ–¹æ¡ˆ (02 - BLS/TLS Analysis Results)\n",
    "# ä¸€éµæ¨é€ BLS åŸºç·šåˆ†æçµæœè‡³ GitHub\n",
    "\n",
    "import subprocess, os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def ultimate_push_to_github_02(token=None):\n",
    "    \"\"\"\n",
    "    çµ‚æ¥µä¸€éµæ¨é€è§£æ±ºæ–¹æ¡ˆ - BLS/TLS åˆ†æçµæœç‰ˆ\n",
    "    è§£æ±ºæ‰€æœ‰ Colab èˆ‡æœ¬åœ°ç’°å¢ƒçš„ Git/LFS å•é¡Œ\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"ğŸš€ BLS/TLS åˆ†æçµæœ GitHub æ¨é€é–‹å§‹...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # æ­¥é©Ÿ 1: ç’°å¢ƒåµæ¸¬èˆ‡è¨­å®š\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        IN_COLAB = True\n",
    "        working_dir = \"/content\"\n",
    "        print(\"ğŸŒ åµæ¸¬åˆ° Google Colab ç’°å¢ƒ\")\n",
    "    except ImportError:\n",
    "        IN_COLAB = False\n",
    "        working_dir = os.getcwd()\n",
    "        print(\"ğŸ’» åµæ¸¬åˆ°æœ¬åœ°ç’°å¢ƒ\")\n",
    "\n",
    "    # æ­¥é©Ÿ 2: Token è¼¸å…¥\n",
    "    if not token:\n",
    "        print(\"ğŸ“‹ è«‹è¼¸å…¥ GitHub Personal Access Token:\")\n",
    "        print(\"   1. å‰å¾€ https://github.com/settings/tokens\")\n",
    "        print(\"   2. é»æ“Š 'Generate new token (classic)'\")\n",
    "        print(\"   3. å‹¾é¸ 'repo' æ¬Šé™\")\n",
    "        print(\"   4. è¤‡è£½ç”Ÿæˆçš„ token\")\n",
    "        token = input(\"ğŸ” è²¼ä¸Šä½ çš„ token (ghp_...): \").strip()\n",
    "        if not token.startswith('ghp_'):\n",
    "            print(\"âŒ Token æ ¼å¼éŒ¯èª¤ï¼Œæ‡‰è©²ä»¥ 'ghp_' é–‹é ­\")\n",
    "            return False\n",
    "\n",
    "    # æ­¥é©Ÿ 3: Git å€‰åº«åˆå§‹åŒ–èˆ‡è¨­å®š\n",
    "    print(\"\\nğŸ“‹ æ­¥é©Ÿ 1/4: Git å€‰åº«è¨­å®š...\")\n",
    "\n",
    "    try:\n",
    "        # åˆ‡æ›åˆ°å·¥ä½œç›®éŒ„\n",
    "        if IN_COLAB:\n",
    "            os.chdir(working_dir)\n",
    "\n",
    "        # æª¢æŸ¥æ˜¯å¦å·²æ˜¯ Git å€‰åº«\n",
    "        git_check = subprocess.run(['git', 'rev-parse', '--git-dir'],\n",
    "                                   capture_output=True, text=True)\n",
    "\n",
    "        if git_check.returncode != 0:\n",
    "            print(\"   ğŸ”§ åˆå§‹åŒ– Git å€‰åº«...\")\n",
    "            subprocess.run(['git', 'init'], check=True)\n",
    "            print(\"   âœ… Git å€‰åº«åˆå§‹åŒ–å®Œæˆ\")\n",
    "        else:\n",
    "            print(\"   âœ… å·²åœ¨ Git å€‰åº«ä¸­\")\n",
    "\n",
    "        # è¨­å®š Git ç”¨æˆ¶ï¼ˆå¦‚æœæœªè¨­å®šï¼‰\n",
    "        try:\n",
    "            subprocess.run(['git', 'config', 'user.name', 'Colab User'], check=True)\n",
    "            subprocess.run(['git', 'config', 'user.email', 'colab@spaceapps.com'], check=True)\n",
    "            print(\"   âœ… Git ç”¨æˆ¶è¨­å®šå®Œæˆ\")\n",
    "        except:\n",
    "            print(\"   âš ï¸ Git ç”¨æˆ¶è¨­å®šè·³é\")\n",
    "\n",
    "        # è¨­å®šé ç«¯å€‰åº«ï¼ˆè‡ªå‹•åµæ¸¬æˆ–ä½¿ç”¨é è¨­ï¼‰\n",
    "        try:\n",
    "            remote_check = subprocess.run(['git', 'remote', 'get-url', 'origin'],\n",
    "                                        capture_output=True, text=True)\n",
    "            if remote_check.returncode != 0:\n",
    "                print(\"   ğŸ”§ è¨­å®šé ç«¯å€‰åº«...\")\n",
    "                # ä½¿ç”¨é è¨­å€‰åº« URLï¼ˆç”¨æˆ¶éœ€è¦ä¿®æ”¹ç‚ºè‡ªå·±çš„å€‰åº«ï¼‰\n",
    "                default_repo = \"https://github.com/exoplanet-spaceapps/exoplanet-starter.git\"\n",
    "                subprocess.run(['git', 'remote', 'add', 'origin', default_repo], check=True)\n",
    "                print(f\"   âœ… é ç«¯å€‰åº«è¨­å®š: {default_repo}\")\n",
    "                print(\"   ğŸ’¡ è«‹ç¢ºä¿ä½ æœ‰è©²å€‰åº«çš„å¯«å…¥æ¬Šé™ï¼Œæˆ–ä¿®æ”¹ç‚ºä½ çš„å€‰åº«\")\n",
    "            else:\n",
    "                print(f\"   âœ… é ç«¯å€‰åº«å·²è¨­å®š: {remote_check.stdout.strip()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ é ç«¯å€‰åº«è¨­å®šè­¦å‘Š: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Git è¨­å®šå¤±æ•—: {e}\")\n",
    "        return False\n",
    "\n",
    "    # æ­¥é©Ÿ 4: Git LFS è¨­å®š\n",
    "    print(\"\\nğŸ“‹ æ­¥é©Ÿ 2/4: Git LFS è¨­å®š...\")\n",
    "\n",
    "    try:\n",
    "        # å®‰è£ Git LFSï¼ˆColabï¼‰\n",
    "        if IN_COLAB:\n",
    "            print(\"   ğŸ“¦ åœ¨ Colab ä¸­å®‰è£ Git LFS...\")\n",
    "            subprocess.run(['apt-get', 'update', '-qq'], check=True)\n",
    "            subprocess.run(['apt-get', 'install', '-y', '-qq', 'git-lfs'], check=True)\n",
    "            print(\"   âœ… Git LFS å·²å®‰è£\")\n",
    "\n",
    "        # åˆå§‹åŒ– LFS\n",
    "        try:\n",
    "            subprocess.run(['git', 'lfs', 'install'], check=True)\n",
    "            print(\"   âœ… Git LFS åˆå§‹åŒ–å®Œæˆ\")\n",
    "        except:\n",
    "            print(\"   âš ï¸ Git LFS åˆå§‹åŒ–è·³éï¼ˆå¯èƒ½å·²è¨­å®šï¼‰\")\n",
    "\n",
    "        # è¨­å®š LFS è¿½è¹¤ï¼ˆå®¹éŒ¯è™•ç†ï¼‰\n",
    "        lfs_patterns = ['*.csv', '*.json', '*.pkl', '*.parquet', '*.h5', '*.hdf5']\n",
    "        for pattern in lfs_patterns:\n",
    "            try:\n",
    "                result = subprocess.run(['git', 'lfs', 'track', pattern],\n",
    "                                      capture_output=True, text=True)\n",
    "                if result.returncode == 0:\n",
    "                    print(f\"   ğŸ“¦ LFS è¿½è¹¤: {pattern}\")\n",
    "                else:\n",
    "                    print(f\"   âš ï¸ LFS è¿½è¹¤ {pattern} è­¦å‘Š: {result.stderr.strip()}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸ LFS è¿½è¹¤ {pattern} è·³é: {e}\")\n",
    "\n",
    "        # æ·»åŠ  .gitattributes åˆ° staging\n",
    "        try:\n",
    "            subprocess.run(['git', 'add', '.gitattributes'], check=False)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ Git LFS è¨­å®šè­¦å‘Š: {e}\")\n",
    "        print(\"   ğŸ’¡ ç¹¼çºŒåŸ·è¡Œï¼Œä½†å¤§æª”æ¡ˆå¯èƒ½ç„¡æ³•æ­£ç¢ºè¿½è¹¤\")\n",
    "\n",
    "    # æ­¥é©Ÿ 5: æ·»åŠ æª”æ¡ˆä¸¦æäº¤\n",
    "    print(\"\\nğŸ“‹ æ­¥é©Ÿ 3/4: æ·»åŠ æª”æ¡ˆèˆ‡æäº¤...\")\n",
    "\n",
    "    try:\n",
    "        # ç¢ºä¿é‡è¦ç›®éŒ„å­˜åœ¨\n",
    "        important_dirs = ['data', 'notebooks', 'app', 'scripts']\n",
    "        for dir_name in important_dirs:\n",
    "            dir_path = Path(dir_name)\n",
    "            if dir_path.exists():\n",
    "                print(f\"   ğŸ“‚ æ‰¾åˆ°ç›®éŒ„: {dir_name}\")\n",
    "            elif IN_COLAB and dir_name == 'data':\n",
    "                # åœ¨ Colab ä¸­å‰µå»º data ç›®éŒ„ä¸¦è¤‡è£½ç‰¹å¾µæª”æ¡ˆ\n",
    "                dir_path.mkdir(exist_ok=True)\n",
    "                print(f\"   ğŸ“‚ å‰µå»ºç›®éŒ„: {dir_name}\")\n",
    "\n",
    "        # æ·»åŠ æ‰€æœ‰æª”æ¡ˆ\n",
    "        subprocess.run(['git', 'add', '.'], check=True)\n",
    "        print(\"   âœ… æª”æ¡ˆæ·»åŠ å®Œæˆ\")\n",
    "\n",
    "        # æª¢æŸ¥æ˜¯å¦æœ‰è®Šæ›´\n",
    "        status_result = subprocess.run(['git', 'status', '--porcelain'],\n",
    "                                      capture_output=True, text=True, check=True)\n",
    "\n",
    "        if not status_result.stdout.strip():\n",
    "            print(\"   âœ… æ²’æœ‰æ–°çš„è®Šæ›´éœ€è¦æäº¤\")\n",
    "            return True\n",
    "\n",
    "        # å‰µå»ºæäº¤\n",
    "        commit_message = \"\"\"data: update BLS/TLS baseline analysis results\n",
    "\n",
    "- ğŸ“Š å®Œæˆ BLS (Box Least Squares) é€±æœŸæœå°‹åˆ†æ\n",
    "- ğŸ” å®Œæˆ TLS (Transit Least Squares) é«˜ç²¾åº¦åˆ†æ\n",
    "- ğŸ“ˆ æå–æ©Ÿå™¨å­¸ç¿’ç‰¹å¾µ: bls_tls_features.csv\n",
    "- ğŸ“‹ ç”Ÿæˆåˆ†æå ±å‘Šèˆ‡è¦–è¦ºåŒ–çµæœ\n",
    "- ğŸ¯ æ¸¬è©¦å¤šå€‹ TESS/Kepler ç›®æ¨™çš„å‡Œæ—¥åµæ¸¬æ•ˆèƒ½\n",
    "- ğŸš€ æº–å‚™é€²è¡Œåˆæˆæ³¨å…¥è¨“ç·´ (03_injection_train.ipynb)\n",
    "\n",
    "Co-Authored-By: hctsai1006 <39769660@cuni.cz>\n",
    "        \"\"\"\n",
    "\n",
    "        subprocess.run(['git', 'commit', '-m', commit_message], check=True)\n",
    "        print(\"   âœ… æäº¤å®Œæˆ\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"   âŒ æª”æ¡ˆæäº¤å¤±æ•—: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ æª”æ¡ˆè™•ç†å¤±æ•—: {e}\")\n",
    "        return False\n",
    "\n",
    "    # æ­¥é©Ÿ 6: æ¨é€åˆ° GitHub\n",
    "    print(\"\\nğŸ“‹ æ­¥é©Ÿ 4/4: æ¨é€åˆ° GitHub...\")\n",
    "\n",
    "    try:\n",
    "        # ç²å–é ç«¯ URL ä¸¦æ’å…¥ token\n",
    "        remote_result = subprocess.run(['git', 'remote', 'get-url', 'origin'],\n",
    "                                      capture_output=True, text=True, check=True)\n",
    "        remote_url = remote_result.stdout.strip()\n",
    "\n",
    "        # æ§‹é€ å¸¶ token çš„ URL\n",
    "        if remote_url.startswith('https://github.com/'):\n",
    "            # æå–å€‰åº«è·¯å¾‘\n",
    "            repo_path = remote_url.replace('https://github.com/', '').replace('.git', '')\n",
    "            auth_url = f\"https://{token}@github.com/{repo_path}.git\"\n",
    "        else:\n",
    "            print(f\"   âš ï¸ é ç«¯ URL æ ¼å¼ç•°å¸¸: {remote_url}\")\n",
    "            auth_url = remote_url\n",
    "\n",
    "        # æ¨é€\n",
    "        push_result = subprocess.run([\n",
    "            'git', 'push', auth_url, 'main'\n",
    "        ], capture_output=True, text=True, timeout=300)\n",
    "\n",
    "        if push_result.returncode == 0:\n",
    "            print(\"   âœ… æ¨é€æˆåŠŸï¼\")\n",
    "            print(f\"   ğŸ“¡ æ¨é€è¼¸å‡º: {push_result.stdout[:200]}...\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"   âŒ æ¨é€å¤±æ•—: {push_result.stderr}\")\n",
    "            # å˜—è©¦æ¨é€åˆ°å…¶ä»–åˆ†æ”¯\n",
    "            try:\n",
    "                alt_push = subprocess.run([\n",
    "                    'git', 'push', auth_url, 'HEAD:main'\n",
    "                ], capture_output=True, text=True, timeout=300)\n",
    "                if alt_push.returncode == 0:\n",
    "                    print(\"   âœ… å‚™ç”¨æ¨é€æˆåŠŸï¼\")\n",
    "                    return True\n",
    "            except:\n",
    "                pass\n",
    "            return False\n",
    "\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"   âŒ æ¨é€è¶…æ™‚ï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£æ¥\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ æ¨é€å¤±æ•—: {e}\")\n",
    "        return False\n",
    "\n",
    "    finally:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸ“‹ BLS/TLS åˆ†æçµæœæ¨é€å®Œæˆ!\")\n",
    "        if IN_COLAB:\n",
    "            print(\"ğŸ’¡ å¦‚æœé‡åˆ°å•é¡Œ:\")\n",
    "            print(\"   1. ç¢ºä¿ token æœ‰ 'repo' æ¬Šé™\")\n",
    "            print(\"   2. ç¢ºä¿ä½ æœ‰ç›®æ¨™å€‰åº«çš„å¯«å…¥æ¬Šé™\")\n",
    "            print(\"   3. æª¢æŸ¥å€‰åº« URL æ˜¯å¦æ­£ç¢º\")\n",
    "\n",
    "# å‘¼å«å‡½æ•¸ï¼ˆè«‹åœ¨åŸ·è¡Œæ™‚æä¾› tokenï¼‰\n",
    "print(\"ğŸ” æº–å‚™æ¨é€ BLS/TLS åˆ†æçµæœ...\")\n",
    "print(\"ğŸ’¡ åŸ·è¡Œæ–¹å¼: ultimate_push_to_github_02(token='ä½ çš„GitHub_token')\")\n",
    "print(\"ğŸ“ æˆ–ç›´æ¥åŸ·è¡Œä¸‹æ–¹ cell ä¸¦åœ¨æç¤ºæ™‚è¼¸å…¥ token\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ GitHub Push çµ‚æ¥µè§£æ±ºæ–¹æ¡ˆ\n",
    "\n",
    "å°‡ BLS/TLS åˆ†æçµæœæ¨é€åˆ° GitHub å€‰åº«ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Phase 5 & 6 ç¸½çµå ±å‘Š\n",
    "\n",
    "### Phase 5: Wotan å»è¶¨å‹¢æ–¹æ³•æ¯”è¼ƒ\n",
    "\n",
    "æ¸¬è©¦äº†4ç¨®å»è¶¨å‹¢æ–¹æ³•ï¼š\n",
    "1. **Lightkurve flatten()**: æ»‘å‹•ä¸­ä½æ•¸æ¿¾æ³¢å™¨ï¼ˆwindow_length=401ï¼‰\n",
    "2. **Wotan biweight**: é›™é‡æ¬Šé‡æ³•ï¼Œå°é›¢ç¾¤å€¼å…·æœ‰ç©©å¥æ€§\n",
    "3. **Wotan rspline**: æ­£å‰‡åŒ–æ¨£æ¢æ³•ï¼Œå¹³æ»‘é€£çºŒæ›²ç·š\n",
    "4. **Wotan hspline**: è¶…æ¨£æ¢æ³•ï¼Œé©åˆé•·é€±æœŸè®ŠåŒ–\n",
    "\n",
    "**è©•ä¼°æŒ‡æ¨™**: Signal-to-Noise Ratio (SNR)\n",
    "\n",
    "### Phase 6: é€²éš BLS æŒ‡æ¨™\n",
    "\n",
    "æ–°å¢ç‰¹å¾µé¡åˆ¥ï¼š\n",
    "1. **å¥‡å¶æ·±åº¦åˆ†æ** (Odd/Even Transit Depth):\n",
    "   - ç”¨æ–¼æª¢æ¸¬å‡é™½æ€§ï¼ˆå¦‚é›™æ˜Ÿç³»çµ±ï¼‰\n",
    "   - çœŸå¯¦è¡Œæ˜Ÿçš„å¥‡å¶æ·±åº¦æ‡‰è©²ç›¸è¿‘\n",
    "   \n",
    "2. **å‡Œæ—¥å½¢ç‹€æŒ‡æ¨™** (Transit Shape):\n",
    "   - æ›²ç‡ (Curvature): å€åˆ† V-shape vs U-shape\n",
    "   - å°ç¨±æ€§ (Symmetry): è©•ä¼°å‡Œæ—¥çš„å·¦å³å°ç¨±æ€§\n",
    "   \n",
    "3. **å»è¶¨å‹¢å“è³ªæŒ‡æ¨™**:\n",
    "   - å„æ–¹æ³•çš„ SNR æ¯”è¼ƒ\n",
    "   - æœ€ä½³æ–¹æ³•é¸æ“‡\n",
    "   - SNR æ”¹å–„ç™¾åˆ†æ¯”\n",
    "\n",
    "### è¼¸å‡ºæª”æ¡ˆ\n",
    "\n",
    "- **C:\\Users\\thc1006\\Desktop\\dev\\exoplanet-starter\\data\\bls_tls_features_enhanced.csv**: å®Œæ•´å¢å¼·ç‰¹å¾µé›†\n",
    "- **C:\\Users\\thc1006\\Desktop\\dev\\exoplanet-starter\\data\\bls_tls_features_enhanced_stats.json**: ç‰¹å¾µçµ±è¨ˆèˆ‡èªªæ˜\n",
    "\n",
    "### ä¸‹ä¸€æ­¥: Phase 3 ç›£ç£å­¸ç¿’\n",
    "\n",
    "é€™äº›å¢å¼·ç‰¹å¾µå°‡ç”¨æ–¼ï¼š\n",
    "- è¨“ç·´æ©Ÿå™¨å­¸ç¿’åˆ†é¡å™¨ï¼ˆLogReg, XGBoost, Random Forestï¼‰\n",
    "- æ”¹å–„è¡Œæ˜Ÿå€™é¸é«”çš„åµæ¸¬ç²¾åº¦\n",
    "- é™ä½å‡é™½æ€§ç‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å„²å­˜å¢å¼·ç‰¹å¾µåˆ°æª”æ¡ˆ\n",
    "output_dir = Path(\"../data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# å„²å­˜å¢å¼·ç‰¹å¾µ CSV\n",
    "enhanced_features_file = output_dir / \"bls_tls_features_enhanced.csv\"\n",
    "enhanced_features_df.to_csv(enhanced_features_file, index=False)\n",
    "print(f\"\\nğŸ’¾ å¢å¼·ç‰¹å¾µå·²å„²å­˜è‡³: {enhanced_features_file}\")\n",
    "\n",
    "# å„²å­˜ç‰¹å¾µçµ±è¨ˆèˆ‡èªªæ˜\n",
    "enhanced_stats = {\n",
    "    'n_samples': len(enhanced_features_df),\n",
    "    'n_features': len(enhanced_features_df.columns),\n",
    "    'n_positive': int((enhanced_features_df['label'] == 1).sum()),\n",
    "    'n_negative': int((enhanced_features_df['label'] == 0).sum()),\n",
    "    'feature_categories': {\n",
    "        'basic_info': ['target_id', 'target_name', 'label', 'source', 'known_period', 'known_depth'],\n",
    "        'bls_features': [col for col in enhanced_features_df.columns if col.startswith('bls_')],\n",
    "        'tls_features': [col for col in enhanced_features_df.columns if col.startswith('tls_')],\n",
    "        'comparison_features': ['period_ratio', 'depth_ratio', 'snr_ratio', 'period_diff_pct', 'depth_diff_pct', 'snr_improvement'],\n",
    "        'detrending_features': [col for col in enhanced_features_df.columns if 'detrend' in col or col.endswith('_snr')],\n",
    "        'odd_even_features': ['odd_depth_ppm', 'even_depth_ppm', 'odd_even_ratio', 'odd_even_diff_ppm'],\n",
    "        'shape_features': ['transit_curvature', 'transit_symmetry', 'transit_points']\n",
    "    },\n",
    "    'phase_5_features': [col for col in enhanced_features_df.columns if 'detrend' in col or (col.endswith('_snr') and 'wotan' in col)],\n",
    "    'phase_6_features': ['odd_depth_ppm', 'even_depth_ppm', 'odd_even_ratio', 'odd_even_diff_ppm', \n",
    "                         'transit_curvature', 'transit_symmetry', 'transit_points']\n",
    "}\n",
    "\n",
    "# å„²å­˜çµ±è¨ˆè³‡è¨Š\n",
    "import json\n",
    "enhanced_stats_file = output_dir / \"bls_tls_features_enhanced_stats.json\"\n",
    "with open(enhanced_stats_file, 'w') as f:\n",
    "    json.dump(enhanced_stats, f, indent=2)\n",
    "print(f\"ğŸ“Š å¢å¼·ç‰¹å¾µçµ±è¨ˆå·²å„²å­˜è‡³: {enhanced_stats_file}\")\n",
    "\n",
    "# é¡¯ç¤ºå„é¡åˆ¥ç‰¹å¾µæ•¸é‡\n",
    "print(\"\\nğŸ“‹ ç‰¹å¾µåˆ†é¡çµ±è¨ˆï¼š\")\n",
    "for category, features_list in enhanced_stats['feature_categories'].items():\n",
    "    print(f\"   â€¢ {category}: {len(features_list)} å€‹ç‰¹å¾µ\")\n",
    "\n",
    "print(f\"\\nğŸŒŸ Phase 5 æ–°å¢ç‰¹å¾µ: {len(enhanced_stats['phase_5_features'])} å€‹\")\n",
    "print(f\"   {enhanced_stats['phase_5_features']}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Phase 6 æ–°å¢ç‰¹å¾µ: {len(enhanced_stats['phase_6_features'])} å€‹\")\n",
    "print(f\"   {enhanced_stats['phase_6_features']}\")\n",
    "\n",
    "print(\"\\nâœ… Phase 5 & 6 å®Œæˆï¼\")\n",
    "print(\"   æ‰€æœ‰å¢å¼·ç‰¹å¾µå·²æº–å‚™å®Œæˆï¼Œå¯ç”¨æ–¼ Phase 3 ç›£ç£å­¸ç¿’è¨“ç·´\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ Phase 6: Advanced BLS Metrics Extraction\n",
    "\"\"\"\n",
    "æå–é¡å¤–çš„ BLS æŒ‡æ¨™èˆ‡ç‰¹å¾µ\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ¯ Phase 6: Advanced BLS Metrics Extraction\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def calculate_odd_even_depth(lc: lk.LightCurve, period: float, t0: float, duration: float) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    è¨ˆç®—å¥‡å¶æ¬¡å‡Œæ—¥æ·±åº¦å·®ç•°ï¼ˆç”¨æ–¼æª¢æ¸¬å‡é™½æ€§ï¼Œå¦‚é›™æ˜Ÿç³»çµ±ï¼‰\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lc : lightkurve.LightCurve\n",
    "        å»è¶¨å‹¢å…‰æ›²ç·š\n",
    "    period : float\n",
    "        å‡Œæ—¥é€±æœŸ\n",
    "    t0 : float\n",
    "        ç¬¬ä¸€æ¬¡å‡Œæ—¥æ™‚é–“\n",
    "    duration : float\n",
    "        å‡Œæ—¥æŒçºŒæ™‚é–“\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : åŒ…å«å¥‡å¶æ·±åº¦èˆ‡æ¯”ç‡çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    try:\n",
    "        time_array = lc.time.value if hasattr(lc.time, 'value') else np.array(lc.time)\n",
    "        flux_array = lc.flux.value if hasattr(lc.flux, 'value') else np.array(lc.flux)\n",
    "        \n",
    "        # è¨ˆç®—æ¯å€‹è³‡æ–™é»æ‰€å±¬çš„é€±æœŸç·¨è™Ÿ\n",
    "        phase = (time_array - t0) / period\n",
    "        cycle_number = np.floor(phase)\n",
    "        \n",
    "        # åˆ†é›¢å¥‡æ•¸å’Œå¶æ•¸é€±æœŸ\n",
    "        odd_mask = (cycle_number % 2 == 1) & (np.abs(phase - cycle_number) < duration / period)\n",
    "        even_mask = (cycle_number % 2 == 0) & (np.abs(phase - cycle_number) < duration / period)\n",
    "        \n",
    "        # è¨ˆç®—æ·±åº¦ï¼ˆç›¸å°æ–¼ 1.0ï¼‰\n",
    "        if np.sum(odd_mask) > 0 and np.sum(even_mask) > 0:\n",
    "            odd_depth = 1.0 - np.median(flux_array[odd_mask])\n",
    "            even_depth = 1.0 - np.median(flux_array[even_mask])\n",
    "            \n",
    "            # è¨ˆç®—å·®ç•°æ¯”ç‡\n",
    "            if even_depth > 0:\n",
    "                depth_ratio = odd_depth / even_depth\n",
    "            else:\n",
    "                depth_ratio = np.nan\n",
    "            \n",
    "            return {\n",
    "                'odd_depth_ppm': odd_depth * 1e6,\n",
    "                'even_depth_ppm': even_depth * 1e6,\n",
    "                'odd_even_ratio': depth_ratio,\n",
    "                'odd_even_diff_ppm': (odd_depth - even_depth) * 1e6\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'odd_depth_ppm': np.nan,\n",
    "                'even_depth_ppm': np.nan,\n",
    "                'odd_even_ratio': np.nan,\n",
    "                'odd_even_diff_ppm': np.nan\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"      âš ï¸ è¨ˆç®—å¥‡å¶æ·±åº¦å¤±æ•—: {e}\")\n",
    "        return {\n",
    "            'odd_depth_ppm': np.nan,\n",
    "            'even_depth_ppm': np.nan,\n",
    "            'odd_even_ratio': np.nan,\n",
    "            'odd_even_diff_ppm': np.nan\n",
    "        }\n",
    "\n",
    "def calculate_transit_shape_metrics(lc: lk.LightCurve, period: float, t0: float, duration: float) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    è¨ˆç®—å‡Œæ—¥å½¢ç‹€æŒ‡æ¨™\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lc : lightkurve.LightCurve\n",
    "        å»è¶¨å‹¢å…‰æ›²ç·š\n",
    "    period : float\n",
    "        å‡Œæ—¥é€±æœŸ\n",
    "    t0 : float\n",
    "        ç¬¬ä¸€æ¬¡å‡Œæ—¥æ™‚é–“\n",
    "    duration : float\n",
    "        å‡Œæ—¥æŒçºŒæ™‚é–“\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : åŒ…å«å½¢ç‹€æŒ‡æ¨™çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # æ‘ºç–Šå…‰æ›²ç·š\n",
    "        folded_lc = lc.fold(period=period, epoch_time=t0)\n",
    "        \n",
    "        time_array = folded_lc.time.value if hasattr(folded_lc.time, 'value') else np.array(folded_lc.time)\n",
    "        flux_array = folded_lc.flux.value if hasattr(folded_lc.flux, 'value') else np.array(folded_lc.flux)\n",
    "        \n",
    "        # é¸æ“‡å‡Œæ—¥å€åŸŸ\n",
    "        transit_mask = np.abs(time_array) < duration / 2\n",
    "        \n",
    "        if np.sum(transit_mask) > 10:  # è‡³å°‘éœ€è¦10å€‹é»\n",
    "            transit_flux = flux_array[transit_mask]\n",
    "            transit_time = time_array[transit_mask]\n",
    "            \n",
    "            # è¨ˆç®— V-shape vs U-shape (æ›²ç‡)\n",
    "            # ç°¡åŒ–ç‰ˆï¼šè¨ˆç®—æœ€æ·±é»é™„è¿‘çš„æ›²ç‡\n",
    "            min_idx = np.argmin(transit_flux)\n",
    "            if min_idx > 0 and min_idx < len(transit_flux) - 1:\n",
    "                curvature = (transit_flux[min_idx-1] + transit_flux[min_idx+1] - 2*transit_flux[min_idx])\n",
    "            else:\n",
    "                curvature = np.nan\n",
    "            \n",
    "            # è¨ˆç®—å°ç¨±æ€§ï¼ˆå·¦å³åŠéƒ¨çš„å·®ç•°ï¼‰\n",
    "            mid_idx = len(transit_flux) // 2\n",
    "            left_mean = np.mean(transit_flux[:mid_idx])\n",
    "            right_mean = np.mean(transit_flux[mid_idx:])\n",
    "            symmetry = abs(left_mean - right_mean) / np.std(transit_flux)\n",
    "            \n",
    "            return {\n",
    "                'transit_curvature': curvature,\n",
    "                'transit_symmetry': symmetry,\n",
    "                'transit_points': int(np.sum(transit_mask))\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'transit_curvature': np.nan,\n",
    "                'transit_symmetry': np.nan,\n",
    "                'transit_points': int(np.sum(transit_mask))\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"      âš ï¸ è¨ˆç®—å‡Œæ—¥å½¢ç‹€å¤±æ•—: {e}\")\n",
    "        return {\n",
    "            'transit_curvature': np.nan,\n",
    "            'transit_symmetry': np.nan,\n",
    "            'transit_points': 0\n",
    "        }\n",
    "\n",
    "def extract_enhanced_bls_features(\n",
    "    search_result: Dict[str, Any],\n",
    "    detrending_result: Dict[str, Any]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    æå–å¢å¼·çš„ BLS ç‰¹å¾µï¼ˆåŒ…å« Phase 5 å’Œ Phase 6ï¼‰\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    search_result : dict\n",
    "        BLS/TLS æœå°‹çµæœ\n",
    "    detrending_result : dict\n",
    "        å»è¶¨å‹¢æ–¹æ³•æ¯”è¼ƒçµæœ\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : å¢å¼·ç‰¹å¾µå­—å…¸\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # åŸºæœ¬è³‡è¨Š\n",
    "    target = search_result['target']\n",
    "    features['target_id'] = target.get('id', '')\n",
    "    features['target_name'] = target.get('name', '')\n",
    "    features['label'] = target.get('label', -1)\n",
    "    features['source'] = target.get('source', '')\n",
    "    features['known_period'] = target.get('known_period', np.nan)\n",
    "    features['known_depth'] = target.get('known_depth', np.nan)\n",
    "    \n",
    "    # BLS åŸºæœ¬ç‰¹å¾µ\n",
    "    if 'bls' in search_result:\n",
    "        bls = search_result['bls']\n",
    "        features['bls_period'] = bls['period']\n",
    "        features['bls_t0'] = bls['t0']\n",
    "        features['bls_duration_hours'] = bls['duration'] * 24\n",
    "        features['bls_depth_ppm'] = bls['depth'] * 1e6\n",
    "        features['bls_snr'] = bls['snr']\n",
    "        features['bls_duration_phase'] = bls['duration'] / bls['period'] if bls['period'] > 0 else np.nan\n",
    "    \n",
    "    # TLS åŸºæœ¬ç‰¹å¾µ\n",
    "    if 'tls' in search_result:\n",
    "        tls = search_result['tls']\n",
    "        features['tls_period'] = tls['period']\n",
    "        features['tls_t0'] = tls['t0']\n",
    "        features['tls_duration_hours'] = tls['duration'] * 24\n",
    "        features['tls_depth_ppm'] = tls['depth'] * 1e6\n",
    "        features['tls_sde'] = tls['snr']\n",
    "        features['tls_duration_phase'] = tls['duration'] / tls['period'] if tls['period'] > 0 else np.nan\n",
    "    \n",
    "    # BLS vs TLS æ¯”è¼ƒç‰¹å¾µ\n",
    "    if 'bls' in search_result and 'tls' in search_result:\n",
    "        bls = search_result['bls']\n",
    "        tls = search_result['tls']\n",
    "        \n",
    "        features['period_ratio'] = tls['period'] / bls['period'] if bls['period'] > 0 else np.nan\n",
    "        features['period_diff_pct'] = abs(tls['period'] - bls['period']) / bls['period'] * 100 if bls['period'] > 0 else np.nan\n",
    "        features['depth_ratio'] = tls['depth'] / bls['depth'] if bls['depth'] > 0 else np.nan\n",
    "        features['depth_diff_pct'] = abs(tls['depth'] - bls['depth']) / bls['depth'] * 100 if bls['depth'] > 0 else np.nan\n",
    "        features['snr_ratio'] = tls['snr'] / bls['snr'] if bls['snr'] > 0 else np.nan\n",
    "        features['snr_improvement'] = (tls['snr'] - bls['snr']) / bls['snr'] * 100 if bls['snr'] > 0 else np.nan\n",
    "    \n",
    "    # Phase 5: å»è¶¨å‹¢æ–¹æ³•æ¯”è¼ƒç‰¹å¾µ\n",
    "    if detrending_result:\n",
    "        methods = detrending_result['methods']\n",
    "        features['best_detrend_method'] = detrending_result['best_method']\n",
    "        features['best_detrend_snr'] = detrending_result['best_snr']\n",
    "        \n",
    "        # å„æ–¹æ³•çš„ SNR\n",
    "        for method_key in ['lightkurve_flatten', 'wotan_biweight', 'wotan_rspline', 'wotan_hspline']:\n",
    "            if method_key in methods:\n",
    "                features[f'{method_key}_snr'] = methods[method_key]['snr']\n",
    "        \n",
    "        # SNR æ”¹å–„\n",
    "        if 'lightkurve_flatten' in methods and detrending_result['best_method'] != 'lightkurve_flatten':\n",
    "            baseline_snr = methods['lightkurve_flatten']['snr']\n",
    "            best_snr = detrending_result['best_snr']\n",
    "            if baseline_snr > 0:\n",
    "                features['snr_improvement_by_wotan'] = (best_snr - baseline_snr) / baseline_snr * 100\n",
    "    \n",
    "    # Phase 6: å¥‡å¶æ·±åº¦èˆ‡å½¢ç‹€ç‰¹å¾µ\n",
    "    if 'bls' in search_result and 'lc_flat' in search_result:\n",
    "        bls = search_result['bls']\n",
    "        lc_flat = search_result['lc_flat']\n",
    "        \n",
    "        # è¨ˆç®—å¥‡å¶æ·±åº¦\n",
    "        odd_even = calculate_odd_even_depth(lc_flat, bls['period'], bls['t0'], bls['duration'])\n",
    "        features.update(odd_even)\n",
    "        \n",
    "        # è¨ˆç®—å½¢ç‹€æŒ‡æ¨™\n",
    "        shape = calculate_transit_shape_metrics(lc_flat, bls['period'], bls['t0'], bls['duration'])\n",
    "        features.update(shape)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# æå–æ‰€æœ‰ç›®æ¨™çš„å¢å¼·ç‰¹å¾µ\n",
    "print(\"\\né–‹å§‹æå–å¢å¼· BLS ç‰¹å¾µ...\")\n",
    "enhanced_features_list = []\n",
    "\n",
    "for target_id in search_results.keys():\n",
    "    print(f\"\\nğŸ¯ æå– {search_results[target_id]['target']['name']} çš„å¢å¼·ç‰¹å¾µ...\")\n",
    "    \n",
    "    # ç²å–å»è¶¨å‹¢çµæœ\n",
    "    detrend_result = detrending_results.get(target_id, None)\n",
    "    \n",
    "    # æå–ç‰¹å¾µ\n",
    "    enhanced_features = extract_enhanced_bls_features(\n",
    "        search_results[target_id],\n",
    "        detrend_result\n",
    "    )\n",
    "    \n",
    "    enhanced_features_list.append(enhanced_features)\n",
    "    \n",
    "    print(f\"   âœ… ç‰¹å¾µæå–å®Œæˆ\")\n",
    "\n",
    "# è½‰æ›ç‚º DataFrame\n",
    "enhanced_features_df = pd.DataFrame(enhanced_features_list)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š å¢å¼·ç‰¹å¾µçµ±è¨ˆï¼š\")\n",
    "print(f\"   æ¨£æœ¬æ•¸: {len(enhanced_features_df)}\")\n",
    "print(f\"   ç‰¹å¾µæ•¸: {len(enhanced_features_df.columns)}\")\n",
    "print(f\"   æ­£æ¨£æœ¬: {(enhanced_features_df['label'] == 1).sum()}\")\n",
    "print(f\"   è² æ¨£æœ¬: {(enhanced_features_df['label'] == 0).sum()}\")\n",
    "\n",
    "print(\"\\nğŸ“ æ–°å¢ç‰¹å¾µåˆ—è¡¨ï¼š\")\n",
    "new_features = [col for col in enhanced_features_df.columns if col not in features_df.columns]\n",
    "for i, col in enumerate(new_features, 1):\n",
    "    print(f\"   {i}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Phase 6: Advanced BLS Metrics and Feature Preparation\n",
    "\n",
    "æå–é¡å¤–çš„ BLS æŒ‡æ¨™ï¼Œç‚º Phase 3 ç›£ç£å­¸ç¿’ç®¡ç·šæº–å‚™å®Œæ•´ç‰¹å¾µï¼š\n",
    "- **Depth (æ·±åº¦)**: å‡Œæ—¥æœŸé–“çš„æµé‡ä¸‹é™\n",
    "- **Duration (æŒçºŒæ™‚é–“)**: å‡Œæ—¥äº‹ä»¶çš„æ™‚é–“é•·åº¦\n",
    "- **SNR (ä¿¡å™ªæ¯”)**: è¨Šè™Ÿå¼·åº¦è©•ä¼°\n",
    "- **Odd/Even Transit Depth**: å¥‡å¶æ¬¡å‡Œæ—¥æ·±åº¦æ¯”è¼ƒï¼ˆæª¢æ¸¬å‡é™½æ€§ï¼‰\n",
    "- **Transit Shape**: å‡Œæ—¥å½¢ç‹€åƒæ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¦–è¦ºåŒ–ï¼š4ç¨®å»è¶¨å‹¢æ–¹æ³•çš„ä¸¦æ’æ¯”è¼ƒ\n",
    "def plot_detrending_comparison(detrending_result: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    ç¹ªè£½4ç¨®å»è¶¨å‹¢æ–¹æ³•çš„ä¸¦æ’æ¯”è¼ƒåœ–\n",
    "    \"\"\"\n",
    "    target = detrending_result['target']\n",
    "    methods = detrending_result['methods']\n",
    "    best_method = detrending_result['best_method']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    fig.suptitle(f\"{target['name']} ({target['id']}) - å»è¶¨å‹¢æ–¹æ³•æ¯”è¼ƒ\", \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    method_names = ['lightkurve_flatten', 'wotan_biweight', 'wotan_rspline', 'wotan_hspline']\n",
    "    method_titles = [\n",
    "        'Lightkurve flatten()',\n",
    "        'Wotan Biweight',\n",
    "        'Wotan R-Spline',\n",
    "        'Wotan H-Spline'\n",
    "    ]\n",
    "    \n",
    "    for idx, (method_key, title) in enumerate(zip(method_names, method_titles)):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        if method_key in methods:\n",
    "            lc = methods[method_key]['lc']\n",
    "            snr = methods[method_key]['snr']\n",
    "            \n",
    "            # ç¹ªè£½å…‰æ›²ç·š\n",
    "            lc.scatter(ax=ax, s=0.5, color='blue', alpha=0.4)\n",
    "            \n",
    "            # æ¨™é¡Œï¼ˆæœ€ä½³æ–¹æ³•åŠ æ˜Ÿè™Ÿï¼‰\n",
    "            is_best = (method_key == best_method)\n",
    "            title_text = f\"{title}\\nSNR: {snr:.2f}\"\n",
    "            if is_best:\n",
    "                title_text = f\"ğŸ† {title_text} ğŸ†\"\n",
    "                ax.set_facecolor('#ffffcc')  # æ·¡é»ƒè‰²èƒŒæ™¯\n",
    "            \n",
    "            ax.set_title(title_text, fontsize=11, fontweight='bold' if is_best else 'normal')\n",
    "            ax.set_xlabel('æ™‚é–“ (BTJD)', fontsize=9)\n",
    "            ax.set_ylabel('æ¨™æº–åŒ–æµé‡', fontsize=9)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # è¨ˆç®—ä¸¦é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š\n",
    "            flux = lc.flux.value if hasattr(lc.flux, 'value') else np.array(lc.flux)\n",
    "            flux_clean = flux[~np.isnan(flux)]\n",
    "            \n",
    "            textstr = f'Mean: {np.mean(flux_clean):.4f}\\nStd: {np.std(flux_clean):.4f}\\nPoints: {len(flux_clean):,}'\n",
    "            ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=8,\n",
    "                   verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, f'{title}\\nè³‡æ–™ä¸å¯ç”¨', \n",
    "                   ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# ç¹ªè£½æ‰€æœ‰ç›®æ¨™çš„å»è¶¨å‹¢æ–¹æ³•æ¯”è¼ƒåœ–\n",
    "print(\"\\nğŸ“Š ç¹ªè£½å»è¶¨å‹¢æ–¹æ³•æ¯”è¼ƒåœ–...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for target_id, result in detrending_results.items():\n",
    "    print(f\"\\nğŸ“Š {result['target']['name']} - æœ€ä½³æ–¹æ³•: {result['best_method']}\")\n",
    "    fig = plot_detrending_comparison(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°æ¯å€‹ç›®æ¨™åŸ·è¡Œå¤šæ–¹æ³•å»è¶¨å‹¢æ¯”è¼ƒ\n",
    "for target_id, data in processed_data.items():\n",
    "    print(f\"\\nğŸ¯ åˆ†æ {data['target']['name']} ({target_id})...\")\n",
    "    \n",
    "    lc_clean = data['lc_clean']\n",
    "    lc_flat_original = data['lc_flat']\n",
    "    \n",
    "    # å„²å­˜å„æ–¹æ³•çµæœ\n",
    "    methods_results = {}\n",
    "    \n",
    "    # 1. Lightkurve flatten() - å·²æœ‰çš„çµæœ\n",
    "    snr_lightkurve = calculate_snr(lc_flat_original)\n",
    "    methods_results['lightkurve_flatten'] = {\n",
    "        'lc': lc_flat_original,\n",
    "        'snr': snr_lightkurve,\n",
    "        'method': 'lightkurve_flatten'\n",
    "    }\n",
    "    print(f\"   âœ… Lightkurve flatten() - SNR: {snr_lightkurve:.2f}\")\n",
    "    \n",
    "    # 2. Wotan biweight\n",
    "    lc_biweight, snr_biweight, meta_biweight = apply_wotan_detrending(\n",
    "        lc_clean, method='biweight', window_length=0.5\n",
    "    )\n",
    "    methods_results['wotan_biweight'] = {\n",
    "        'lc': lc_biweight,\n",
    "        'snr': snr_biweight,\n",
    "        'method': 'wotan_biweight',\n",
    "        'metadata': meta_biweight\n",
    "    }\n",
    "    \n",
    "    # 3. Wotan rspline\n",
    "    lc_rspline, snr_rspline, meta_rspline = apply_wotan_detrending(\n",
    "        lc_clean, method='rspline', window_length=0.5\n",
    "    )\n",
    "    methods_results['wotan_rspline'] = {\n",
    "        'lc': lc_rspline,\n",
    "        'snr': snr_rspline,\n",
    "        'method': 'wotan_rspline',\n",
    "        'metadata': meta_rspline\n",
    "    }\n",
    "    \n",
    "    # 4. Wotan hspline\n",
    "    lc_hspline, snr_hspline, meta_hspline = apply_wotan_detrending(\n",
    "        lc_clean, method='hspline', window_length=0.5\n",
    "    )\n",
    "    methods_results['wotan_hspline'] = {\n",
    "        'lc': lc_hspline,\n",
    "        'snr': snr_hspline,\n",
    "        'method': 'wotan_hspline',\n",
    "        'metadata': meta_hspline\n",
    "    }\n",
    "    \n",
    "    # æ‰¾å‡ºæœ€ä½³ SNR çš„æ–¹æ³•\n",
    "    best_method = max(methods_results.items(), key=lambda x: x[1]['snr'])\n",
    "    best_method_name = best_method[0]\n",
    "    best_snr = best_method[1]['snr']\n",
    "    \n",
    "    print(f\"\\n   ğŸ† æœ€ä½³æ–¹æ³•: {best_method_name} (SNR: {best_snr:.2f})\")\n",
    "    \n",
    "    # å„²å­˜çµæœ\n",
    "    detrending_results[target_id] = {\n",
    "        'target': data['target'],\n",
    "        'methods': methods_results,\n",
    "        'best_method': best_method_name,\n",
    "        'best_snr': best_snr\n",
    "    }\n",
    "\n",
    "print(\"\\nâœ… æ‰€æœ‰ç›®æ¨™çš„å»è¶¨å‹¢æ–¹æ³•æ¯”è¼ƒå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸŒŸ Phase 5: Wotan Detrending Comparison\n",
    "\"\"\"\n",
    "æ¯”è¼ƒä¸åŒå»è¶¨å‹¢æ–¹æ³•çš„æ•ˆèƒ½\n",
    "- Lightkurve flatten() (å·²ä½¿ç”¨)\n",
    "- Wotan biweight\n",
    "- Wotan rspline\n",
    "- Wotan hspline\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸŒŸ Phase 5: Wotan Detrending Method Comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# å°å…¥ wotan\n",
    "try:\n",
    "    from wotan import flatten as wotan_flatten\n",
    "    print(\"âœ… Wotan å°å…¥æˆåŠŸ\")\n",
    "except ImportError:\n",
    "    print(\"âŒ Wotan æœªå®‰è£ï¼Œæ­£åœ¨å®‰è£...\")\n",
    "    import subprocess, sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"wotan\"])\n",
    "    from wotan import flatten as wotan_flatten\n",
    "    print(\"âœ… Wotan å®‰è£ä¸¦å°å…¥æˆåŠŸ\")\n",
    "\n",
    "def calculate_snr(lc: lk.LightCurve) -> float:\n",
    "    \"\"\"\n",
    "    è¨ˆç®—å…‰æ›²ç·šçš„ä¿¡å™ªæ¯” (SNR)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lc : lightkurve.LightCurve\n",
    "        è¼¸å…¥å…‰æ›²ç·š\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float : ä¿¡å™ªæ¯”\n",
    "    \"\"\"\n",
    "    flux = lc.flux.value if hasattr(lc.flux, 'value') else np.array(lc.flux)\n",
    "    \n",
    "    # ç§»é™¤ NaN å€¼\n",
    "    flux_clean = flux[~np.isnan(flux)]\n",
    "    \n",
    "    if len(flux_clean) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # SNR = mean / std\n",
    "    mean_flux = np.mean(flux_clean)\n",
    "    std_flux = np.std(flux_clean)\n",
    "    \n",
    "    if std_flux == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return mean_flux / std_flux\n",
    "\n",
    "def apply_wotan_detrending(\n",
    "    lc_clean: lk.LightCurve,\n",
    "    method: str = 'biweight',\n",
    "    window_length: float = 0.5\n",
    ") -> Tuple[lk.LightCurve, float, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ Wotan é€²è¡Œå»è¶¨å‹¢è™•ç†\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lc_clean : lightkurve.LightCurve\n",
    "        æ¸…ç†éçš„å…‰æ›²ç·š\n",
    "    method : str\n",
    "        Wotan æ–¹æ³•: 'biweight', 'rspline', 'hspline'\n",
    "    window_length : float\n",
    "        æ»‘å‹•è¦–çª—é•·åº¦ï¼ˆå¤©ï¼‰\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (å»è¶¨å‹¢å…‰æ›²ç·š, SNR, metadata)\n",
    "    \"\"\"\n",
    "    print(f\"   ğŸ”§ æ­£åœ¨ä½¿ç”¨ Wotan {method} æ–¹æ³•å»è¶¨å‹¢...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # æº–å‚™è³‡æ–™\n",
    "    time_array = lc_clean.time.value if hasattr(lc_clean.time, 'value') else np.array(lc_clean.time)\n",
    "    flux_array = lc_clean.flux.value if hasattr(lc_clean.flux, 'value') else np.array(lc_clean.flux)\n",
    "    \n",
    "    try:\n",
    "        # åŸ·è¡Œ Wotan å»è¶¨å‹¢\n",
    "        flatten_flux, trend_flux = wotan_flatten(\n",
    "            time_array,\n",
    "            flux_array,\n",
    "            method=method,\n",
    "            window_length=window_length,\n",
    "            return_trend=True\n",
    "        )\n",
    "        \n",
    "        # å‰µå»ºæ–°çš„ LightCurve ç‰©ä»¶\n",
    "        lc_wotan = lc_clean.copy()\n",
    "        lc_wotan.flux = flatten_flux\n",
    "        \n",
    "        # è¨ˆç®— SNR\n",
    "        snr = calculate_snr(lc_wotan)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        metadata = {\n",
    "            'method': method,\n",
    "            'window_length': window_length,\n",
    "            'snr': snr,\n",
    "            'elapsed_time': elapsed_time,\n",
    "            'n_points': len(flatten_flux)\n",
    "        }\n",
    "        \n",
    "        print(f\"   âœ… Wotan {method} å®Œæˆï¼ˆè€—æ™‚ {elapsed_time:.2f} ç§’ï¼‰\")\n",
    "        print(f\"      SNR: {snr:.2f}\")\n",
    "        \n",
    "        return lc_wotan, snr, metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Wotan {method} å¤±æ•—: {e}\")\n",
    "        # è¿”å›åŸå§‹å…‰æ›²ç·šä½œç‚º fallback\n",
    "        return lc_clean, 0.0, {'method': method, 'error': str(e)}\n",
    "\n",
    "# å„²å­˜æ‰€æœ‰å»è¶¨å‹¢çµæœ\n",
    "detrending_results = {}\n",
    "\n",
    "print(\"\\né–‹å§‹å°æ‰€æœ‰ç›®æ¨™é€²è¡Œå¤šæ–¹æ³•å»è¶¨å‹¢æ¯”è¼ƒ...\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒŸ Phase 5: Wotan Detrending Method Comparison\n",
    "\n",
    "æ¯”è¼ƒä¸åŒå»è¶¨å‹¢æ–¹æ³•çš„æ•ˆèƒ½ï¼Œæ‰¾å‡ºæœ€ä½³çš„è¨Šè™Ÿå“è³ªï¼š\n",
    "- **Lightkurve flatten()**: å·²ä½¿ç”¨çš„é è¨­æ–¹æ³•\n",
    "- **Wotan biweight**: é›™é‡æ¬Šé‡æ³•ï¼Œå°é›¢ç¾¤å€¼å…·æœ‰ç©©å¥æ€§\n",
    "- **Wotan rspline**: æ­£å‰‡åŒ–æ¨£æ¢æ³•ï¼Œå¹³æ»‘é€£çºŒæ›²ç·š\n",
    "- **Wotan hspline**: è¶…æ¨£æ¢æ³•ï¼Œé©åˆé•·é€±æœŸè®ŠåŒ–"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
