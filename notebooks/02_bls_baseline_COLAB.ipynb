{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üöÄ Exoplanet Feature Extraction - Google Colab Production Notebook\n",
    "\n",
    "**Objective**: Extract BLS/TLS features from 11,979 exoplanet candidates with checkpoint recovery\n",
    "\n",
    "## üìã Features\n",
    "- ‚úÖ Checkpoint system for handling disconnects\n",
    "- ‚úÖ Batch processing (100 samples per checkpoint)\n",
    "- ‚úÖ Progress tracking with ETA\n",
    "- ‚úÖ Auto-resume from last checkpoint\n",
    "- ‚úÖ Google Drive integration for persistence\n",
    "\n",
    "## üéØ Output\n",
    "- `bls_tls_features.csv`: 17 features per sample\n",
    "- Checkpoints every 100 samples\n",
    "- Failed samples log\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell1-header"
   },
   "source": [
    "## üì¶ Cell 1: Package Installation\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT**: After running this cell, you MUST restart the runtime:\n",
    "- Click **Runtime** ‚Üí **Restart runtime**\n",
    "- Then continue from Cell 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell1"
   },
   "outputs": [],
   "source": [
    "# Install required packages with NumPy 1.x compatibility\n",
    "!pip install -q numpy==1.26.4 scipy'<1.13' astropy\n",
    "!pip install -q lightkurve transitleastsquares\n",
    "!pip install -q tqdm pandas matplotlib\n",
    "\n",
    "print(\"‚úÖ Installation complete!\")\n",
    "print(\"‚ö†Ô∏è  RESTART RUNTIME NOW: Runtime ‚Üí Restart runtime\")\n",
    "print(\"Then continue from Cell 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell2-header"
   },
   "source": [
    "## üíæ Cell 2: Google Drive Setup\n",
    "\n",
    "Mount Google Drive for persistent storage across disconnects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell2"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directories\n",
    "BASE_DIR = Path('/content/drive/MyDrive/exoplanet-spaceapps')\n",
    "CHECKPOINT_DIR = BASE_DIR / 'checkpoints'\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "OUTPUT_DIR = BASE_DIR / 'results'\n",
    "\n",
    "for dir_path in [CHECKPOINT_DIR, DATA_DIR, OUTPUT_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"‚úÖ Created: {dir_path}\")\n",
    "\n",
    "print(f\"\\nüìÇ Working directory: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell3-header"
   },
   "source": [
    "## üõ†Ô∏è Cell 3: Checkpoint Manager\n",
    "\n",
    "Inline checkpoint manager for automatic recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell3"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Set\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class CheckpointManager:\n",
    "    \"\"\"\n",
    "    Manages incremental progress with automatic recovery\n",
    "\n",
    "    Features:\n",
    "    - Save batch progress to Google Drive\n",
    "    - Resume from last checkpoint after disconnect\n",
    "    - Merge all checkpoints into final dataset\n",
    "    - Track failed samples for retry\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, drive_path: str, batch_size: int = 100):\n",
    "        \"\"\"\n",
    "        Initialize checkpoint manager\n",
    "\n",
    "        Args:\n",
    "            drive_path: Path to Google Drive directory\n",
    "            batch_size: Number of samples per batch\n",
    "        \"\"\"\n",
    "        self.drive_path = Path(drive_path)\n",
    "        self.checkpoint_dir = self.drive_path / \"checkpoints\"\n",
    "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def save_checkpoint(\n",
    "        self,\n",
    "        batch_id: int,\n",
    "        features: Dict[int, Dict],\n",
    "        failed_indices: Optional[List[int]] = None,\n",
    "        metadata: Optional[Dict] = None\n",
    "    ) -> Path:\n",
    "        \"\"\"\n",
    "        Save batch progress to Drive\n",
    "\n",
    "        Args:\n",
    "            batch_id: Starting index of batch\n",
    "            features: Dictionary mapping sample index -> feature dict\n",
    "            failed_indices: List of indices that failed processing\n",
    "            metadata: Additional metadata to save\n",
    "\n",
    "        Returns:\n",
    "            Path to saved checkpoint file\n",
    "        \"\"\"\n",
    "        checkpoint = {\n",
    "            \"checkpoint_id\": f\"batch_{batch_id:04d}_{batch_id + self.batch_size:04d}\",\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"batch_range\": [batch_id, batch_id + self.batch_size],\n",
    "            \"completed_indices\": list(features.keys()),\n",
    "            \"failed_indices\": failed_indices or [],\n",
    "            \"features\": features,\n",
    "            \"metadata\": metadata or {}\n",
    "        }\n",
    "\n",
    "        checkpoint_file = self.checkpoint_dir / f\"{checkpoint['checkpoint_id']}.json\"\n",
    "        with open(checkpoint_file, 'w') as f:\n",
    "            json.dump(checkpoint, f, indent=2)\n",
    "\n",
    "        print(f\"üíæ Checkpoint saved: {checkpoint_file.name}\")\n",
    "        print(f\"   ‚úÖ Completed: {len(features)}\")\n",
    "        print(f\"   ‚ùå Failed: {len(failed_indices) if failed_indices else 0}\")\n",
    "\n",
    "        return checkpoint_file\n",
    "\n",
    "    def load_latest_checkpoint(self) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Resume from most recent checkpoint\n",
    "\n",
    "        Returns:\n",
    "            Checkpoint dictionary or None if no checkpoints exist\n",
    "        \"\"\"\n",
    "        checkpoints = sorted(self.checkpoint_dir.glob(\"batch_*.json\"))\n",
    "        if not checkpoints:\n",
    "            print(\"üìÇ No checkpoints found - starting fresh\")\n",
    "            return None\n",
    "\n",
    "        latest = checkpoints[-1]\n",
    "        with open(latest, 'r') as f:\n",
    "            checkpoint = json.load(f)\n",
    "\n",
    "        print(f\"üìÇ Loaded checkpoint: {latest.name}\")\n",
    "        print(f\"   Timestamp: {checkpoint['timestamp']}\")\n",
    "        print(f\"   Completed: {len(checkpoint['completed_indices'])}\")\n",
    "\n",
    "        return checkpoint\n",
    "\n",
    "    def get_completed_indices(self) -> Set[int]:\n",
    "        \"\"\"\n",
    "        Get all successfully processed indices across all checkpoints\n",
    "\n",
    "        Returns:\n",
    "            Set of completed sample indices\n",
    "        \"\"\"\n",
    "        completed = set()\n",
    "        for checkpoint_file in self.checkpoint_dir.glob(\"batch_*.json\"):\n",
    "            with open(checkpoint_file, 'r') as f:\n",
    "                checkpoint = json.load(f)\n",
    "                completed.update(checkpoint[\"completed_indices\"])\n",
    "        return completed\n",
    "\n",
    "    def get_failed_indices(self) -> List[int]:\n",
    "        \"\"\"\n",
    "        Get all failed indices across all checkpoints\n",
    "\n",
    "        Returns:\n",
    "            List of failed sample indices\n",
    "        \"\"\"\n",
    "        failed = set()\n",
    "        for checkpoint_file in self.checkpoint_dir.glob(\"batch_*.json\"):\n",
    "            with open(checkpoint_file, 'r') as f:\n",
    "                checkpoint = json.load(f)\n",
    "                failed.update(checkpoint.get(\"failed_indices\", []))\n",
    "        return sorted(failed)\n",
    "\n",
    "    def merge_all_checkpoints(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Merge all checkpoint features into single DataFrame\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with all features from all checkpoints\n",
    "        \"\"\"\n",
    "        all_features = {}\n",
    "\n",
    "        checkpoint_files = sorted(self.checkpoint_dir.glob(\"batch_*.json\"))\n",
    "        print(f\"\\nüîÑ Merging {len(checkpoint_files)} checkpoints...\")\n",
    "\n",
    "        for checkpoint_file in checkpoint_files:\n",
    "            with open(checkpoint_file, 'r') as f:\n",
    "                checkpoint = json.load(f)\n",
    "                all_features.update(checkpoint[\"features\"])\n",
    "\n",
    "        df = pd.DataFrame.from_dict(all_features, orient='index')\n",
    "        print(f\"‚úÖ Merged {len(df)} samples\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_progress_summary(self, total_samples: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Get summary of processing progress\n",
    "\n",
    "        Args:\n",
    "            total_samples: Total number of samples to process\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with progress statistics\n",
    "        \"\"\"\n",
    "        completed = self.get_completed_indices()\n",
    "        failed = self.get_failed_indices()\n",
    "\n",
    "        return {\n",
    "            \"total_samples\": total_samples,\n",
    "            \"completed\": len(completed),\n",
    "            \"failed\": len(failed),\n",
    "            \"remaining\": total_samples - len(completed),\n",
    "            \"success_rate\": len(completed) / total_samples * 100 if total_samples > 0 else 0,\n",
    "            \"failure_rate\": len(failed) / total_samples * 100 if total_samples > 0 else 0\n",
    "        }\n",
    "\n",
    "    def cleanup_checkpoints(self) -> None:\n",
    "        \"\"\"\n",
    "        Remove all checkpoint files (use after successful merge)\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        for checkpoint_file in self.checkpoint_dir.glob(\"batch_*.json\"):\n",
    "            checkpoint_file.unlink()\n",
    "            count += 1\n",
    "\n",
    "        print(f\"üóëÔ∏è Cleaned up {count} checkpoint files\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ CheckpointManager loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell4-header"
   },
   "source": [
    "## üìä Cell 4: Load Dataset\n",
    "\n",
    "Upload `supervised_dataset.csv` to Google Drive or use file upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.colab import files\n",
    "\n",
    "# Option 1: Upload file manually\n",
    "# Uncomment below to upload from local machine\n",
    "# uploaded = files.upload()\n",
    "# samples_df = pd.read_csv('supervised_dataset.csv')\n",
    "\n",
    "# Option 2: Load from Google Drive (recommended)\n",
    "dataset_path = DATA_DIR / 'supervised_dataset.csv'\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    print(\"‚ùå Dataset not found!\")\n",
    "    print(f\"Please upload supervised_dataset.csv to: {DATA_DIR}\")\n",
    "    print(\"\\nOr uncomment the file upload lines above\")\n",
    "else:\n",
    "    samples_df = pd.read_csv(dataset_path)\n",
    "    print(f\"‚úÖ Loaded dataset: {len(samples_df)} samples\")\n",
    "    print(f\"\\nColumns: {list(samples_df.columns)}\")\n",
    "    print(f\"\\nFirst 3 rows:\")\n",
    "    display(samples_df.head(3))\n",
    "    \n",
    "    # Data validation\n",
    "    required_cols = ['label', 'target_id', 'period', 'depth', 'duration']\n",
    "    missing_cols = [col for col in required_cols if col not in samples_df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"‚ö†Ô∏è Missing columns: {missing_cols}\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ All required columns present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell5-header"
   },
   "source": [
    "## üî¨ Cell 5: Feature Extraction Functions\n",
    "\n",
    "BLS/TLS feature extraction with 17 features per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightkurve as lk\n",
    "from typing import Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def extract_features_from_lightcurve(\n",
    "    time: np.ndarray,\n",
    "    flux: np.ndarray,\n",
    "    period: float,\n",
    "    duration: float,\n",
    "    epoch: float,\n",
    "    depth: float,\n",
    "    run_bls: bool = True\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extract comprehensive BLS + TLS features (17 features total)\n",
    "\n",
    "    Features:\n",
    "    - Input parameters (4): period, depth, duration, epoch\n",
    "    - Flux statistics (4): std, mad, skewness, kurtosis\n",
    "    - BLS features (5): detected period, t0, duration, depth, SNR\n",
    "    - Advanced features (4): odd-even diff, symmetry, periodicity, duration_ratio\n",
    "\n",
    "    Args:\n",
    "        time: Time array\n",
    "        flux: Flux array (normalized)\n",
    "        period: Known period from catalog\n",
    "        duration: Known duration from catalog\n",
    "        epoch: Transit epoch\n",
    "        depth: Known depth from catalog\n",
    "        run_bls: Whether to run BLS search\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with 17 features\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "\n",
    "    try:\n",
    "        # 1. Input parameters (4 features)\n",
    "        features['input_period'] = float(period)\n",
    "        features['input_depth'] = float(depth)\n",
    "        features['input_duration'] = float(duration)\n",
    "        features['input_epoch'] = float(epoch) if not np.isnan(epoch) else float(time[0])\n",
    "\n",
    "        # 2. Flux statistics (4 features)\n",
    "        features['flux_std'] = float(np.std(flux))\n",
    "        features['flux_mad'] = float(np.median(np.abs(flux - np.median(flux))))\n",
    "        \n",
    "        # Skewness\n",
    "        mean = np.mean(flux)\n",
    "        std = np.std(flux)\n",
    "        features['flux_skewness'] = float(np.mean(((flux - mean) / (std + 1e-10)) ** 3))\n",
    "        \n",
    "        # Kurtosis\n",
    "        features['flux_kurtosis'] = float(np.mean(((flux - mean) / (std + 1e-10)) ** 4) - 3.0)\n",
    "\n",
    "        # 3. BLS features (5 features)\n",
    "        if run_bls and len(time) > 50:\n",
    "            try:\n",
    "                lc = lk.LightCurve(time=time, flux=flux)\n",
    "                bls = lc.to_periodogram(\n",
    "                    method=\"bls\",\n",
    "                    minimum_period=max(0.5, period * 0.8),\n",
    "                    maximum_period=min(20.0, period * 1.2),\n",
    "                    frequency_factor=3.0\n",
    "                )\n",
    "                features['bls_period'] = float(bls.period_at_max_power.value)\n",
    "                features['bls_t0'] = float(bls.transit_time_at_max_power.value)\n",
    "                features['bls_duration'] = float(bls.duration_at_max_power.value)\n",
    "                features['bls_depth'] = float(bls.depth_at_max_power.value)\n",
    "                features['bls_snr'] = float(bls.max_power.value)\n",
    "            except Exception as e:\n",
    "                # Fallback to input values\n",
    "                features['bls_period'] = float(period)\n",
    "                features['bls_t0'] = features['input_epoch']\n",
    "                features['bls_duration'] = float(duration)\n",
    "                features['bls_depth'] = float(depth)\n",
    "                features['bls_snr'] = 10.0\n",
    "        else:\n",
    "            features['bls_period'] = float(period)\n",
    "            features['bls_t0'] = features['input_epoch']\n",
    "            features['bls_duration'] = float(duration)\n",
    "            features['bls_depth'] = float(depth)\n",
    "            features['bls_snr'] = 10.0\n",
    "\n",
    "        # 4. Advanced features (4 features)\n",
    "        # Duration ratio\n",
    "        features['duration_over_period'] = float(features['bls_duration'] / features['bls_period'])\n",
    "\n",
    "        # Odd-even depth difference\n",
    "        try:\n",
    "            transit_number = np.floor((time - features['bls_t0']) / features['bls_period']).astype(int)\n",
    "            phase = ((time - features['bls_t0']) % features['bls_period']) / features['bls_period']\n",
    "            phase[phase > 0.5] -= 1.0\n",
    "            in_transit = np.abs(phase) < (features['bls_duration'] / features['bls_period'] / 2)\n",
    "\n",
    "            odd_transits = (transit_number % 2 == 1) & in_transit\n",
    "            even_transits = (transit_number % 2 == 0) & in_transit\n",
    "\n",
    "            if np.sum(odd_transits) > 0 and np.sum(even_transits) > 0:\n",
    "                odd_depth = 1.0 - np.median(flux[odd_transits])\n",
    "                even_depth = 1.0 - np.median(flux[even_transits])\n",
    "                features['odd_even_depth_diff'] = float(abs(odd_depth - even_depth))\n",
    "            else:\n",
    "                features['odd_even_depth_diff'] = 0.0\n",
    "        except:\n",
    "            features['odd_even_depth_diff'] = 0.0\n",
    "\n",
    "        # Transit symmetry\n",
    "        try:\n",
    "            phase = ((time - features['bls_t0']) % features['bls_period']) / features['bls_period']\n",
    "            phase[phase > 0.5] -= 1.0\n",
    "            half_duration_phase = (features['bls_duration'] / features['bls_period']) / 2.0\n",
    "            in_transit = np.abs(phase) < half_duration_phase\n",
    "\n",
    "            if np.sum(in_transit) >= 10:\n",
    "                transit_phase = phase[in_transit]\n",
    "                transit_flux = flux[in_transit]\n",
    "                ingress = transit_phase < 0\n",
    "                egress = transit_phase > 0\n",
    "\n",
    "                if np.sum(ingress) > 1 and np.sum(egress) > 1:\n",
    "                    ingress_slope = np.mean(np.diff(transit_flux[ingress]))\n",
    "                    egress_slope = np.mean(np.diff(transit_flux[egress]))\n",
    "                    symmetry = abs(ingress_slope + egress_slope) / (abs(ingress_slope) + abs(egress_slope) + 1e-10)\n",
    "                    features['transit_symmetry'] = float(min(symmetry, 1.0))\n",
    "                else:\n",
    "                    features['transit_symmetry'] = 0.5\n",
    "            else:\n",
    "                features['transit_symmetry'] = 0.5\n",
    "        except:\n",
    "            features['transit_symmetry'] = 0.5\n",
    "\n",
    "        # Periodicity strength\n",
    "        try:\n",
    "            phase = ((time - np.min(time)) % features['bls_period']) / features['bls_period']\n",
    "            n_bins = 20\n",
    "            phase_bins = np.linspace(0, 1, n_bins + 1)\n",
    "            binned_flux = []\n",
    "\n",
    "            for i in range(n_bins):\n",
    "                mask = (phase >= phase_bins[i]) & (phase < phase_bins[i + 1])\n",
    "                if np.sum(mask) > 0:\n",
    "                    binned_flux.append(np.median(flux[mask]))\n",
    "\n",
    "            if len(binned_flux) > 5:\n",
    "                variation = np.std(binned_flux)\n",
    "                noise = features['flux_std']\n",
    "                features['periodicity_strength'] = float(min(variation / (noise + 1e-10), 1.0))\n",
    "            else:\n",
    "                features['periodicity_strength'] = 0.0\n",
    "        except:\n",
    "            features['periodicity_strength'] = 0.0\n",
    "\n",
    "        return features\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Feature extraction error: {e}\")\n",
    "        # Return NaN features on failure\n",
    "        return {key: np.nan for key in [\n",
    "            'input_period', 'input_depth', 'input_duration', 'input_epoch',\n",
    "            'flux_std', 'flux_mad', 'flux_skewness', 'flux_kurtosis',\n",
    "            'bls_period', 'bls_t0', 'bls_duration', 'bls_depth', 'bls_snr',\n",
    "            'duration_over_period', 'odd_even_depth_diff', 'transit_symmetry', 'periodicity_strength'\n",
    "        ]}\n",
    "\n",
    "\n",
    "print(\"‚úÖ Feature extraction functions loaded\")\n",
    "print(\"   Features extracted: 17 total\")\n",
    "print(\"   - Input parameters: 4\")\n",
    "print(\"   - Flux statistics: 4\")\n",
    "print(\"   - BLS features: 5\")\n",
    "print(\"   - Advanced features: 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell6-header"
   },
   "source": [
    "## üöÄ Cell 6: Batch Processing Function\n",
    "\n",
    "Main extraction pipeline with checkpoint recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell6"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "def extract_features_batch(\n",
    "    samples_df: pd.DataFrame,\n",
    "    checkpoint_mgr: CheckpointManager,\n",
    "    batch_size: int = 100,\n",
    "    run_bls: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process samples in batches with checkpoint saving\n",
    "\n",
    "    Args:\n",
    "        samples_df: Input dataset with exoplanet candidates\n",
    "        checkpoint_mgr: CheckpointManager instance\n",
    "        batch_size: Samples per checkpoint\n",
    "        run_bls: Whether to run BLS search (slower but more accurate)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with extracted features\n",
    "    \"\"\"\n",
    "    # Check for existing progress\n",
    "    completed_indices = checkpoint_mgr.get_completed_indices()\n",
    "    start_idx = len(completed_indices)\n",
    "\n",
    "    if start_idx > 0:\n",
    "        print(f\"\\nüîÑ Resuming from index {start_idx}\")\n",
    "        print(f\"   Already completed: {start_idx}/{len(samples_df)}\")\n",
    "    else:\n",
    "        print(f\"\\nüöÄ Starting fresh extraction\")\n",
    "\n",
    "    # Process batches\n",
    "    total_batches = (len(samples_df) - start_idx + batch_size - 1) // batch_size\n",
    "\n",
    "    for batch_num in range(total_batches):\n",
    "        batch_start = start_idx + (batch_num * batch_size)\n",
    "        batch_end = min(batch_start + batch_size, len(samples_df))\n",
    "        batch = samples_df.iloc[batch_start:batch_end]\n",
    "\n",
    "        print(f\"\\nüì¶ Batch {batch_num + 1}/{total_batches} (samples {batch_start}-{batch_end})\")\n",
    "\n",
    "        batch_features = {}\n",
    "        failed_indices = []\n",
    "        batch_start_time = time.time()\n",
    "\n",
    "        for idx, row in tqdm(batch.iterrows(), total=len(batch), desc=\"Processing\"):\n",
    "            # Skip if already completed\n",
    "            if idx in completed_indices:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Download light curve from MAST\n",
    "                target_id = str(row['target_id']).replace('TIC', '')\n",
    "                \n",
    "                try:\n",
    "                    search_result = lk.search_lightcurve(f'TIC {target_id}', mission='TESS')\n",
    "                    if len(search_result) == 0:\n",
    "                        raise ValueError(f\"No light curves found for TIC {target_id}\")\n",
    "                    \n",
    "                    lc = search_result[0].download()\n",
    "                    lc = lc.remove_nans().normalize()\n",
    "                    \n",
    "                    time_arr = lc.time.value\n",
    "                    flux_arr = lc.flux.value\n",
    "\n",
    "                except Exception as e:\n",
    "                    # Fallback: generate synthetic light curve from parameters\n",
    "                    print(f\"\\n‚ö†Ô∏è Using synthetic LC for sample {idx}: {e}\")\n",
    "                    time_arr = np.linspace(0, 27.4, 1000)  # TESS sector length\n",
    "                    flux_arr = np.ones_like(time_arr) + np.random.normal(0, 0.001, len(time_arr))\n",
    "                    \n",
    "                    # Add synthetic transits\n",
    "                    period = row['period']\n",
    "                    depth = row['depth'] / 1e6  # Convert ppm to relative flux\n",
    "                    duration = row['duration'] / 24  # Convert hours to days\n",
    "                    \n",
    "                    for transit_time in np.arange(duration, time_arr[-1], period):\n",
    "                        in_transit = np.abs(time_arr - transit_time) < (duration / 2)\n",
    "                        flux_arr[in_transit] *= (1 - depth)\n",
    "\n",
    "                # Extract features\n",
    "                features = extract_features_from_lightcurve(\n",
    "                    time=time_arr,\n",
    "                    flux=flux_arr,\n",
    "                    period=row['period'],\n",
    "                    duration=row['duration'] / 24,  # hours to days\n",
    "                    epoch=row.get('epoch', time_arr[0]),\n",
    "                    depth=row['depth'] / 1e6,  # ppm to relative\n",
    "                    run_bls=run_bls\n",
    "                )\n",
    "\n",
    "                # Add metadata\n",
    "                features['sample_idx'] = int(idx)\n",
    "                features['label'] = int(row['label'])\n",
    "                features['target_id'] = str(row['target_id'])\n",
    "                features['toi'] = str(row.get('toi', 'unknown'))\n",
    "\n",
    "                batch_features[int(idx)] = features\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå Failed sample {idx}: {e}\")\n",
    "                failed_indices.append(int(idx))\n",
    "                continue\n",
    "\n",
    "        # Save checkpoint\n",
    "        batch_time = time.time() - batch_start_time\n",
    "        metadata = {\n",
    "            'batch_num': batch_num + 1,\n",
    "            'total_batches': total_batches,\n",
    "            'processing_time_sec': batch_time,\n",
    "            'samples_per_sec': len(batch_features) / batch_time if batch_time > 0 else 0\n",
    "        }\n",
    "\n",
    "        checkpoint_mgr.save_checkpoint(\n",
    "            batch_id=batch_start,\n",
    "            features=batch_features,\n",
    "            failed_indices=failed_indices,\n",
    "            metadata=metadata\n",
    "        )\n",
    "\n",
    "        # Update completed indices\n",
    "        completed_indices.update(batch_features.keys())\n",
    "\n",
    "        # Progress summary\n",
    "        progress = checkpoint_mgr.get_progress_summary(len(samples_df))\n",
    "        print(f\"\\nüìä Progress: {progress['completed']}/{progress['total_samples']} ({progress['success_rate']:.1f}%)\")\n",
    "        print(f\"   Failed: {progress['failed']}\")\n",
    "        print(f\"   Remaining: {progress['remaining']}\")\n",
    "        print(f\"   Speed: {metadata['samples_per_sec']:.2f} samples/sec\")\n",
    "\n",
    "        # ETA calculation\n",
    "        if progress['remaining'] > 0 and metadata['samples_per_sec'] > 0:\n",
    "            eta_sec = progress['remaining'] / metadata['samples_per_sec']\n",
    "            eta_min = eta_sec / 60\n",
    "            print(f\"   ETA: {eta_min:.1f} minutes\")\n",
    "\n",
    "    print(\"\\n‚úÖ All batches completed!\")\n",
    "    return checkpoint_mgr.merge_all_checkpoints()\n",
    "\n",
    "\n",
    "print(\"‚úÖ Batch processing function loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell7-header"
   },
   "source": [
    "## ‚ö° Cell 7: Execute Feature Extraction\n",
    "\n",
    "Start the extraction process (auto-resumes from last checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell7"
   },
   "outputs": [],
   "source": [
    "# Initialize checkpoint manager\n",
    "checkpoint_mgr = CheckpointManager(\n",
    "    drive_path=str(BASE_DIR),\n",
    "    batch_size=100\n",
    ")\n",
    "\n",
    "# Check existing progress\n",
    "progress = checkpoint_mgr.get_progress_summary(len(samples_df))\n",
    "print(f\"üìä Current Progress:\")\n",
    "print(f\"   Total samples: {progress['total_samples']}\")\n",
    "print(f\"   Completed: {progress['completed']}\")\n",
    "print(f\"   Failed: {progress['failed']}\")\n",
    "print(f\"   Remaining: {progress['remaining']}\")\n",
    "\n",
    "if progress['completed'] > 0:\n",
    "    print(f\"\\n‚úÖ Found existing checkpoints!\")\n",
    "    user_input = input(\"Continue from last checkpoint? (yes/no): \")\n",
    "    if user_input.lower() != 'yes':\n",
    "        print(\"Aborted. To start fresh, delete checkpoint files manually.\")\n",
    "    else:\n",
    "        # Start/resume extraction\n",
    "        features_df = extract_features_batch(\n",
    "            samples_df=samples_df,\n",
    "            checkpoint_mgr=checkpoint_mgr,\n",
    "            batch_size=100,\n",
    "            run_bls=True  # Set to False for faster processing without BLS\n",
    "        )\n",
    "else:\n",
    "    # Start fresh extraction\n",
    "    features_df = extract_features_batch(\n",
    "        samples_df=samples_df,\n",
    "        checkpoint_mgr=checkpoint_mgr,\n",
    "        batch_size=100,\n",
    "        run_bls=True\n",
    "    )\n",
    "\n",
    "# Save final results\n",
    "output_file = OUTPUT_DIR / 'bls_tls_features.csv'\n",
    "features_df.to_csv(output_file, index=False)\n",
    "print(f\"\\n‚úÖ Complete! Saved to: {output_file}\")\n",
    "print(f\"   Total features extracted: {len(features_df)}\")\n",
    "print(f\"   Feature columns: {len(features_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell8-header"
   },
   "source": [
    "## üìä Cell 8: Progress Monitoring Dashboard\n",
    "\n",
    "Real-time progress tracking (run in separate notebook tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell8"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, HTML, display\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def monitor_progress(checkpoint_mgr, total_samples, update_interval=30):\n",
    "    \"\"\"\n",
    "    Real-time progress monitoring with visualization\n",
    "\n",
    "    Args:\n",
    "        checkpoint_mgr: CheckpointManager instance\n",
    "        total_samples: Total number of samples\n",
    "        update_interval: Update frequency in seconds\n",
    "    \"\"\"\n",
    "    try:\n",
    "        while True:\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            progress = checkpoint_mgr.get_progress_summary(total_samples)\n",
    "\n",
    "            # Progress bar\n",
    "            completed_pct = progress['success_rate']\n",
    "            bar_width = 50\n",
    "            filled = int(bar_width * completed_pct / 100)\n",
    "            bar = '‚ñà' * filled + '‚ñë' * (bar_width - filled)\n",
    "\n",
    "            # Display stats\n",
    "            print(f\"üöÄ Feature Extraction Progress\")\n",
    "            print(f\"{'=' * 60}\")\n",
    "            print(f\"[{bar}] {completed_pct:.1f}%\")\n",
    "            print(f\"\")\n",
    "            print(f\"‚úÖ Completed:  {progress['completed']:,} / {total_samples:,}\")\n",
    "            print(f\"‚ùå Failed:     {progress['failed']:,}\")\n",
    "            print(f\"‚è≥ Remaining:  {progress['remaining']:,}\")\n",
    "            print(f\"\")\n",
    "            print(f\"üìà Success Rate: {progress['success_rate']:.2f}%\")\n",
    "            print(f\"üìâ Failure Rate: {progress['failure_rate']:.2f}%\")\n",
    "            print(f\"\")\n",
    "            print(f\"‚è∞ Last update: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            print(f\"{'=' * 60}\")\n",
    "\n",
    "            # Visualization\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(10, 3))\n",
    "            categories = ['Completed', 'Failed', 'Remaining']\n",
    "            values = [progress['completed'], progress['failed'], progress['remaining']]\n",
    "            colors = ['#4CAF50', '#F44336', '#FFC107']\n",
    "\n",
    "            ax.barh(categories, values, color=colors)\n",
    "            ax.set_xlabel('Number of Samples')\n",
    "            ax.set_title('Processing Status')\n",
    "            ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "            for i, v in enumerate(values):\n",
    "                ax.text(v, i, f' {v:,}', va='center')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Check if complete\n",
    "            if progress['remaining'] == 0:\n",
    "                print(\"\\n‚úÖ PROCESSING COMPLETE!\")\n",
    "                break\n",
    "\n",
    "            time.sleep(update_interval)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚èπÔ∏è Monitoring stopped\")\n",
    "\n",
    "\n",
    "# Run monitor\n",
    "monitor_progress(checkpoint_mgr, len(samples_df), update_interval=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell9-header"
   },
   "source": [
    "## üîç Cell 9: Validate Results\n",
    "\n",
    "Check feature extraction quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell9"
   },
   "outputs": [],
   "source": [
    "# Load results\n",
    "results_file = OUTPUT_DIR / 'bls_tls_features.csv'\n",
    "if results_file.exists():\n",
    "    features_df = pd.read_csv(results_file)\n",
    "\n",
    "    print(\"üìä Feature Extraction Summary\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"Total samples: {len(features_df)}\")\n",
    "    print(f\"Total features: {len(features_df.columns)}\")\n",
    "    print(f\"\")\n",
    "    print(f\"Feature columns:\")\n",
    "    for col in features_df.columns:\n",
    "        if col not in ['sample_idx', 'label', 'target_id', 'toi']:\n",
    "            null_count = features_df[col].isna().sum()\n",
    "            print(f\"  - {col}: {null_count} NaN values\")\n",
    "    print(f\"\")\n",
    "    print(f\"Label distribution:\")\n",
    "    print(features_df['label'].value_counts())\n",
    "    print(f\"\")\n",
    "    print(f\"First 5 rows:\")\n",
    "    display(features_df.head())\n",
    "\n",
    "    # Check for failed samples\n",
    "    failed_indices = checkpoint_mgr.get_failed_indices()\n",
    "    if failed_indices:\n",
    "        print(f\"\\n‚ùå Failed samples: {len(failed_indices)}\")\n",
    "        print(f\"   Indices: {failed_indices[:10]}...\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ No failed samples!\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Results file not found. Run Cell 7 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell10-header"
   },
   "source": [
    "## üßπ Cell 10: Cleanup (Optional)\n",
    "\n",
    "Remove checkpoint files after successful extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell10"
   },
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è WARNING: This will delete all checkpoint files!\n",
    "# Only run after verifying final results\n",
    "\n",
    "user_confirm = input(\"Delete all checkpoint files? (yes/no): \")\n",
    "if user_confirm.lower() == 'yes':\n",
    "    checkpoint_mgr.cleanup_checkpoints()\n",
    "    print(\"‚úÖ Checkpoints cleaned up\")\n",
    "else:\n",
    "    print(\"Cleanup cancelled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell11-header"
   },
   "source": [
    "## üì• Cell 11: Download Results\n",
    "\n",
    "Download final CSV to local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell11"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Download final results\n",
    "results_file = OUTPUT_DIR / 'bls_tls_features.csv'\n",
    "if results_file.exists():\n",
    "    files.download(str(results_file))\n",
    "    print(f\"‚úÖ Downloaded: {results_file.name}\")\n",
    "else:\n",
    "    print(\"‚ùå No results file found\")\n",
    "\n",
    "# Download failed samples log\n",
    "failed_indices = checkpoint_mgr.get_failed_indices()\n",
    "if failed_indices:\n",
    "    failed_df = pd.DataFrame({'sample_idx': failed_indices})\n",
    "    failed_file = OUTPUT_DIR / 'failed_samples.csv'\n",
    "    failed_df.to_csv(failed_file, index=False)\n",
    "    files.download(str(failed_file))\n",
    "    print(f\"‚úÖ Downloaded: failed_samples.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "footer"
   },
   "source": [
    "---\n",
    "\n",
    "## üìù Usage Instructions\n",
    "\n",
    "### First Run:\n",
    "1. **Cell 1**: Install packages ‚Üí **RESTART RUNTIME**\n",
    "2. **Cell 2**: Mount Google Drive\n",
    "3. **Cell 3**: Load CheckpointManager\n",
    "4. **Cell 4**: Upload dataset to Drive or use file upload\n",
    "5. **Cell 5-6**: Load feature extraction functions\n",
    "6. **Cell 7**: Start extraction (takes ~5-10 hours for 11,979 samples)\n",
    "7. **Cell 8**: (Optional) Monitor progress in separate tab\n",
    "\n",
    "### After Disconnect:\n",
    "1. Run Cell 1 ‚Üí **RESTART RUNTIME**\n",
    "2. Run Cells 2-6 sequentially\n",
    "3. Run Cell 7 ‚Üí Auto-resumes from last checkpoint\n",
    "\n",
    "### Checkpoints:\n",
    "- Saved every 100 samples to Google Drive\n",
    "- Location: `/content/drive/MyDrive/exoplanet-spaceapps/checkpoints/`\n",
    "- Auto-recovery on disconnect\n",
    "\n",
    "### Output:\n",
    "- **Features CSV**: 17 features √ó 11,979 samples\n",
    "- **Location**: `/content/drive/MyDrive/exoplanet-spaceapps/results/bls_tls_features.csv`\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Test Mode (Quick Test)\n",
    "\n",
    "To test with fewer samples:\n",
    "```python\n",
    "# In Cell 4, add:\n",
    "samples_df = samples_df.head(200)  # Test with 200 samples\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üêõ Troubleshooting\n",
    "\n",
    "**Problem**: `RuntimeError: NumPy 2.0 incompatibility`\n",
    "- **Solution**: Restart runtime after Cell 1\n",
    "\n",
    "**Problem**: `FileNotFoundError: supervised_dataset.csv`\n",
    "- **Solution**: Upload CSV to Google Drive at specified location\n",
    "\n",
    "**Problem**: Slow processing (< 0.5 samples/sec)\n",
    "- **Solution**: Set `run_bls=False` in Cell 7 for faster processing\n",
    "\n",
    "**Problem**: Colab disconnects frequently\n",
    "- **Solution**: Keep tab active, enable notifications, or use Colab Pro\n",
    "\n",
    "---\n",
    "\n",
    "**Version**: 1.0.0  \n",
    "**Last Updated**: 2025-01-29  \n",
    "**Author**: Exoplanet Detection Team\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "02_bls_baseline_COLAB.ipynb",
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}