{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Â· BLS Baseline (æ‰¹æ¬¡è™•ç†ç‰ˆ) - ä½¿ç”¨ 01 ä¸‹è¼‰çš„è³‡æ–™\n",
    "\n",
    "## æ”¹é€²å…§å®¹\n",
    "- âœ… è®€å– 01_tap_download å„²å­˜çš„ CSV è³‡æ–™\n",
    "- âœ… æ‰¹é‡è™•ç† TOI å’Œ KOI ç›®æ¨™\n",
    "- âœ… è¨ˆç®— BLS/TLS ç‰¹å¾µä¸¦å„²å­˜\n",
    "- âœ… æ”¯æ´ Google Drive å’Œæœ¬åœ°å„²å­˜\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç’°å¢ƒè¨­å®šèˆ‡ NumPy ä¿®å¾©\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"ğŸ“ åµæ¸¬åˆ° Google Colab ç’°å¢ƒ\")\n",
    "    print(\"ğŸ”§ å®‰è£ç›¸å®¹ç‰ˆæœ¬å¥—ä»¶...\")\n",
    "    !pip install -q numpy==1.26.4 pandas astropy scipy'<1.13' matplotlib scikit-learn\n",
    "    !pip install -q lightkurve astroquery transitleastsquares wotan\n",
    "    print(\"âœ… å¥—ä»¶å®‰è£å®Œæˆ!\")\n",
    "    print(\"âš ï¸ è«‹ç¾åœ¨æ‰‹å‹•é‡å•Ÿ Runtime: Runtime â†’ Restart runtime\")\n",
    "    print(\"   ç„¶å¾Œå¾ä¸‹ä¸€å€‹ cell ç¹¼çºŒåŸ·è¡Œ\")\n",
    "else:\n",
    "    print(\"ğŸ’» æœ¬åœ°ç’°å¢ƒï¼Œè·³éå¥—ä»¶å®‰è£\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. è¼‰å…¥ 01 ä¸‹è¼‰çš„è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# æª¢æŸ¥è³‡æ–™ä¾†æº\n",
    "if IN_COLAB:\n",
    "    # å˜—è©¦å¾ Google Drive è¼‰å…¥\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        \n",
    "        # Google Drive è³‡æ–™è·¯å¾‘\n",
    "        drive_data_dir = Path('/content/drive/MyDrive/spaceapps-exoplanet/data/latest')\n",
    "        if drive_data_dir.exists():\n",
    "            data_dir = drive_data_dir\n",
    "            print(f\"âœ… ä½¿ç”¨ Google Drive è³‡æ–™: {data_dir}\")\n",
    "        else:\n",
    "            data_dir = Path('../data')\n",
    "            print(f\"âš ï¸ Google Drive ç„¡è³‡æ–™ï¼Œä½¿ç”¨æœ¬åœ°: {data_dir}\")\n",
    "    except:\n",
    "        data_dir = Path('../data')\n",
    "        print(f\"ğŸ’» ä½¿ç”¨æœ¬åœ°è³‡æ–™ç›®éŒ„: {data_dir}\")\n",
    "else:\n",
    "    data_dir = Path('../data')\n",
    "    print(f\"ğŸ’» ä½¿ç”¨æœ¬åœ°è³‡æ–™ç›®éŒ„: {data_dir}\")\n",
    "\n",
    "# è¼‰å…¥è³‡æ–™\n",
    "print(\"\\nğŸ“‚ è¼‰å…¥è³‡æ–™æª”æ¡ˆ...\")\n",
    "\n",
    "# è¼‰å…¥åˆä½µçš„è¨“ç·´è³‡æ–™é›†\n",
    "supervised_file = data_dir / 'supervised_dataset.csv'\n",
    "if supervised_file.exists():\n",
    "    supervised_df = pd.read_csv(supervised_file)\n",
    "    print(f\"   âœ… è¼‰å…¥ supervised_dataset.csv: {len(supervised_df)} ç­†\")\n",
    "    \n",
    "    # é¡¯ç¤ºè³‡æ–™æ‘˜è¦\n",
    "    print(f\"\\nğŸ“Š è³‡æ–™æ‘˜è¦:\")\n",
    "    print(f\"   æ­£æ¨£æœ¬ (label=1): {(supervised_df['label']==1).sum()} ç­†\")\n",
    "    print(f\"   è² æ¨£æœ¬ (label=0): {(supervised_df['label']==0).sum()} ç­†\")\n",
    "    \n",
    "    if 'source' in supervised_df.columns:\n",
    "        print(f\"\\n   è³‡æ–™ä¾†æºåˆ†å¸ƒ:\")\n",
    "        for source, count in supervised_df['source'].value_counts().items():\n",
    "            print(f\"   - {source}: {count} ç­†\")\n",
    "else:\n",
    "    print(\"âŒ æ‰¾ä¸åˆ° supervised_dataset.csv\")\n",
    "    print(\"   è«‹å…ˆåŸ·è¡Œ 01_tap_download.ipynb ä¸‹è¼‰è³‡æ–™\")\n",
    "    raise FileNotFoundError(\"Missing supervised_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ç¯©é¸å¯è™•ç†çš„ç›®æ¨™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¯©é¸æœ‰ TIC æˆ– KIC ID çš„ç›®æ¨™\n",
    "print(\"\\nğŸ” ç¯©é¸å¯è™•ç†çš„ç›®æ¨™...\")\n",
    "\n",
    "# æå– TIC ç›®æ¨™ (TESS)\n",
    "tess_targets = supervised_df[supervised_df['target_id'].str.startswith('TIC', na=False)].copy()\n",
    "print(f\"   TESS ç›®æ¨™ (TIC): {len(tess_targets)} ç­†\")\n",
    "\n",
    "# æå– KIC ç›®æ¨™ (Kepler)\n",
    "kepler_targets = supervised_df[supervised_df['target_id'].str.startswith('KIC', na=False)].copy()\n",
    "print(f\"   Kepler ç›®æ¨™ (KIC): {len(kepler_targets)} ç­†\")\n",
    "\n",
    "# åˆä½µç›®æ¨™\n",
    "processable_targets = pd.concat([tess_targets, kepler_targets])\n",
    "print(f\"\\nâœ… ç¸½å…±å¯è™•ç†ç›®æ¨™: {len(processable_targets)} ç­†\")\n",
    "\n",
    "# ç‚ºäº†ç¤ºç¯„ï¼Œå…ˆè™•ç†å‰ 10 å€‹ç›®æ¨™\n",
    "# å¯¦éš›åŸ·è¡Œæ™‚å¯ä»¥å¢åŠ æ•¸é‡æˆ–ç§»é™¤é™åˆ¶\n",
    "SAMPLE_SIZE = 10\n",
    "sample_targets = processable_targets.head(SAMPLE_SIZE)\n",
    "print(f\"\\nğŸ“Œ æœ¬æ¬¡ç¤ºç¯„è™•ç†å‰ {SAMPLE_SIZE} å€‹ç›®æ¨™\")\n",
    "print(f\"   (å¯ä¿®æ”¹ SAMPLE_SIZE è™•ç†æ›´å¤šç›®æ¨™)\")\n",
    "\n",
    "# é¡¯ç¤ºæ¨£æœ¬\n",
    "print(\"\\næ¨£æœ¬ç›®æ¨™:\")\n",
    "print(sample_targets[['target_id', 'label', 'source', 'period', 'depth']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æ‰¹é‡ä¸‹è¼‰å…‰æ›²ç·šä¸¦è¨ˆç®— BLS ç‰¹å¾µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightkurve as lk\n",
    "import time\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "def process_target_with_bls(\n",
    "    target_id: str,\n",
    "    label: int,\n",
    "    known_period: Optional[float] = None\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    ä¸‹è¼‰å…‰æ›²ç·šä¸¦è¨ˆç®— BLS ç‰¹å¾µ\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    target_id : str\n",
    "        TIC æˆ– KIC ID\n",
    "    label : int\n",
    "        æ¨™ç±¤ (0 æˆ– 1)\n",
    "    known_period : float, optional\n",
    "        å·²çŸ¥é€±æœŸï¼ˆå¦‚æœæœ‰ï¼‰\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : ç‰¹å¾µå­—å…¸ï¼Œå¦‚æœå¤±æ•—å‰‡è¿”å› None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # æ±ºå®šä»»å‹™é¡å‹\n",
    "        if target_id.startswith('TIC'):\n",
    "            mission = 'TESS'\n",
    "            search_id = target_id.replace('TIC', 'TIC ')\n",
    "        elif target_id.startswith('KIC'):\n",
    "            mission = 'Kepler'\n",
    "            search_id = target_id.replace('KIC', 'KIC ')\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        # æœå°‹å…‰æ›²ç·š\n",
    "        search_result = lk.search_lightcurve(\n",
    "            search_id,\n",
    "            mission=mission,\n",
    "            cadence='short',\n",
    "            author='SPOC' if mission == 'TESS' else None\n",
    "        )\n",
    "        \n",
    "        if len(search_result) == 0:\n",
    "            print(f\"   âš ï¸ {target_id}: ç„¡å…‰æ›²ç·šè³‡æ–™\")\n",
    "            return None\n",
    "        \n",
    "        # ä¸‹è¼‰ç¬¬ä¸€å€‹ sector/quarter\n",
    "        lc = search_result[0].download()\n",
    "        \n",
    "        # å¦‚æœæ˜¯ collectionï¼Œå–ç¬¬ä¸€å€‹\n",
    "        if hasattr(lc, '__iter__'):\n",
    "            lc = lc[0]\n",
    "        \n",
    "        # æ¸…ç†å’Œå»è¶¨å‹¢\n",
    "        lc_clean = lc.remove_nans()\n",
    "        lc_flat = lc_clean.flatten(window_length=401)\n",
    "        \n",
    "        # åŸ·è¡Œ BLS\n",
    "        bls = lc_flat.to_periodogram(\n",
    "            method=\"bls\",\n",
    "            minimum_period=0.5,\n",
    "            maximum_period=20.0,\n",
    "            frequency_factor=5.0\n",
    "        )\n",
    "        \n",
    "        # æå–ç‰¹å¾µ\n",
    "        features = {\n",
    "            'target_id': target_id,\n",
    "            'label': label,\n",
    "            'mission': mission,\n",
    "            'n_points': len(lc_flat.time),\n",
    "            'bls_period': bls.period_at_max_power.value if hasattr(bls.period_at_max_power, 'value') else bls.period_at_max_power,\n",
    "            'bls_depth': bls.depth_at_max_power.value if hasattr(bls.depth_at_max_power, 'value') else bls.depth_at_max_power,\n",
    "            'bls_duration': bls.duration_at_max_power.value if hasattr(bls.duration_at_max_power, 'value') else bls.duration_at_max_power,\n",
    "            'bls_snr': bls.max_power.value if hasattr(bls.max_power, 'value') else bls.max_power,\n",
    "            'known_period': known_period,\n",
    "            'flux_std': np.std(lc_flat.flux.value if hasattr(lc_flat.flux, 'value') else lc_flat.flux),\n",
    "            'flux_mad': np.median(np.abs(lc_flat.flux.value if hasattr(lc_flat.flux, 'value') else lc_flat.flux - np.median(lc_flat.flux.value if hasattr(lc_flat.flux, 'value') else lc_flat.flux)))\n",
    "        }\n",
    "        \n",
    "        # è¨ˆç®—é€±æœŸæº–ç¢ºåº¦ï¼ˆå¦‚æœæœ‰å·²çŸ¥é€±æœŸï¼‰\n",
    "        if known_period and not pd.isna(known_period) and known_period > 0:\n",
    "            features['period_accuracy'] = abs(features['bls_period'] - known_period) / known_period\n",
    "        else:\n",
    "            features['period_accuracy'] = None\n",
    "        \n",
    "        print(f\"   âœ… {target_id}: BLS é€±æœŸ={features['bls_period']:.3f}å¤©, SNR={features['bls_snr']:.2f}\")\n",
    "        return features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ {target_id}: è™•ç†å¤±æ•— - {str(e)[:50]}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰¹é‡è™•ç†ç›®æ¨™\n",
    "print(\"\\nğŸš€ é–‹å§‹æ‰¹é‡è™•ç†ç›®æ¨™...\\n\")\n",
    "\n",
    "features_list = []\n",
    "success_count = 0\n",
    "fail_count = 0\n",
    "\n",
    "for idx, row in sample_targets.iterrows():\n",
    "    target_id = row['target_id']\n",
    "    label = row['label']\n",
    "    known_period = row.get('period', None)\n",
    "    \n",
    "    print(f\"è™•ç† {idx+1}/{len(sample_targets)}: {target_id}\")\n",
    "    \n",
    "    # è™•ç†ç›®æ¨™\n",
    "    features = process_target_with_bls(target_id, label, known_period)\n",
    "    \n",
    "    if features:\n",
    "        features_list.append(features)\n",
    "        success_count += 1\n",
    "    else:\n",
    "        fail_count += 1\n",
    "    \n",
    "    # é¿å… API é™åˆ¶\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"\\nğŸ“Š è™•ç†çµæœ:\")\n",
    "print(f\"   æˆåŠŸ: {success_count} å€‹\")\n",
    "print(f\"   å¤±æ•—: {fail_count} å€‹\")\n",
    "print(f\"   æˆåŠŸç‡: {success_count/len(sample_targets)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ç‰¹å¾µåˆ†æèˆ‡è¦–è¦ºåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if features_list:\n",
    "    # å»ºç«‹ç‰¹å¾µ DataFrame\n",
    "    features_df = pd.DataFrame(features_list)\n",
    "    print(\"\\nğŸ“‹ ç‰¹å¾µè³‡æ–™æ‘˜è¦:\")\n",
    "    print(features_df[['target_id', 'label', 'bls_period', 'bls_snr', 'bls_depth']].head(10))\n",
    "    \n",
    "    # è¦–è¦ºåŒ–ï¼šBLS é€±æœŸåˆ†å¸ƒ\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # é€±æœŸåˆ†å¸ƒï¼ˆæŒ‰æ¨™ç±¤ï¼‰\n",
    "    ax1 = axes[0]\n",
    "    for label in [0, 1]:\n",
    "        data = features_df[features_df['label'] == label]['bls_period']\n",
    "        if len(data) > 0:\n",
    "            ax1.hist(data, bins=20, alpha=0.6, \n",
    "                    label=f\"Label {label} (n={len(data)})\")\n",
    "    ax1.set_xlabel('BLS é€±æœŸ (å¤©)')\n",
    "    ax1.set_ylabel('æ•¸é‡')\n",
    "    ax1.set_title('BLS é€±æœŸåˆ†å¸ƒ')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # SNR åˆ†å¸ƒï¼ˆæŒ‰æ¨™ç±¤ï¼‰\n",
    "    ax2 = axes[1]\n",
    "    for label in [0, 1]:\n",
    "        data = features_df[features_df['label'] == label]['bls_snr']\n",
    "        if len(data) > 0:\n",
    "            ax2.hist(data, bins=20, alpha=0.6,\n",
    "                    label=f\"Label {label} (n={len(data)})\")\n",
    "    ax2.set_xlabel('BLS SNR')\n",
    "    ax2.set_ylabel('æ•¸é‡')\n",
    "    ax2.set_title('BLS SNR åˆ†å¸ƒ')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # çµ±è¨ˆåˆ†æ\n",
    "    print(\"\\nğŸ“Š ç‰¹å¾µçµ±è¨ˆï¼ˆæŒ‰æ¨™ç±¤ï¼‰:\")\n",
    "    print(\"\\næ­£æ¨£æœ¬ (Label=1):\")\n",
    "    pos_features = features_df[features_df['label'] == 1]\n",
    "    if len(pos_features) > 0:\n",
    "        print(pos_features[['bls_period', 'bls_snr', 'bls_depth']].describe())\n",
    "    \n",
    "    print(\"\\nè² æ¨£æœ¬ (Label=0):\")\n",
    "    neg_features = features_df[features_df['label'] == 0]\n",
    "    if len(neg_features) > 0:\n",
    "        print(neg_features[['bls_period', 'bls_snr', 'bls_depth']].describe())\n",
    "else:\n",
    "    print(\"âš ï¸ æ²’æœ‰æˆåŠŸè™•ç†çš„ç›®æ¨™\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å„²å­˜ç‰¹å¾µè³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if features_list:\n",
    "    # å„²å­˜ç‰¹å¾µ\n",
    "    output_dir = Path('../data')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    output_file = output_dir / 'bls_features.csv'\n",
    "    features_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nğŸ’¾ ç‰¹å¾µå·²å„²å­˜è‡³: {output_file}\")\n",
    "    print(f\"   å…± {len(features_df)} ç­†ç‰¹å¾µ\")\n",
    "    \n",
    "    # å¦‚æœåœ¨ Colabï¼Œä¹Ÿå„²å­˜åˆ° Google Drive\n",
    "    if IN_COLAB:\n",
    "        try:\n",
    "            from google.colab import drive\n",
    "            drive_output = Path('/content/drive/MyDrive/spaceapps-exoplanet/data/latest/bls_features.csv')\n",
    "            features_df.to_csv(drive_output, index=False)\n",
    "            print(f\"   âœ… åŒæ™‚å„²å­˜åˆ° Google Drive: {drive_output}\")\n",
    "        except:\n",
    "            print(\"   âš ï¸ ç„¡æ³•å„²å­˜åˆ° Google Drive\")\n",
    "    \n",
    "    # é¡¯ç¤ºå¦‚ä½•ä½¿ç”¨é€™äº›ç‰¹å¾µ\n",
    "    print(\"\\nğŸ’¡ ä¸‹ä¸€æ­¥ï¼š\")\n",
    "    print(\"   1. å¢åŠ  SAMPLE_SIZE è™•ç†æ›´å¤šç›®æ¨™\")\n",
    "    print(\"   2. åŸ·è¡Œ 03_injection_train.ipynb è¨“ç·´åˆ†é¡å™¨\")\n",
    "    print(\"   3. ä½¿ç”¨é€™äº› BLS ç‰¹å¾µé€²è¡Œæ©Ÿå™¨å­¸ç¿’\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ç¸½çµ\n",
    "\n",
    "### âœ… å·²å®Œæˆï¼š\n",
    "1. æˆåŠŸè¼‰å…¥ 01_tap_download çš„è³‡æ–™\n",
    "2. æ‰¹é‡ä¸‹è¼‰å…‰æ›²ç·šä¸¦è¨ˆç®— BLS ç‰¹å¾µ\n",
    "3. å„²å­˜ç‰¹å¾µä¾›å¾ŒçºŒè¨“ç·´ä½¿ç”¨\n",
    "\n",
    "### ğŸ“Š è³‡æ–™æµç¨‹ï¼š\n",
    "```\n",
    "01_tap_download.ipynb\n",
    "    â†“ (11,979 ç­† TOI/KOI è³‡æ–™)\n",
    "supervised_dataset.csv\n",
    "    â†“\n",
    "02_bls_baseline_batch.ipynb (æœ¬ç­†è¨˜æœ¬)\n",
    "    â†“ (ä¸‹è¼‰å…‰æ›²ç·š + BLS åˆ†æ)\n",
    "bls_features.csv\n",
    "    â†“\n",
    "03_injection_train.ipynb (è¨“ç·´æ¨¡å‹)\n",
    "```\n",
    "\n",
    "### ğŸš€ å„ªåŒ–å»ºè­°ï¼š\n",
    "1. **å¹³è¡Œè™•ç†**ï¼šä½¿ç”¨ multiprocessing åŠ é€Ÿ\n",
    "2. **éŒ¯èª¤æ¢å¾©**ï¼šå„²å­˜ä¸­é–“çµæœï¼Œæ”¯æ´æ–·é»çºŒå‚³\n",
    "3. **æ›´å¤šç‰¹å¾µ**ï¼šåŠ å…¥ TLSã€å°æ³¢è®Šæ›ç­‰ç‰¹å¾µ\n",
    "4. **è³‡æ–™å¢å¼·**ï¼šå°æ­£æ¨£æœ¬é€²è¡Œç›¸ä½å¹³ç§»å¢å¼·"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}