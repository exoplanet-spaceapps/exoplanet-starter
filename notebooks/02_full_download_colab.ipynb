{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exoplanet Light Curve Download - Full Scale (Colab Version)\n",
    "\n",
    "**ç›®æ¨™**: ä¸‹è¼‰ 11,979 å€‹ TESS å…‰æ›²ç·šæ¨£æœ¬  \n",
    "**é ä¼°æ™‚é–“**: 6-7 å°æ™‚  \n",
    "**é ä¼°æˆåŠŸç‡**: ~57% (~6,800 æ¨£æœ¬)  \n",
    "**å„²å­˜æ ¼å¼**: HDF5\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ Colab ä½¿ç”¨æ³¨æ„äº‹é …\n",
    "\n",
    "1. **åŸ·è¡Œæ™‚é–“**: æ­¤ notebook éœ€è¦ 6-7 å°æ™‚é€£çºŒåŸ·è¡Œ\n",
    "2. **Colab é™åˆ¶**: å…è²»ç‰ˆ 12 å°æ™‚é™åˆ¶ï¼ŒPro ç‰ˆ 24 å°æ™‚\n",
    "3. **ä¸­æ–·æ¢å¾©**: ä½¿ç”¨ checkpoint ç³»çµ±ï¼Œå¯å¾ä¸­æ–·è™•ç¹¼çºŒ\n",
    "4. **å„²å­˜ç©ºé–“**: å»ºè­°æ›è¼‰ Google Drive (éœ€ç´„ 5-10 GB)\n",
    "5. **ç¶²è·¯ç©©å®š**: ç¢ºä¿ç¶²è·¯é€£ç·šç©©å®š\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ åŸ·è¡Œæ­¥é©Ÿ\n",
    "\n",
    "1. **Cell 1**: æ›è¼‰ Google Drive (å¯é¸)\n",
    "2. **Cell 2**: å®‰è£ä¾è³´å¥—ä»¶\n",
    "3. **Cell 3**: Clone GitHub å€‰åº«\n",
    "4. **Cell 4**: è¨­å®šé…ç½®åƒæ•¸\n",
    "5. **Cell 5**: åŸ·è¡Œä¸‹è¼‰ï¼ˆä¸»ç¨‹å¼ï¼‰\n",
    "6. **Cell 6**: æŸ¥çœ‹çµ±è¨ˆèˆ‡é©—è­‰\n",
    "7. **Cell 7**: ä¸‹è¼‰çµæœåˆ°æœ¬åœ°æˆ–æ¨é€ GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ æ›è¼‰ Google Drive (å¯é¸ä½†å»ºè­°)\n",
    "\n",
    "å°‡ä¸‹è¼‰çš„è³‡æ–™å­˜åˆ° Google Driveï¼Œé¿å… Colab é‡å•Ÿå¾Œéºå¤±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¸é … A: ä½¿ç”¨ Google Drive\n",
    "USE_GDRIVE = True  # æ”¹ç‚º False å‰‡å­˜åœ¨ Colab æœ¬åœ°\n",
    "\n",
    "if USE_GDRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # è¨­å®šå·¥ä½œç›®éŒ„\n",
    "    WORK_DIR = '/content/drive/MyDrive/exoplanet-lightcurves'\n",
    "    !mkdir -p {WORK_DIR}\n",
    "    %cd {WORK_DIR}\n",
    "else:\n",
    "    WORK_DIR = '/content/exoplanet-starter'\n",
    "    !mkdir -p {WORK_DIR}\n",
    "    %cd {WORK_DIR}\n",
    "\n",
    "print(f\"Working directory: {WORK_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ å®‰è£ä¾è³´å¥—ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q lightkurve h5py pandas numpy tqdm pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Clone GitHub å€‰åº«ä¸¦ç²å–æ•¸æ“šé›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Clone å€‰åº«ï¼ˆå¦‚æœå°šæœªå­˜åœ¨ï¼‰\n",
    "if not Path('exoplanet-starter').exists():\n",
    "    !git clone https://github.com/exoplanet-spaceapps/exoplanet-starter.git\n",
    "else:\n",
    "    print(\"Repository already exists, pulling latest changes...\")\n",
    "    !cd exoplanet-starter && git pull\n",
    "\n",
    "# è¨­å®šå°ˆæ¡ˆæ ¹ç›®éŒ„\n",
    "PROJECT_ROOT = Path('exoplanet-starter').resolve()\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "\n",
    "# ç¢ºèªæ•¸æ“šé›†å­˜åœ¨\n",
    "dataset_path = PROJECT_ROOT / 'data' / 'supervised_dataset.csv'\n",
    "if dataset_path.exists():\n",
    "    print(f\"âœ… Dataset found: {dataset_path}\")\n",
    "else:\n",
    "    print(f\"âŒ Dataset not found: {dataset_path}\")\n",
    "    print(\"Please check the repository structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ é…ç½®åƒæ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é…ç½®åƒæ•¸\n",
    "CONFIG = {\n",
    "    'max_workers': 4,          # ä¸¦è¡Œä¸‹è¼‰æ•¸ï¼ˆColab å»ºè­° 2-4ï¼‰\n",
    "    'max_retries': 3,          # å¤±æ•—é‡è©¦æ¬¡æ•¸\n",
    "    'timeout': 60,             # è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰\n",
    "    'save_interval': 20,       # æ¯ N å€‹æ¨£æœ¬ä¿å­˜ checkpoint\n",
    "    'test_samples': 11979,     # å…¨é‡ä¸‹è¼‰ï¼ˆæ”¹ç‚ºè¼ƒå°æ•¸å­—å¯æ¸¬è©¦ï¼‰\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, val in CONFIG.items():\n",
    "    print(f\"  {key}: {val}\")\n",
    "\n",
    "# ä¼°ç®—æ™‚é–“\n",
    "estimated_hours = (CONFIG['test_samples'] * 5) / 3600 / CONFIG['max_workers']\n",
    "print(f\"\\nEstimated time: {estimated_hours:.1f} hours\")\n",
    "print(f\"Expected success: ~{int(CONFIG['test_samples'] * 0.57)} samples (57% rate)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ ä¸»ä¸‹è¼‰ç¨‹å¼\n",
    "\n",
    "**æ³¨æ„**: æ­¤ cell å°‡é‹è¡Œ 6-7 å°æ™‚ï¼Œè«‹ç¢ºä¿ Colab é€£ç·šç©©å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm  # Colab ä½¿ç”¨ notebook ç‰ˆæœ¬\n",
    "import h5py\n",
    "import lightkurve as lk\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Exoplanet Detection - Full Download (Colab Version)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# è¨­å®šè·¯å¾‘\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "LIGHTCURVE_DIR = DATA_DIR / 'lightcurves'\n",
    "CHECKPOINT_DIR = PROJECT_ROOT / 'checkpoints'\n",
    "\n",
    "LIGHTCURVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n[1/5] Paths configured\")\n",
    "print(f\"  Lightcurves: {LIGHTCURVE_DIR}\")\n",
    "print(f\"  Checkpoints: {CHECKPOINT_DIR}\")\n",
    "\n",
    "# è¼‰å…¥æ•¸æ“šé›†\n",
    "print(f\"\\n[2/5] Loading dataset...\")\n",
    "samples_df = pd.read_csv(dataset_path)\n",
    "samples_df = samples_df.head(CONFIG['test_samples'])\n",
    "\n",
    "if 'sample_id' not in samples_df.columns:\n",
    "    samples_df['sample_id'] = [f\"SAMPLE_{i:06d}\" for i in range(len(samples_df))]\n",
    "if 'tic_id' not in samples_df.columns:\n",
    "    if 'tid' in samples_df.columns:\n",
    "        samples_df['tic_id'] = samples_df['tid']\n",
    "\n",
    "print(f\"  Total samples: {len(samples_df)}\")\n",
    "print(f\"  Positive: {samples_df['label'].sum()}, Negative: {(~samples_df['label'].astype(bool)).sum()}\")\n",
    "\n",
    "# ä¸‹è¼‰å‡½æ•¸\n",
    "def download_lightcurve(row, retries=3):\n",
    "    \"\"\"Download TESS light curve and save as HDF5\"\"\"\n",
    "    sample_id = row['sample_id']\n",
    "    tic_id = int(float(row['tic_id']))\n",
    "    \n",
    "    result = {\n",
    "        'sample_id': sample_id,\n",
    "        'tic_id': tic_id,\n",
    "        'status': 'failed',\n",
    "        'file_path': None,\n",
    "        'n_sectors': 0,\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    file_path = LIGHTCURVE_DIR / f\"{sample_id}_TIC{tic_id}.h5\"\n",
    "    \n",
    "    # æª¢æŸ¥æ˜¯å¦å·²ä¸‹è¼‰\n",
    "    if file_path.exists():\n",
    "        result['status'] = 'cached'\n",
    "        result['file_path'] = str(file_path)\n",
    "        try:\n",
    "            with h5py.File(file_path, 'r') as f:\n",
    "                result['n_sectors'] = f.attrs.get('n_sectors', 0)\n",
    "        except:\n",
    "            pass\n",
    "        return result\n",
    "    \n",
    "    # å˜—è©¦ä¸‹è¼‰\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            search_result = lk.search_lightcurve(f\"TIC {tic_id}\", author='SPOC')\n",
    "            if search_result is None or len(search_result) == 0:\n",
    "                result['error'] = 'no_data'\n",
    "                return result\n",
    "            \n",
    "            lc_collection = search_result.download_all()\n",
    "            if lc_collection is None or len(lc_collection) == 0:\n",
    "                result['error'] = 'download_failed'\n",
    "                return result\n",
    "            \n",
    "            # ä¿å­˜ç‚º HDF5\n",
    "            with h5py.File(file_path, 'w') as f:\n",
    "                f.attrs['sample_id'] = sample_id\n",
    "                f.attrs['tic_id'] = tic_id\n",
    "                f.attrs['n_sectors'] = len(lc_collection)\n",
    "                f.attrs['download_time'] = datetime.now().isoformat()\n",
    "                \n",
    "                for i, lc in enumerate(lc_collection):\n",
    "                    grp = f.create_group(f'sector_{i}')\n",
    "                    grp.create_dataset('time', data=np.array(lc.time.value))\n",
    "                    grp.create_dataset('flux', data=np.array(lc.flux.value))\n",
    "                    grp.create_dataset('flux_err', data=np.array(lc.flux_err.value))\n",
    "                    grp.attrs['sector'] = lc.meta.get('SECTOR', '?')\n",
    "                    grp.attrs['mission'] = str(lc.meta.get('MISSION', 'TESS'))\n",
    "            \n",
    "            result['status'] = 'success'\n",
    "            result['file_path'] = str(file_path)\n",
    "            result['n_sectors'] = len(lc_collection)\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            result['error'] = str(e)[:50]\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(2 ** attempt)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Checkpoint ç®¡ç†\n",
    "def load_checkpoint():\n",
    "    cp = CHECKPOINT_DIR / 'download_progress.parquet'\n",
    "    if cp.exists():\n",
    "        df = pd.read_parquet(cp)\n",
    "        print(f\"  âœ… Loaded checkpoint: {len(df)} records\")\n",
    "        return df\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def save_checkpoint(df):\n",
    "    cp = CHECKPOINT_DIR / 'download_progress.parquet'\n",
    "    df.to_parquet(cp, index=False)\n",
    "\n",
    "# ä¸»ä¸‹è¼‰æµç¨‹\n",
    "print(f\"\\n[3/5] Starting download...\")\n",
    "progress_df = load_checkpoint()\n",
    "\n",
    "if len(progress_df) > 0:\n",
    "    completed = set(progress_df[progress_df['status'].isin(['success', 'cached'])]['sample_id'])\n",
    "    remaining = samples_df[~samples_df['sample_id'].isin(completed)]\n",
    "else:\n",
    "    remaining = samples_df.copy()\n",
    "\n",
    "print(f\"  Total: {len(samples_df)}\")\n",
    "print(f\"  Completed: {len(samples_df)-len(remaining)}\")\n",
    "print(f\"  Remaining: {len(remaining)}\")\n",
    "\n",
    "if len(remaining) == 0:\n",
    "    print(\"  âœ… All samples already downloaded!\")\n",
    "else:\n",
    "    print(f\"  â±ï¸ Estimated time: {len(remaining)*5/60/CONFIG['max_workers']:.1f} minutes\")\n",
    "    print(f\"\\nâš ï¸ This will take approximately {len(remaining)*5/3600/CONFIG['max_workers']:.1f} hours\")\n",
    "    print(\"  Keep this notebook running...\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=CONFIG['max_workers']) as executor:\n",
    "        futures = {executor.submit(download_lightcurve, row, CONFIG['max_retries']): row\n",
    "                   for _, row in remaining.iterrows()}\n",
    "        \n",
    "        with tqdm(total=len(remaining), desc=\"Downloading\") as pbar:\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # å®šæœŸä¿å­˜ checkpoint\n",
    "                if len(results) % CONFIG['save_interval'] == 0:\n",
    "                    temp_df = pd.concat([progress_df, pd.DataFrame(results)], ignore_index=True)\n",
    "                    save_checkpoint(temp_df)\n",
    "                    print(f\"  ğŸ’¾ Checkpoint saved: {len(results)} new downloads\")\n",
    "    \n",
    "    # æœ€çµ‚ä¿å­˜\n",
    "    if len(results) > 0:\n",
    "        progress_df = pd.concat([progress_df, pd.DataFrame(results)], ignore_index=True)\n",
    "        save_checkpoint(progress_df)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\nâœ… Download complete!\")\n",
    "    print(f\"  Time: {elapsed/60:.1f} min ({elapsed/3600:.1f} hours)\")\n",
    "    print(f\"  Average: {elapsed/len(results):.1f} sec/sample\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ æŸ¥çœ‹çµ±è¨ˆèˆ‡é©—è­‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é‡æ–°è¼‰å…¥æœ€æ–°çš„ checkpoint\n",
    "progress_df = load_checkpoint()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Download Statistics\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ç‹€æ…‹çµ±è¨ˆ\n",
    "status_counts = progress_df['status'].value_counts()\n",
    "print(\"\\nStatus breakdown:\")\n",
    "for status, count in status_counts.items():\n",
    "    print(f\"  {status}: {count}\")\n",
    "\n",
    "success_count = status_counts.get('success', 0) + status_counts.get('cached', 0)\n",
    "success_rate = success_count / len(progress_df) * 100\n",
    "print(f\"\\nSuccess rate: {success_rate:.1f}% ({success_count}/{len(progress_df)})\")\n",
    "\n",
    "# æª”æ¡ˆé©—è­‰\n",
    "h5_files = list(LIGHTCURVE_DIR.glob('*.h5'))\n",
    "print(f\"\\nFiles on disk: {len(h5_files)}\")\n",
    "\n",
    "if len(h5_files) > 0:\n",
    "    total_size = sum(f.stat().st_size for f in h5_files) / 1024 / 1024 / 1024\n",
    "    print(f\"Total size: {total_size:.2f} GB\")\n",
    "    print(f\"Average: {total_size/len(h5_files)*1024:.1f} MB/file\")\n",
    "    \n",
    "    # éš¨æ©Ÿæª¢æŸ¥ 3 å€‹æª”æ¡ˆ\n",
    "    print(\"\\nSample verification:\")\n",
    "    samples = np.random.choice(h5_files, min(3, len(h5_files)), replace=False)\n",
    "    for h5_file in samples:\n",
    "        try:\n",
    "            with h5py.File(h5_file, 'r') as f:\n",
    "                tic_id = f.attrs['tic_id']\n",
    "                n_sectors = f.attrs['n_sectors']\n",
    "                total_points = sum(len(f[f'sector_{i}']['time']) for i in range(n_sectors))\n",
    "                print(f\"  TIC{tic_id}: {n_sectors} sectors, {total_points:,} data points\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ {h5_file.name}: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ ä¿å­˜çµæœ\n",
    "\n",
    "é¸æ“‡ä»¥ä¸‹å…¶ä¸­ä¸€ç¨®æ–¹å¼ä¿å­˜çµæœ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é¸é … A: ä¸‹è¼‰ checkpoint åˆ°æœ¬åœ°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# ä¸‹è¼‰ checkpoint æª”æ¡ˆ\n",
    "checkpoint_file = CHECKPOINT_DIR / 'download_progress.parquet'\n",
    "if checkpoint_file.exists():\n",
    "    files.download(str(checkpoint_file))\n",
    "    print(\"âœ… Checkpoint downloaded!\")\n",
    "else:\n",
    "    print(\"âŒ Checkpoint file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é¸é … B: æ¨é€åˆ° GitHub (éœ€è¦è¨­å®š token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­å®š GitHub token (ä¸è¦ç›´æ¥å¯«åœ¨ notebook ä¸­ï¼)\n",
    "# ä½¿ç”¨ Colab Secrets æˆ–ç’°å¢ƒè®Šæ•¸\n",
    "\n",
    "# from google.colab import userdata\n",
    "# GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n",
    "\n",
    "# %cd {PROJECT_ROOT}\n",
    "# !git config user.name \"Your Name\"\n",
    "# !git config user.email \"your.email@example.com\"\n",
    "# !git add checkpoints/download_progress.parquet\n",
    "# !git commit -m \"feat: complete full-scale download from Colab\"\n",
    "# !git push https://{GITHUB_TOKEN}@github.com/exoplanet-spaceapps/exoplanet-starter.git main\n",
    "\n",
    "print(\"âš ï¸ Uncomment and configure the above code to push to GitHub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é¸é … C: å£“ç¸®ä¸¦ä¸‹è¼‰éƒ¨åˆ†è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å£“ç¸®å‰ 100 å€‹æˆåŠŸä¸‹è¼‰çš„æª”æ¡ˆï¼ˆç¤ºä¾‹ï¼‰\n",
    "!cd {LIGHTCURVE_DIR} && tar -czf sample_lightcurves_100.tar.gz $(ls *.h5 | head -100)\n",
    "\n",
    "# ä¸‹è¼‰å£“ç¸®æª”\n",
    "sample_archive = LIGHTCURVE_DIR / 'sample_lightcurves_100.tar.gz'\n",
    "if sample_archive.exists():\n",
    "    files.download(str(sample_archive))\n",
    "    print(\"âœ… Sample archive downloaded!\")\n",
    "else:\n",
    "    print(\"âŒ Archive creation failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ ä¸‹ä¸€æ­¥\n",
    "\n",
    "ä¸‹è¼‰å®Œæˆå¾Œï¼ŒåŸ·è¡Œç‰¹å¾µæå–ï¼š\n",
    "\n",
    "```python\n",
    "!python scripts/test_features.py\n",
    "```\n",
    "\n",
    "æˆ–ä½¿ç”¨å°æ‡‰çš„ Colab notebook é€²è¡Œç‰¹å¾µæå–ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ æ•…éšœæ’é™¤\n",
    "\n",
    "**å•é¡Œ 1: Colab ä¸­æ–·é€£ç·š**\n",
    "- è§£æ±º: ä½¿ç”¨ checkpoint ç³»çµ±é‡æ–°åŸ·è¡Œ Cell 5ï¼Œæœƒå¾ä¸­æ–·è™•ç¹¼çºŒ\n",
    "\n",
    "**å•é¡Œ 2: å„²å­˜ç©ºé–“ä¸è¶³**\n",
    "- è§£æ±º: ä½¿ç”¨ Google Drive æˆ–å®šæœŸä¸‹è¼‰ä¸¦åˆªé™¤æœ¬åœ°æª”æ¡ˆ\n",
    "\n",
    "**å•é¡Œ 3: ä¸‹è¼‰é€Ÿåº¦éæ…¢**\n",
    "- è§£æ±º: æ¸›å°‘ `max_workers` æˆ–æª¢æŸ¥ç¶²è·¯é€£ç·š\n",
    "\n",
    "**å•é¡Œ 4: MAST å¿«å–éŒ¯èª¤**\n",
    "- è§£æ±º: é‡è©¦æ©Ÿåˆ¶æœƒè‡ªå‹•è™•ç†ï¼Œéƒ¨åˆ†å¤±æ•—å±¬æ­£å¸¸ç¾è±¡ï¼ˆ57% æˆåŠŸç‡ï¼‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
