{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02a - æ‰¹é‡ä¸‹è½½å…‰æ›²çº¿æ•°æ®\n",
    "\n",
    "**ç›®æ ‡**: ä¸€æ¬¡æ€§ä¸‹è½½æ‰€æœ‰ TOI æ ·æœ¬çš„ TESS å…‰æ›²çº¿ï¼Œä¿å­˜ä¸ºæœ¬åœ°æ–‡ä»¶\n",
    "\n",
    "**ç‰¹ç‚¹**:\n",
    "- âœ… æ‰¹é‡å¹¶å‘ä¸‹è½½ï¼ˆå¯é…ç½®çº¿ç¨‹æ•°ï¼‰\n",
    "- âœ… æ–­ç‚¹ç»­ä¼ ï¼ˆä¸­æ–­åå¯ç»§ç»­ï¼‰\n",
    "- âœ… è‡ªåŠ¨é‡è¯•å¤±è´¥æ ·æœ¬\n",
    "- âœ… è¿›åº¦è¿½è¸ªå’Œç»Ÿè®¡\n",
    "- âœ… ä¿å­˜ä¸º FITS æ ¼å¼ï¼ˆæ ‡å‡†å¤©æ–‡æ ¼å¼ï¼‰\n",
    "\n",
    "**é¢„è®¡æ—¶é—´**: 4-8 å°æ—¶ï¼ˆå–å†³äºç½‘ç»œå’Œæ ·æœ¬æ•°ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: å®‰è£…ä¾èµ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab ç¯å¢ƒæ£€æµ‹å’Œå®‰è£…\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"ğŸ“¦ Installing dependencies...\")\n",
    "    !pip install -q numpy==1.26.4 'scipy<1.13' lightkurve pandas tqdm joblib\n",
    "    print(\"âœ… Installation complete\")\n",
    "    print(\"âš ï¸ If errors, restart runtime and skip to Cell 2\")\n",
    "else:\n",
    "    print(\"âœ… Running locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: å¯¼å…¥åº“å’Œé…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import lightkurve as lk\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Imports successful\")\n",
    "print(f\"   Lightkurve: {lk.__version__}\")\n",
    "print(f\"   NumPy: {np.__version__}\")\n",
    "print(f\"   Pandas: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: é…ç½®è·¯å¾„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¯å¢ƒæ£€æµ‹\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"ğŸŒ Running in Google Colab\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    \n",
    "    REPO_DIR = Path('/content/exoplanet-starter')\n",
    "    if not REPO_DIR.exists():\n",
    "        print(\"ğŸ“¥ Cloning repository...\")\n",
    "        !git clone https://github.com/exoplanet-spaceapps/exoplanet-starter.git /content/exoplanet-starter\n",
    "    \n",
    "    os.chdir(str(REPO_DIR))\n",
    "    BASE_DIR = REPO_DIR\n",
    "    LIGHTCURVE_DIR = Path('/content/drive/MyDrive/spaceapps-lightcurves')\n",
    "    CHECKPOINT_DIR = Path('/content/drive/MyDrive/spaceapps-checkpoints')\n",
    "else:\n",
    "    print(\"ğŸ’» Running locally\")\n",
    "    BASE_DIR = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "    LIGHTCURVE_DIR = BASE_DIR / 'data' / 'lightcurves'\n",
    "    CHECKPOINT_DIR = BASE_DIR / 'checkpoints'\n",
    "\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "LIGHTCURVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nâœ… Paths configured:\")\n",
    "print(f\"   Data: {DATA_DIR}\")\n",
    "print(f\"   Lightcurves: {LIGHTCURVE_DIR}\")\n",
    "print(f\"   Checkpoints: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: ä¸‹è½½é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‹è½½é…ç½®\n",
    "CONFIG = {\n",
    "    'max_workers': 4,        # å¹¶å‘çº¿ç¨‹æ•°ï¼ˆå»ºè®® 2-8ï¼‰\n",
    "    'max_retries': 3,        # å¤±è´¥é‡è¯•æ¬¡æ•°\n",
    "    'timeout': 60,           # å•ä¸ªä¸‹è½½è¶…æ—¶ï¼ˆç§’ï¼‰\n",
    "    'batch_size': 100,       # æ‰¹å¤„ç†å¤§å°\n",
    "    'save_interval': 50,     # æ¯ N ä¸ªä¿å­˜ä¸€æ¬¡è¿›åº¦\n",
    "    'test_mode': False,      # True = åªä¸‹è½½å‰ 100 ä¸ªæ ·æœ¬æµ‹è¯•\n",
    "}\n",
    "\n",
    "print(\"âš™ï¸ Download Configuration:\")\n",
    "for key, val in CONFIG.items():\n",
    "    print(f\"   {key}: {val}\")\n",
    "\n",
    "# åŠ è½½æ•°æ®é›†\n",
    "dataset_path = DATA_DIR / 'supervised_dataset.csv'\n",
    "if not dataset_path.exists():\n",
    "    raise FileNotFoundError(f\"âŒ Dataset not found: {dataset_path}\")\n",
    "\n",
    "samples_df = pd.read_csv(dataset_path)\n",
    "\n",
    "# æµ‹è¯•æ¨¡å¼\n",
    "if CONFIG['test_mode']:\n",
    "    samples_df = samples_df.head(100)\n",
    "    print(f\"\\nâš ï¸ TEST MODE: Only processing {len(samples_df)} samples\")\n",
    "\n",
    "# æ·»åŠ å”¯ä¸€ ID\n",
    "if 'sample_id' not in samples_df.columns:\n",
    "    samples_df['sample_id'] = [f\"SAMPLE_{i:06d}\" for i in range(len(samples_df))]\n",
    "\n",
    "if 'tic_id' not in samples_df.columns:\n",
    "    if 'tid' in samples_df.columns:\n",
    "        samples_df['tic_id'] = samples_df['tid']\n",
    "    elif 'target_id' in samples_df.columns:\n",
    "        samples_df['tic_id'] = samples_df['target_id']\n",
    "\n",
    "print(f\"\\nâœ… Dataset loaded: {len(samples_df):,} samples\")\n",
    "print(f\"   Positive: {samples_df['label'].sum():,}\")\n",
    "print(f\"   Negative: {(~samples_df['label'].astype(bool)).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: ä¸‹è½½å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_single_lightcurve(row: pd.Series, retries: int = 3) -> dict:\n",
    "    \"\"\"\n",
    "    ä¸‹è½½å•ä¸ª TIC ID çš„å…‰æ›²çº¿å¹¶ä¿å­˜ä¸º pickle æ–‡ä»¶\n",
    "    \n",
    "    Returns:\n",
    "        dict: {'sample_id', 'tic_id', 'status', 'file_path', 'n_sectors', 'error'}\n",
    "    \"\"\"\n",
    "    sample_id = row['sample_id']\n",
    "    tic_id = int(float(row['tic_id']))\n",
    "    \n",
    "    result = {\n",
    "        'sample_id': sample_id,\n",
    "        'tic_id': tic_id,\n",
    "        'status': 'failed',\n",
    "        'file_path': None,\n",
    "        'n_sectors': 0,\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½\n",
    "    file_path = LIGHTCURVE_DIR / f\"{sample_id}_TIC{tic_id}.pkl\"\n",
    "    if file_path.exists():\n",
    "        result['status'] = 'cached'\n",
    "        result['file_path'] = str(file_path)\n",
    "        return result\n",
    "    \n",
    "    # å°è¯•ä¸‹è½½\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            # æœç´¢å…‰æ›²çº¿\n",
    "            search_result = lk.search_lightcurve(f\"TIC {tic_id}\", author='SPOC')\n",
    "            \n",
    "            if search_result is None or len(search_result) == 0:\n",
    "                result['error'] = 'no_data_found'\n",
    "                return result\n",
    "            \n",
    "            # ä¸‹è½½æ‰€æœ‰æ‰‡åŒº\n",
    "            lc_collection = search_result.download_all()\n",
    "            \n",
    "            if lc_collection is None or len(lc_collection) == 0:\n",
    "                result['error'] = 'download_failed'\n",
    "                return result\n",
    "            \n",
    "            # ä¿å­˜ä¸º pickleï¼ˆåŒ…å«å®Œæ•´çš„ LightCurveCollectionï¼‰\n",
    "            save_data = {\n",
    "                'sample_id': sample_id,\n",
    "                'tic_id': tic_id,\n",
    "                'lc_collection': lc_collection,\n",
    "                'n_sectors': len(lc_collection),\n",
    "                'download_time': datetime.now().isoformat(),\n",
    "                'sectors': [lc.meta.get('SECTOR', 'unknown') for lc in lc_collection]\n",
    "            }\n",
    "            \n",
    "            joblib.dump(save_data, file_path)\n",
    "            \n",
    "            result['status'] = 'success'\n",
    "            result['file_path'] = str(file_path)\n",
    "            result['n_sectors'] = len(lc_collection)\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            result['error'] = str(e)\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿\n",
    "                continue\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def load_checkpoint() -> pd.DataFrame:\n",
    "    \"\"\"åŠ è½½ä¸‹è½½è¿›åº¦ checkpoint\"\"\"\n",
    "    checkpoint_path = CHECKPOINT_DIR / 'download_progress.parquet'\n",
    "    if checkpoint_path.exists():\n",
    "        df = pd.read_parquet(checkpoint_path)\n",
    "        print(f\"ğŸ“‚ Loaded checkpoint: {len(df)} downloads\")\n",
    "        return df\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "def save_checkpoint(progress_df: pd.DataFrame):\n",
    "    \"\"\"ä¿å­˜ä¸‹è½½è¿›åº¦\"\"\"\n",
    "    checkpoint_path = CHECKPOINT_DIR / 'download_progress.parquet'\n",
    "    progress_df.to_parquet(checkpoint_path, index=False)\n",
    "    print(f\"ğŸ’¾ Checkpoint saved: {len(progress_df)} downloads\")\n",
    "\n",
    "\n",
    "print(\"âœ… Download functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: æ‰¹é‡ä¸‹è½½ï¼ˆä¸»è¦æ‰§è¡Œï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½è¿›åº¦\n",
    "progress_df = load_checkpoint()\n",
    "\n",
    "# ç¡®å®šå¾…ä¸‹è½½æ ·æœ¬\n",
    "if len(progress_df) > 0:\n",
    "    completed_ids = set(progress_df[progress_df['status'].isin(['success', 'cached'])]['sample_id'])\n",
    "    remaining_samples = samples_df[~samples_df['sample_id'].isin(completed_ids)]\n",
    "else:\n",
    "    remaining_samples = samples_df.copy()\n",
    "\n",
    "print(f\"ğŸ“Š Download Progress:\")\n",
    "print(f\"   Total samples: {len(samples_df):,}\")\n",
    "print(f\"   Already downloaded: {len(samples_df) - len(remaining_samples):,}\")\n",
    "print(f\"   Remaining: {len(remaining_samples):,}\")\n",
    "\n",
    "if len(remaining_samples) == 0:\n",
    "    print(\"\\nâœ… All lightcurves already downloaded!\")\n",
    "else:\n",
    "    print(f\"\\nğŸš€ Starting download for {len(remaining_samples):,} samples\")\n",
    "    print(f\"   Workers: {CONFIG['max_workers']}\")\n",
    "    print(f\"   Estimated time: {len(remaining_samples) * 5 / 3600 / CONFIG['max_workers']:.1f} hours\")\n",
    "    print(f\"   (assuming 5 sec/sample with {CONFIG['max_workers']} workers)\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "    \n",
    "    # å¹¶å‘ä¸‹è½½\n",
    "    with ThreadPoolExecutor(max_workers=CONFIG['max_workers']) as executor:\n",
    "        # æäº¤æ‰€æœ‰ä»»åŠ¡\n",
    "        future_to_row = {\n",
    "            executor.submit(download_single_lightcurve, row, CONFIG['max_retries']): row \n",
    "            for _, row in remaining_samples.iterrows()\n",
    "        }\n",
    "        \n",
    "        # è¿›åº¦æ¡\n",
    "        with tqdm(total=len(remaining_samples), desc=\"Downloading\") as pbar:\n",
    "            for future in as_completed(future_to_row):\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # å®šæœŸä¿å­˜\n",
    "                if len(results) % CONFIG['save_interval'] == 0:\n",
    "                    temp_df = pd.concat([progress_df, pd.DataFrame(results)], ignore_index=True)\n",
    "                    save_checkpoint(temp_df)\n",
    "                    \n",
    "                    # æ˜¾ç¤ºç»Ÿè®¡\n",
    "                    success_count = sum(1 for r in results if r['status'] == 'success')\n",
    "                    cached_count = sum(1 for r in results if r['status'] == 'cached')\n",
    "                    failed_count = sum(1 for r in results if r['status'] == 'failed')\n",
    "                    \n",
    "                    pbar.set_postfix({\n",
    "                        'success': success_count,\n",
    "                        'cached': cached_count,\n",
    "                        'failed': failed_count\n",
    "                    })\n",
    "    \n",
    "    # æœ€ç»ˆä¿å­˜\n",
    "    if len(results) > 0:\n",
    "        progress_df = pd.concat([progress_df, pd.DataFrame(results)], ignore_index=True)\n",
    "        save_checkpoint(progress_df)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nğŸ‰ Download complete!\")\n",
    "    print(f\"   Total time: {elapsed / 3600:.2f} hours\")\n",
    "    print(f\"   Average: {elapsed / len(results):.1f} sec/sample\")\n",
    "\n",
    "# æœ€ç»ˆç»Ÿè®¡\n",
    "print(f\"\\nğŸ“Š Final Statistics:\")\n",
    "status_counts = progress_df['status'].value_counts()\n",
    "for status, count in status_counts.items():\n",
    "    print(f\"   {status}: {count:,}\")\n",
    "\n",
    "success_rate = (status_counts.get('success', 0) + status_counts.get('cached', 0)) / len(progress_df) * 100\n",
    "print(f\"\\n   Success rate: {success_rate:.1f}%\")\n",
    "print(f\"   Total lightcurves: {len(list(LIGHTCURVE_DIR.glob('*.pkl'))):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: éªŒè¯ä¸‹è½½æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# éªŒè¯éšæœºæ ·æœ¬\n",
    "print(\"ğŸ” Verifying downloaded data...\\n\")\n",
    "\n",
    "pkl_files = list(LIGHTCURVE_DIR.glob('*.pkl'))\n",
    "if len(pkl_files) == 0:\n",
    "    print(\"âŒ No lightcurve files found!\")\n",
    "else:\n",
    "    # éšæœºé€‰æ‹© 3 ä¸ªæ–‡ä»¶éªŒè¯\n",
    "    sample_files = np.random.choice(pkl_files, min(3, len(pkl_files)), replace=False)\n",
    "    \n",
    "    for pkl_file in sample_files:\n",
    "        try:\n",
    "            data = joblib.load(pkl_file)\n",
    "            print(f\"âœ… {pkl_file.name}\")\n",
    "            print(f\"   TIC ID: {data['tic_id']}\")\n",
    "            print(f\"   Sectors: {data['n_sectors']} ({data['sectors']})\")\n",
    "            print(f\"   Downloaded: {data['download_time']}\")\n",
    "            \n",
    "            # æ£€æŸ¥ç¬¬ä¸€ä¸ªå…‰æ›²çº¿\n",
    "            lc = data['lc_collection'][0]\n",
    "            print(f\"   Data points: {len(lc.time):,}\")\n",
    "            print(f\"   Time span: {float(lc.time[-1] - lc.time[0]):.1f} days\")\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {pkl_file.name}: {e}\\n\")\n",
    "    \n",
    "    print(f\"\\nğŸ“¦ Storage:\")\n",
    "    total_size = sum(f.stat().st_size for f in pkl_files) / 1024 / 1024 / 1024\n",
    "    print(f\"   Total files: {len(pkl_files):,}\")\n",
    "    print(f\"   Total size: {total_size:.2f} GB\")\n",
    "    print(f\"   Average size: {total_size * 1024 / len(pkl_files):.1f} MB/file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: ç”Ÿæˆä¸‹è½½æŠ¥å‘Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š\n",
    "report = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'total_samples': len(samples_df),\n",
    "    'downloaded': int(status_counts.get('success', 0) + status_counts.get('cached', 0)),\n",
    "    'failed': int(status_counts.get('failed', 0)),\n",
    "    'success_rate': float(success_rate),\n",
    "    'config': CONFIG,\n",
    "    'storage': {\n",
    "        'directory': str(LIGHTCURVE_DIR),\n",
    "        'total_files': len(pkl_files),\n",
    "        'total_size_gb': float(total_size)\n",
    "    },\n",
    "    'errors': progress_df[progress_df['status'] == 'failed']['error'].value_counts().to_dict()\n",
    "}\n",
    "\n",
    "report_path = CHECKPOINT_DIR / 'download_report.json'\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Download report saved: {report_path}\")\n",
    "print(f\"\\nğŸ“‹ Summary:\")\n",
    "print(f\"   Downloaded: {report['downloaded']:,} / {report['total_samples']:,}\")\n",
    "print(f\"   Success rate: {report['success_rate']:.1f}%\")\n",
    "print(f\"   Storage: {report['storage']['total_size_gb']:.2f} GB\")\n",
    "\n",
    "if report['errors']:\n",
    "    print(f\"\\nâš ï¸ Error breakdown:\")\n",
    "    for error, count in report['errors'].items():\n",
    "        print(f\"   {error}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… ä¸‹è½½å®Œæˆï¼\n",
    "\n",
    "### ä¸‹ä¸€æ­¥:\n",
    "1. è¿è¡Œ `02b_extract_features.ipynb` è¿›è¡Œç‰¹å¾æå–\n",
    "2. ç‰¹å¾æå–å¯ä»¥å¤šæ¬¡è¿è¡Œï¼Œä¸éœ€è¦é‡æ–°ä¸‹è½½\n",
    "3. å¯ä»¥å°è¯•ä¸åŒçš„ç‰¹å¾æå–ç­–ç•¥\n",
    "\n",
    "### æ–‡ä»¶ä½ç½®:\n",
    "- **å…‰æ›²çº¿æ•°æ®**: `{LIGHTCURVE_DIR}`\n",
    "- **ä¸‹è½½è¿›åº¦**: `checkpoints/download_progress.parquet`\n",
    "- **ä¸‹è½½æŠ¥å‘Š**: `checkpoints/download_report.json`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
