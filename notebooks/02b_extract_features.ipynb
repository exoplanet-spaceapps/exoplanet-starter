{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02b - 从本地光曲线提取特征\n",
    "\n",
    "**前置条件**: 必须先运行 `02a_download_lightcurves.ipynb` 下载光曲线\n",
    "\n",
    "**目标**: 从已下载的光曲线文件中提取 BLS 特征\n",
    "\n",
    "**优势**:\n",
    "- ✅ 不需要网络连接\n",
    "- ✅ 可以快速迭代不同特征\n",
    "- ✅ 支持并行处理\n",
    "- ✅ 内存高效（批处理）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: 安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"📦 Installing dependencies...\")\n",
    "    !pip install -q numpy==1.26.4 'scipy<1.13' pandas scikit-learn joblib tqdm\n",
    "    !pip install -q astropy\n",
    "    print(\"✅ Installation complete\")\n",
    "else:\n",
    "    print(\"✅ Running locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: 导入和配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from astropy.timeseries import BoxLeastSquares\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✅ Imports successful\")\n",
    "print(f\"   NumPy: {np.__version__}\")\n",
    "print(f\"   Pandas: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: 配置路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"🌍 Running in Google Colab\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    \n",
    "    REPO_DIR = Path('/content/exoplanet-starter')\n",
    "    os.chdir(str(REPO_DIR))\n",
    "    BASE_DIR = REPO_DIR\n",
    "    LIGHTCURVE_DIR = Path('/content/drive/MyDrive/spaceapps-lightcurves')\n",
    "    CHECKPOINT_DIR = Path('/content/drive/MyDrive/spaceapps-checkpoints')\n",
    "    MODEL_DIR = Path('/content/drive/MyDrive/spaceapps-models')\n",
    "else:\n",
    "    print(\"💻 Running locally\")\n",
    "    BASE_DIR = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "    LIGHTCURVE_DIR = BASE_DIR / 'data' / 'lightcurves'\n",
    "    CHECKPOINT_DIR = BASE_DIR / 'checkpoints'\n",
    "    MODEL_DIR = BASE_DIR / 'models'\n",
    "\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n✅ Paths configured:\")\n",
    "print(f\"   Lightcurves: {LIGHTCURVE_DIR}\")\n",
    "print(f\"   Checkpoints: {CHECKPOINT_DIR}\")\n",
    "print(f\"   Models: {MODEL_DIR}\")\n",
    "\n",
    "# 验证光曲线目录\n",
    "if not LIGHTCURVE_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"❌ Lightcurve directory not found: {LIGHTCURVE_DIR}\\n\"\n",
    "        f\"   Please run 02a_download_lightcurves.ipynb first!\"\n",
    "    )\n",
    "\n",
    "lc_files = list(LIGHTCURVE_DIR.glob('*.pkl'))\n",
    "print(f\"\\n📦 Found {len(lc_files):,} lightcurve files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: 特征提取配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征提取配置\n",
    "CONFIG = {\n",
    "    'max_workers': 4,          # 并行进程数（建议 2-8）\n",
    "    'batch_size': 100,         # 批处理大小\n",
    "    'save_interval': 50,       # 每 N 个保存一次\n",
    "    'bls_periods': 2000,       # BLS 周期搜索点数\n",
    "    'bls_durations': 10,       # BLS 持续时间搜索点数\n",
    "    'period_min': 0.5,         # 最小周期（天）\n",
    "    'period_max': 15.0,        # 最大周期（天）\n",
    "}\n",
    "\n",
    "print(\"⚙️ Feature Extraction Configuration:\")\n",
    "for key, val in CONFIG.items():\n",
    "    print(f\"   {key}: {val}\")\n",
    "\n",
    "# 加载数据集元数据\n",
    "dataset_path = DATA_DIR / 'supervised_dataset.csv'\n",
    "samples_df = pd.read_csv(dataset_path)\n",
    "\n",
    "if 'sample_id' not in samples_df.columns:\n",
    "    samples_df['sample_id'] = [f\"SAMPLE_{i:06d}\" for i in range(len(samples_df))]\n",
    "\n",
    "print(f\"\\n✅ Dataset loaded: {len(samples_df):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: 特征提取函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_lightcurve(\n",
    "    time: np.ndarray,\n",
    "    flux: np.ndarray,\n",
    "    period: float = 1.0,\n",
    "    duration: float = 0.1,\n",
    "    depth: float = 0.01\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"从光曲线提取特征\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # 基础统计\n",
    "    features['flux_mean'] = float(np.nanmean(flux))\n",
    "    features['flux_std'] = float(np.nanstd(flux))\n",
    "    features['flux_median'] = float(np.nanmedian(flux))\n",
    "    features['flux_mad'] = float(np.nanmedian(np.abs(flux - np.nanmedian(flux))))\n",
    "    features['n_points'] = len(time)\n",
    "    features['time_span'] = float(time[-1] - time[0])\n",
    "    \n",
    "    # 输入参数\n",
    "    features['input_period'] = float(period)\n",
    "    features['input_duration'] = float(duration)\n",
    "    features['input_depth'] = float(depth)\n",
    "    \n",
    "    # BLS 分析\n",
    "    try:\n",
    "        bls = BoxLeastSquares(time, flux)\n",
    "        periods = np.linspace(\n",
    "            CONFIG['period_min'], \n",
    "            CONFIG['period_max'], \n",
    "            CONFIG['bls_periods']\n",
    "        )\n",
    "        durations = np.linspace(0.05, 0.5, CONFIG['bls_durations'])\n",
    "        bls_result = bls.power(periods, durations)\n",
    "        \n",
    "        best_idx = np.argmax(bls_result.power)\n",
    "        features['bls_power'] = float(bls_result.power[best_idx])\n",
    "        features['bls_period'] = float(bls_result.period[best_idx])\n",
    "        features['bls_duration'] = float(bls_result.duration[best_idx])\n",
    "        features['bls_depth'] = float(bls_result.depth[best_idx])\n",
    "        features['bls_snr'] = float(bls_result.depth_snr[best_idx])\n",
    "        \n",
    "    except Exception:\n",
    "        # BLS 失败则使用默认值\n",
    "        features.update({\n",
    "            'bls_power': 0.0,\n",
    "            'bls_period': period,\n",
    "            'bls_duration': duration,\n",
    "            'bls_depth': depth,\n",
    "            'bls_snr': 0.0\n",
    "        })\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def process_single_file(file_path: Path, metadata: pd.Series) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    处理单个光曲线文件\n",
    "    \n",
    "    Args:\n",
    "        file_path: 光曲线 pickle 文件路径\n",
    "        metadata: 样本元数据（来自 supervised_dataset.csv）\n",
    "    \n",
    "    Returns:\n",
    "        特征字典，失败则返回 None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 加载光曲线数据\n",
    "        data = joblib.load(file_path)\n",
    "        lc_collection = data['lc_collection']\n",
    "        \n",
    "        # 使用第一个扇区的光曲线\n",
    "        lc = lc_collection[0].remove_nans().normalize()\n",
    "        \n",
    "        # 提取特征\n",
    "        features = extract_features_from_lightcurve(\n",
    "            lc.time.value,\n",
    "            lc.flux.value,\n",
    "            metadata.get('period', 1.0),\n",
    "            metadata.get('duration', 0.1),\n",
    "            metadata.get('depth', 0.01)\n",
    "        )\n",
    "        \n",
    "        # 添加元数据\n",
    "        features['sample_id'] = data['sample_id']\n",
    "        features['tic_id'] = data['tic_id']\n",
    "        features['label'] = metadata.get('label', 0)\n",
    "        features['n_sectors'] = data['n_sectors']\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"✅ Feature extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: 批量特征提取（主要执行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 checkpoint\n",
    "checkpoint_path = CHECKPOINT_DIR / 'features_checkpoint.parquet'\n",
    "if checkpoint_path.exists():\n",
    "    features_df = pd.read_parquet(checkpoint_path)\n",
    "    print(f\"📂 Loaded checkpoint: {len(features_df)} features\")\n",
    "else:\n",
    "    features_df = pd.DataFrame()\n",
    "    print(\"🆕 Starting fresh\")\n",
    "\n",
    "# 确定待处理文件\n",
    "if len(features_df) > 0:\n",
    "    processed_ids = set(features_df['sample_id'])\n",
    "    lc_files_todo = [f for f in lc_files if f.stem.split('_')[0] not in processed_ids]\n",
    "else:\n",
    "    lc_files_todo = lc_files\n",
    "\n",
    "print(f\"\\n📊 Extraction Progress:\")\n",
    "print(f\"   Total files: {len(lc_files):,}\")\n",
    "print(f\"   Already processed: {len(lc_files) - len(lc_files_todo):,}\")\n",
    "print(f\"   Remaining: {len(lc_files_todo):,}\")\n",
    "\n",
    "if len(lc_files_todo) == 0:\n",
    "    print(\"\\n✅ All features already extracted!\")\n",
    "else:\n",
    "    print(f\"\\n🚀 Starting feature extraction\")\n",
    "    print(f\"   Workers: {CONFIG['max_workers']}\")\n",
    "    print(f\"   Estimated time: {len(lc_files_todo) * 2 / 60 / CONFIG['max_workers']:.1f} minutes\")\n",
    "    \n",
    "    # 准备任务\n",
    "    tasks = []\n",
    "    for lc_file in lc_files_todo:\n",
    "        sample_id = lc_file.stem.split('_TIC')[0]\n",
    "        metadata = samples_df[samples_df['sample_id'] == sample_id].iloc[0]\n",
    "        tasks.append((lc_file, metadata))\n",
    "    \n",
    "    new_features = []\n",
    "    \n",
    "    # 并行处理\n",
    "    with ProcessPoolExecutor(max_workers=CONFIG['max_workers']) as executor:\n",
    "        future_to_task = {\n",
    "            executor.submit(process_single_file, lc_file, metadata): (lc_file, metadata)\n",
    "            for lc_file, metadata in tasks\n",
    "        }\n",
    "        \n",
    "        with tqdm(total=len(tasks), desc=\"Extracting\") as pbar:\n",
    "            for future in as_completed(future_to_task):\n",
    "                result = future.result()\n",
    "                if result is not None:\n",
    "                    new_features.append(result)\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # 定期保存\n",
    "                if len(new_features) % CONFIG['save_interval'] == 0 and len(new_features) > 0:\n",
    "                    temp_df = pd.concat([features_df, pd.DataFrame(new_features)], ignore_index=True)\n",
    "                    temp_df.to_parquet(checkpoint_path, index=False)\n",
    "                    pbar.set_postfix({'saved': len(new_features)})\n",
    "    \n",
    "    # 最终保存\n",
    "    if len(new_features) > 0:\n",
    "        features_df = pd.concat([features_df, pd.DataFrame(new_features)], ignore_index=True)\n",
    "        features_df.to_parquet(checkpoint_path, index=False)\n",
    "        print(f\"\\n💾 Saved {len(features_df):,} features\")\n",
    "\n",
    "# 定义特征列\n",
    "feature_cols = [col for col in features_df.columns \n",
    "                if col not in ['sample_id', 'tic_id', 'label', 'n_sectors']]\n",
    "\n",
    "print(f\"\\n✅ Feature extraction complete\")\n",
    "print(f\"   Total features: {len(features_df):,}\")\n",
    "print(f\"   Feature columns: {len(feature_cols)}\")\n",
    "print(f\"   Features: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: 数据质量检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔍 Data Quality Check:\\n\")\n",
    "\n",
    "# 缺失值统计\n",
    "print(\"📊 Missing values:\")\n",
    "missing = features_df[feature_cols].isnull().sum()\n",
    "if missing.sum() == 0:\n",
    "    print(\"   ✅ No missing values!\")\n",
    "else:\n",
    "    for col, count in missing[missing > 0].items():\n",
    "        pct = count / len(features_df) * 100\n",
    "        print(f\"   {col}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# 标签分布\n",
    "print(\"\\n📊 Label distribution:\")\n",
    "label_counts = features_df['label'].value_counts()\n",
    "print(f\"   Positive (1): {label_counts.get(1, 0):,} ({label_counts.get(1, 0)/len(features_df)*100:.1f}%)\")\n",
    "print(f\"   Negative (0): {label_counts.get(0, 0):,} ({label_counts.get(0, 0)/len(features_df)*100:.1f}%)\")\n",
    "\n",
    "# 特征统计\n",
    "print(\"\\n📊 Feature statistics (sample):\")\n",
    "print(features_df[feature_cols].describe().iloc[:, :5])  # 显示前 5 个特征\n",
    "\n",
    "# 异常值检查\n",
    "print(\"\\n🔍 Checking for infinities and extreme values...\")\n",
    "X = features_df[feature_cols].values\n",
    "n_inf = np.isinf(X).sum()\n",
    "n_extreme = (np.abs(X) > 1e10).sum()\n",
    "\n",
    "if n_inf > 0:\n",
    "    print(f\"   ⚠️ Found {n_inf} infinity values\")\n",
    "if n_extreme > 0:\n",
    "    print(f\"   ⚠️ Found {n_extreme} extreme values (>1e10)\")\n",
    "if n_inf == 0 and n_extreme == 0:\n",
    "    print(\"   ✅ No infinities or extreme values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: 保存最终特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# 保存特征数据\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "features_path = MODEL_DIR / f'features_{timestamp}.parquet'\n",
    "features_df.to_parquet(features_path, index=False)\n",
    "print(f\"✅ Features saved: {features_path}\")\n",
    "\n",
    "# 保存特征列定义\n",
    "feature_cols_path = MODEL_DIR / f'feature_columns_{timestamp}.json'\n",
    "with open(feature_cols_path, 'w') as f:\n",
    "    json.dump(feature_cols, f, indent=2)\n",
    "print(f\"✅ Feature columns saved: {feature_cols_path}\")\n",
    "\n",
    "# 保存提取报告\n",
    "report = {\n",
    "    'timestamp': timestamp,\n",
    "    'total_samples': len(features_df),\n",
    "    'n_features': len(feature_cols),\n",
    "    'feature_names': feature_cols,\n",
    "    'label_distribution': {\n",
    "        'positive': int(label_counts.get(1, 0)),\n",
    "        'negative': int(label_counts.get(0, 0))\n",
    "    },\n",
    "    'config': CONFIG,\n",
    "    'data_quality': {\n",
    "        'missing_values': int(missing.sum()),\n",
    "        'infinities': int(n_inf),\n",
    "        'extreme_values': int(n_extreme)\n",
    "    }\n",
    "}\n",
    "\n",
    "report_path = MODEL_DIR / f'extraction_report_{timestamp}.json'\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "print(f\"✅ Report saved: {report_path}\")\n",
    "\n",
    "print(f\"\\n🎉 All files saved to: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ 特征提取完成！\n",
    "\n",
    "### 下一步:\n",
    "1. 运行 `03_injection_train.ipynb` 或 `03_injection_train_PRODUCTION.ipynb` 训练模型\n",
    "2. 可以修改 Cell 4 的配置尝试不同的特征提取策略\n",
    "3. 重新运行只需要 Cell 6，不需要重新下载光曲线\n",
    "\n",
    "### 输出文件:\n",
    "- **特征数据**: `models/features_{timestamp}.parquet`\n",
    "- **特征列定义**: `models/feature_columns_{timestamp}.json`\n",
    "- **提取报告**: `models/extraction_report_{timestamp}.json`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
