{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02b - ä»æœ¬åœ°å…‰æ›²çº¿æå–ç‰¹å¾\n",
    "\n",
    "**å‰ç½®æ¡ä»¶**: å¿…é¡»å…ˆè¿è¡Œ `02a_download_lightcurves.ipynb` ä¸‹è½½å…‰æ›²çº¿\n",
    "\n",
    "**ç›®æ ‡**: ä»å·²ä¸‹è½½çš„å…‰æ›²çº¿æ–‡ä»¶ä¸­æå– BLS ç‰¹å¾\n",
    "\n",
    "**ä¼˜åŠ¿**:\n",
    "- âœ… ä¸éœ€è¦ç½‘ç»œè¿æ¥\n",
    "- âœ… å¯ä»¥å¿«é€Ÿè¿­ä»£ä¸åŒç‰¹å¾\n",
    "- âœ… æ”¯æŒå¹¶è¡Œå¤„ç†\n",
    "- âœ… å†…å­˜é«˜æ•ˆï¼ˆæ‰¹å¤„ç†ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: å®‰è£…ä¾èµ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"ğŸ“¦ Installing dependencies...\")\n",
    "    !pip install -q numpy==1.26.4 'scipy<1.13' pandas scikit-learn joblib tqdm\n",
    "    !pip install -q astropy\n",
    "    print(\"âœ… Installation complete\")\n",
    "else:\n",
    "    print(\"âœ… Running locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: å¯¼å…¥å’Œé…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from astropy.timeseries import BoxLeastSquares\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… Imports successful\")\n",
    "print(f\"   NumPy: {np.__version__}\")\n",
    "print(f\"   Pandas: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: é…ç½®è·¯å¾„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"ğŸŒ Running in Google Colab\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    \n",
    "    REPO_DIR = Path('/content/exoplanet-starter')\n",
    "    os.chdir(str(REPO_DIR))\n",
    "    BASE_DIR = REPO_DIR\n",
    "    LIGHTCURVE_DIR = Path('/content/drive/MyDrive/spaceapps-lightcurves')\n",
    "    CHECKPOINT_DIR = Path('/content/drive/MyDrive/spaceapps-checkpoints')\n",
    "    MODEL_DIR = Path('/content/drive/MyDrive/spaceapps-models')\n",
    "else:\n",
    "    print(\"ğŸ’» Running locally\")\n",
    "    BASE_DIR = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "    LIGHTCURVE_DIR = BASE_DIR / 'data' / 'lightcurves'\n",
    "    CHECKPOINT_DIR = BASE_DIR / 'checkpoints'\n",
    "    MODEL_DIR = BASE_DIR / 'models'\n",
    "\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nâœ… Paths configured:\")\n",
    "print(f\"   Lightcurves: {LIGHTCURVE_DIR}\")\n",
    "print(f\"   Checkpoints: {CHECKPOINT_DIR}\")\n",
    "print(f\"   Models: {MODEL_DIR}\")\n",
    "\n",
    "# éªŒè¯å…‰æ›²çº¿ç›®å½•\n",
    "if not LIGHTCURVE_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"âŒ Lightcurve directory not found: {LIGHTCURVE_DIR}\\n\"\n",
    "        f\"   Please run 02a_download_lightcurves.ipynb first!\"\n",
    "    )\n",
    "\n",
    "lc_files = list(LIGHTCURVE_DIR.glob('*.pkl'))\n",
    "print(f\"\\nğŸ“¦ Found {len(lc_files):,} lightcurve files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: ç‰¹å¾æå–é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç‰¹å¾æå–é…ç½®\n",
    "CONFIG = {\n",
    "    'max_workers': 4,          # å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆå»ºè®® 2-8ï¼‰\n",
    "    'batch_size': 100,         # æ‰¹å¤„ç†å¤§å°\n",
    "    'save_interval': 50,       # æ¯ N ä¸ªä¿å­˜ä¸€æ¬¡\n",
    "    'bls_periods': 2000,       # BLS å‘¨æœŸæœç´¢ç‚¹æ•°\n",
    "    'bls_durations': 10,       # BLS æŒç»­æ—¶é—´æœç´¢ç‚¹æ•°\n",
    "    'period_min': 0.5,         # æœ€å°å‘¨æœŸï¼ˆå¤©ï¼‰\n",
    "    'period_max': 15.0,        # æœ€å¤§å‘¨æœŸï¼ˆå¤©ï¼‰\n",
    "}\n",
    "\n",
    "print(\"âš™ï¸ Feature Extraction Configuration:\")\n",
    "for key, val in CONFIG.items():\n",
    "    print(f\"   {key}: {val}\")\n",
    "\n",
    "# åŠ è½½æ•°æ®é›†å…ƒæ•°æ®\n",
    "dataset_path = DATA_DIR / 'supervised_dataset.csv'\n",
    "samples_df = pd.read_csv(dataset_path)\n",
    "\n",
    "if 'sample_id' not in samples_df.columns:\n",
    "    samples_df['sample_id'] = [f\"SAMPLE_{i:06d}\" for i in range(len(samples_df))]\n",
    "\n",
    "print(f\"\\nâœ… Dataset loaded: {len(samples_df):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: ç‰¹å¾æå–å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_lightcurve(\n",
    "    time: np.ndarray,\n",
    "    flux: np.ndarray,\n",
    "    period: float = 1.0,\n",
    "    duration: float = 0.1,\n",
    "    depth: float = 0.01\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"ä»å…‰æ›²çº¿æå–ç‰¹å¾\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # åŸºç¡€ç»Ÿè®¡\n",
    "    features['flux_mean'] = float(np.nanmean(flux))\n",
    "    features['flux_std'] = float(np.nanstd(flux))\n",
    "    features['flux_median'] = float(np.nanmedian(flux))\n",
    "    features['flux_mad'] = float(np.nanmedian(np.abs(flux - np.nanmedian(flux))))\n",
    "    features['n_points'] = len(time)\n",
    "    features['time_span'] = float(time[-1] - time[0])\n",
    "    \n",
    "    # è¾“å…¥å‚æ•°\n",
    "    features['input_period'] = float(period)\n",
    "    features['input_duration'] = float(duration)\n",
    "    features['input_depth'] = float(depth)\n",
    "    \n",
    "    # BLS åˆ†æ\n",
    "    try:\n",
    "        bls = BoxLeastSquares(time, flux)\n",
    "        periods = np.linspace(\n",
    "            CONFIG['period_min'], \n",
    "            CONFIG['period_max'], \n",
    "            CONFIG['bls_periods']\n",
    "        )\n",
    "        durations = np.linspace(0.05, 0.5, CONFIG['bls_durations'])\n",
    "        bls_result = bls.power(periods, durations)\n",
    "        \n",
    "        best_idx = np.argmax(bls_result.power)\n",
    "        features['bls_power'] = float(bls_result.power[best_idx])\n",
    "        features['bls_period'] = float(bls_result.period[best_idx])\n",
    "        features['bls_duration'] = float(bls_result.duration[best_idx])\n",
    "        features['bls_depth'] = float(bls_result.depth[best_idx])\n",
    "        features['bls_snr'] = float(bls_result.depth_snr[best_idx])\n",
    "        \n",
    "    except Exception:\n",
    "        # BLS å¤±è´¥åˆ™ä½¿ç”¨é»˜è®¤å€¼\n",
    "        features.update({\n",
    "            'bls_power': 0.0,\n",
    "            'bls_period': period,\n",
    "            'bls_duration': duration,\n",
    "            'bls_depth': depth,\n",
    "            'bls_snr': 0.0\n",
    "        })\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def process_single_file(file_path: Path, metadata: pd.Series) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    å¤„ç†å•ä¸ªå…‰æ›²çº¿æ–‡ä»¶\n",
    "    \n",
    "    Args:\n",
    "        file_path: å…‰æ›²çº¿ pickle æ–‡ä»¶è·¯å¾„\n",
    "        metadata: æ ·æœ¬å…ƒæ•°æ®ï¼ˆæ¥è‡ª supervised_dataset.csvï¼‰\n",
    "    \n",
    "    Returns:\n",
    "        ç‰¹å¾å­—å…¸ï¼Œå¤±è´¥åˆ™è¿”å› None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # åŠ è½½å…‰æ›²çº¿æ•°æ®\n",
    "        data = joblib.load(file_path)\n",
    "        lc_collection = data['lc_collection']\n",
    "        \n",
    "        # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰‡åŒºçš„å…‰æ›²çº¿\n",
    "        lc = lc_collection[0].remove_nans().normalize()\n",
    "        \n",
    "        # æå–ç‰¹å¾\n",
    "        features = extract_features_from_lightcurve(\n",
    "            lc.time.value,\n",
    "            lc.flux.value,\n",
    "            metadata.get('period', 1.0),\n",
    "            metadata.get('duration', 0.1),\n",
    "            metadata.get('depth', 0.01)\n",
    "        )\n",
    "        \n",
    "        # æ·»åŠ å…ƒæ•°æ®\n",
    "        features['sample_id'] = data['sample_id']\n",
    "        features['tic_id'] = data['tic_id']\n",
    "        features['label'] = metadata.get('label', 0)\n",
    "        features['n_sectors'] = data['n_sectors']\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"âœ… Feature extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: æ‰¹é‡ç‰¹å¾æå–ï¼ˆä¸»è¦æ‰§è¡Œï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½ checkpoint\n",
    "checkpoint_path = CHECKPOINT_DIR / 'features_checkpoint.parquet'\n",
    "if checkpoint_path.exists():\n",
    "    features_df = pd.read_parquet(checkpoint_path)\n",
    "    print(f\"ğŸ“‚ Loaded checkpoint: {len(features_df)} features\")\n",
    "else:\n",
    "    features_df = pd.DataFrame()\n",
    "    print(\"ğŸ†• Starting fresh\")\n",
    "\n",
    "# ç¡®å®šå¾…å¤„ç†æ–‡ä»¶\n",
    "if len(features_df) > 0:\n",
    "    processed_ids = set(features_df['sample_id'])\n",
    "    lc_files_todo = [f for f in lc_files if f.stem.split('_')[0] not in processed_ids]\n",
    "else:\n",
    "    lc_files_todo = lc_files\n",
    "\n",
    "print(f\"\\nğŸ“Š Extraction Progress:\")\n",
    "print(f\"   Total files: {len(lc_files):,}\")\n",
    "print(f\"   Already processed: {len(lc_files) - len(lc_files_todo):,}\")\n",
    "print(f\"   Remaining: {len(lc_files_todo):,}\")\n",
    "\n",
    "if len(lc_files_todo) == 0:\n",
    "    print(\"\\nâœ… All features already extracted!\")\n",
    "else:\n",
    "    print(f\"\\nğŸš€ Starting feature extraction\")\n",
    "    print(f\"   Workers: {CONFIG['max_workers']}\")\n",
    "    print(f\"   Estimated time: {len(lc_files_todo) * 2 / 60 / CONFIG['max_workers']:.1f} minutes\")\n",
    "    \n",
    "    # å‡†å¤‡ä»»åŠ¡\n",
    "    tasks = []\n",
    "    for lc_file in lc_files_todo:\n",
    "        sample_id = lc_file.stem.split('_TIC')[0]\n",
    "        metadata = samples_df[samples_df['sample_id'] == sample_id].iloc[0]\n",
    "        tasks.append((lc_file, metadata))\n",
    "    \n",
    "    new_features = []\n",
    "    \n",
    "    # å¹¶è¡Œå¤„ç†\n",
    "    with ProcessPoolExecutor(max_workers=CONFIG['max_workers']) as executor:\n",
    "        future_to_task = {\n",
    "            executor.submit(process_single_file, lc_file, metadata): (lc_file, metadata)\n",
    "            for lc_file, metadata in tasks\n",
    "        }\n",
    "        \n",
    "        with tqdm(total=len(tasks), desc=\"Extracting\") as pbar:\n",
    "            for future in as_completed(future_to_task):\n",
    "                result = future.result()\n",
    "                if result is not None:\n",
    "                    new_features.append(result)\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # å®šæœŸä¿å­˜\n",
    "                if len(new_features) % CONFIG['save_interval'] == 0 and len(new_features) > 0:\n",
    "                    temp_df = pd.concat([features_df, pd.DataFrame(new_features)], ignore_index=True)\n",
    "                    temp_df.to_parquet(checkpoint_path, index=False)\n",
    "                    pbar.set_postfix({'saved': len(new_features)})\n",
    "    \n",
    "    # æœ€ç»ˆä¿å­˜\n",
    "    if len(new_features) > 0:\n",
    "        features_df = pd.concat([features_df, pd.DataFrame(new_features)], ignore_index=True)\n",
    "        features_df.to_parquet(checkpoint_path, index=False)\n",
    "        print(f\"\\nğŸ’¾ Saved {len(features_df):,} features\")\n",
    "\n",
    "# å®šä¹‰ç‰¹å¾åˆ—\n",
    "feature_cols = [col for col in features_df.columns \n",
    "                if col not in ['sample_id', 'tic_id', 'label', 'n_sectors']]\n",
    "\n",
    "print(f\"\\nâœ… Feature extraction complete\")\n",
    "print(f\"   Total features: {len(features_df):,}\")\n",
    "print(f\"   Feature columns: {len(feature_cols)}\")\n",
    "print(f\"   Features: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: æ•°æ®è´¨é‡æ£€æŸ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ” Data Quality Check:\\n\")\n",
    "\n",
    "# ç¼ºå¤±å€¼ç»Ÿè®¡\n",
    "print(\"ğŸ“Š Missing values:\")\n",
    "missing = features_df[feature_cols].isnull().sum()\n",
    "if missing.sum() == 0:\n",
    "    print(\"   âœ… No missing values!\")\n",
    "else:\n",
    "    for col, count in missing[missing > 0].items():\n",
    "        pct = count / len(features_df) * 100\n",
    "        print(f\"   {col}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# æ ‡ç­¾åˆ†å¸ƒ\n",
    "print(\"\\nğŸ“Š Label distribution:\")\n",
    "label_counts = features_df['label'].value_counts()\n",
    "print(f\"   Positive (1): {label_counts.get(1, 0):,} ({label_counts.get(1, 0)/len(features_df)*100:.1f}%)\")\n",
    "print(f\"   Negative (0): {label_counts.get(0, 0):,} ({label_counts.get(0, 0)/len(features_df)*100:.1f}%)\")\n",
    "\n",
    "# ç‰¹å¾ç»Ÿè®¡\n",
    "print(\"\\nğŸ“Š Feature statistics (sample):\")\n",
    "print(features_df[feature_cols].describe().iloc[:, :5])  # æ˜¾ç¤ºå‰ 5 ä¸ªç‰¹å¾\n",
    "\n",
    "# å¼‚å¸¸å€¼æ£€æŸ¥\n",
    "print(\"\\nğŸ” Checking for infinities and extreme values...\")\n",
    "X = features_df[feature_cols].values\n",
    "n_inf = np.isinf(X).sum()\n",
    "n_extreme = (np.abs(X) > 1e10).sum()\n",
    "\n",
    "if n_inf > 0:\n",
    "    print(f\"   âš ï¸ Found {n_inf} infinity values\")\n",
    "if n_extreme > 0:\n",
    "    print(f\"   âš ï¸ Found {n_extreme} extreme values (>1e10)\")\n",
    "if n_inf == 0 and n_extreme == 0:\n",
    "    print(\"   âœ… No infinities or extreme values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: ä¿å­˜æœ€ç»ˆç‰¹å¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# ä¿å­˜ç‰¹å¾æ•°æ®\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "features_path = MODEL_DIR / f'features_{timestamp}.parquet'\n",
    "features_df.to_parquet(features_path, index=False)\n",
    "print(f\"âœ… Features saved: {features_path}\")\n",
    "\n",
    "# ä¿å­˜ç‰¹å¾åˆ—å®šä¹‰\n",
    "feature_cols_path = MODEL_DIR / f'feature_columns_{timestamp}.json'\n",
    "with open(feature_cols_path, 'w') as f:\n",
    "    json.dump(feature_cols, f, indent=2)\n",
    "print(f\"âœ… Feature columns saved: {feature_cols_path}\")\n",
    "\n",
    "# ä¿å­˜æå–æŠ¥å‘Š\n",
    "report = {\n",
    "    'timestamp': timestamp,\n",
    "    'total_samples': len(features_df),\n",
    "    'n_features': len(feature_cols),\n",
    "    'feature_names': feature_cols,\n",
    "    'label_distribution': {\n",
    "        'positive': int(label_counts.get(1, 0)),\n",
    "        'negative': int(label_counts.get(0, 0))\n",
    "    },\n",
    "    'config': CONFIG,\n",
    "    'data_quality': {\n",
    "        'missing_values': int(missing.sum()),\n",
    "        'infinities': int(n_inf),\n",
    "        'extreme_values': int(n_extreme)\n",
    "    }\n",
    "}\n",
    "\n",
    "report_path = MODEL_DIR / f'extraction_report_{timestamp}.json'\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "print(f\"âœ… Report saved: {report_path}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ All files saved to: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… ç‰¹å¾æå–å®Œæˆï¼\n",
    "\n",
    "### ä¸‹ä¸€æ­¥:\n",
    "1. è¿è¡Œ `03_injection_train.ipynb` æˆ– `03_injection_train_PRODUCTION.ipynb` è®­ç»ƒæ¨¡å‹\n",
    "2. å¯ä»¥ä¿®æ”¹ Cell 4 çš„é…ç½®å°è¯•ä¸åŒçš„ç‰¹å¾æå–ç­–ç•¥\n",
    "3. é‡æ–°è¿è¡Œåªéœ€è¦ Cell 6ï¼Œä¸éœ€è¦é‡æ–°ä¸‹è½½å…‰æ›²çº¿\n",
    "\n",
    "### è¾“å‡ºæ–‡ä»¶:\n",
    "- **ç‰¹å¾æ•°æ®**: `models/features_{timestamp}.parquet`\n",
    "- **ç‰¹å¾åˆ—å®šä¹‰**: `models/feature_columns_{timestamp}.json`\n",
    "- **æå–æŠ¥å‘Š**: `models/extraction_report_{timestamp}.json`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
