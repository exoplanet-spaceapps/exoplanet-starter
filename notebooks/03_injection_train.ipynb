{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 · 合成注入訓練 · Logistic + Isotonic\n",
    "建立（正/負）樣本 → 特徵 → 訓練 → 可靠度校準"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 環境設定與依賴安裝（Colab）\nimport sys, subprocess, pkgutil\n\ndef pipi(*pkgs):\n    \"\"\"安裝套件的輔助函式\"\"\"\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *pkgs])\n\n# 安裝必要套件（避免 numpy 2.0 相容性問題）\nprint(\"🚀 正在安裝依賴套件...\")\ntry:\n    import numpy as np\n    import lightkurve as lk\n    import sklearn\n    print(\"✅ 基礎套件已安裝\")\nexcept Exception:\n    pipi(\"numpy<2\", \"lightkurve\", \"astroquery\", \"scikit-learn\", \"matplotlib\", \n         \"wotan\", \"transitleastsquares\", \"joblib\", \"torch\")\n    print(\"✅ 依賴套件安裝完成\")\n\n# 檢查 GPU 資訊\nimport torch if 'torch' in [m.name for m in pkgutil.iter_modules()] else None\ngpu_available = False\nis_l4_gpu = False\n\nif torch is not None and torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    print(f\"🖥️ GPU 型號: {gpu_name}\")\n    print(f\"   記憶體: {gpu_memory:.2f} GB\")\n    gpu_available = True\n    \n    # 如果是 NVIDIA L4，提供 BF16 優化建議\n    if \"L4\" in gpu_name:\n        is_l4_gpu = True\n        print(\"💡 偵測到 NVIDIA L4 GPU - 支援高效能 BF16 運算\")\n        print(\"   建議在訓練時使用 torch.autocast('cuda', dtype=torch.bfloat16)\")\n        print(\"\\n   BF16 範例程式碼：\")\n        print(\"   ```python\")\n        print(\"   with torch.autocast('cuda', dtype=torch.bfloat16):\")\n        print(\"       output = model(input)\")\n        print(\"       loss = criterion(output, target)\")\n        print(\"   ```\")\nelse:\n    try:\n        # 使用 nvidia-smi 檢查 GPU\n        result = subprocess.run(['nvidia-smi', '--query-gpu=name', '--format=csv,noheader'], \n                              capture_output=True, text=True, check=False)\n        if result.returncode == 0:\n            gpu_name = result.stdout.strip()\n            print(f\"🖥️ GPU 型號: {gpu_name}\")\n            gpu_available = True\n            if \"L4\" in gpu_name:\n                is_l4_gpu = True\n                print(\"💡 偵測到 NVIDIA L4 GPU - 支援高效能 BF16 運算\")\n    except:\n        print(\"⚠️ 未偵測到 GPU，將使用 CPU 運算\")\n\nprint(\"\\n環境設定完成！\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import lightkurve as lk, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "import json, os, joblib\n",
    "\n",
    "# 下載一條「大致無凌日」的光曲線作負類基底（示意，可換多條）\n",
    "target = \"TIC 25155310\"\n",
    "lc = lk.search_lightcurve(target, mission=\"TESS\", author=\"SPOC\").download().remove_nans()\n",
    "flat = lc.flatten(window_length=401)\n",
    "t = flat.time.value\n",
    "y = flat.flux.value\n",
    "\n",
    "# 建立資料集：注入 200 個正樣本 + 200 個負樣本\n",
    "rng = np.random.default_rng(42)\n",
    "def inject_box(time, flux, period, depth, duration, t0):\n",
    "    model = flux.copy()\n",
    "    phase = ((time - t0) % period) / period\n",
    "    in_transit = (phase < (duration/period))\n",
    "    model[in_transit] *= (1.0 - depth)\n",
    "    return model\n",
    "\n",
    "def bls_feats(time, flux):\n",
    "    bls = lk.LightCurve(time=t, flux=flux).to_periodogram(method=\"bls\", minimum_period=0.5, maximum_period=20)\n",
    "    return dict(\n",
    "        bls_period=bls.period_at_max_power.value,\n",
    "        bls_t0=bls.transit_time_at_max_power.value,\n",
    "        bls_duration=bls.duration_at_max_power.value,\n",
    "        bls_snr=bls.max_power.value)\n",
    "\n",
    "X, ylab = [], []\n",
    "for _ in range(200):\n",
    "    # 正樣本：注入隨機參數\n",
    "    P = rng.uniform(0.6, 10)\n",
    "    D = rng.uniform(0.0005, 0.02)   # 0.05%~2% 深度\n",
    "    W = rng.uniform(0.02, 0.1)      # 週期比例持續時間\n",
    "    T0 = t.min() + rng.uniform(0, P)\n",
    "    fx = inject_box(t, y, P, D, W*P, T0)\n",
    "    X.append(bls_feats(t, fx)); ylab.append(1)\n",
    "\n",
    "for _ in range(200):\n",
    "    # 負樣本：原曲線或加噪\n",
    "    noise = rng.normal(0, np.std(y)*0.2, size=y.size)\n",
    "    fx = y + noise\n",
    "    X.append(bls_feats(t, fx)); ylab.append(0)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(X); df['label']=ylab\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 訓練 + 校準\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\nX = df[['bls_period','bls_duration','bls_snr']].values\ny = df['label'].values\nXtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n# 基礎模型訓練（scikit-learn）\nbase = LogisticRegression(max_iter=200)\nclf = CalibratedClassifierCV(base, method=\"isotonic\", cv=3)\npipe = make_pipeline(StandardScaler(with_mean=False), clf)\npipe.fit(Xtr, ytr)\nprobs = pipe.predict_proba(Xte)[:,1]\nap = average_precision_score(yte, probs)\nprint(f\"Average Precision Score: {ap:.3f}\")\n\n# 若使用 PyTorch 神經網路進行深度學習（範例）\n# 在 NVIDIA L4 GPU 上使用 BF16 加速\nif 'is_l4_gpu' in locals() and is_l4_gpu and 'torch' in sys.modules:\n    import torch\n    import torch.nn as nn\n    \n    print(\"\\n🚀 PyTorch BF16 訓練範例（針對 NVIDIA L4 優化）:\")\n    \n    class SimpleNN(nn.Module):\n        def __init__(self, input_dim=3):\n            super().__init__()\n            self.fc1 = nn.Linear(input_dim, 64)\n            self.fc2 = nn.Linear(64, 32)\n            self.fc3 = nn.Linear(32, 1)\n            self.relu = nn.ReLU()\n            self.sigmoid = nn.Sigmoid()\n        \n        def forward(self, x):\n            x = self.relu(self.fc1(x))\n            x = self.relu(self.fc2(x))\n            x = self.sigmoid(self.fc3(x))\n            return x\n    \n    # 示範 BF16 自動混合精度訓練\n    model = SimpleNN().cuda()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.BCELoss()\n    \n    # 轉換資料到 PyTorch tensors\n    X_tensor = torch.FloatTensor(Xtr).cuda()\n    y_tensor = torch.FloatTensor(ytr).unsqueeze(1).cuda()\n    \n    # 使用 BF16 autocast 進行訓練（NVIDIA L4 優化）\n    print(\"使用 BF16 混合精度訓練...\")\n    model.train()\n    for epoch in range(10):\n        with torch.autocast('cuda', dtype=torch.bfloat16):\n            outputs = model(X_tensor)\n            loss = criterion(outputs, y_tensor)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if epoch % 5 == 0:\n            print(f\"  Epoch {epoch}, Loss: {loss.item():.4f}\")\n    \n    print(\"✅ BF16 訓練完成（使用 NVIDIA L4 硬體加速）\")\nelse:\n    print(\"\\n📝 提示：若需要使用深度學習模型，可安裝 PyTorch 並在 GPU 上執行\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 可靠度曲線\n",
    "from sklearn.calibration import calibration_curve\n",
    "prob_true, prob_pred = calibration_curve(yte, probs, n_bins=10)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(prob_pred, prob_true, marker=\"o\"); plt.plot([0,1],[0,1],'--'); plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(\"Calibration\");"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "mojyq8ry3ng",
   "source": "## Performance Tips\n\n### BF16 vs FP16 vs FP32 精度比較\n\n當使用 GPU 進行深度學習訓練時，選擇適當的數值精度可以顯著影響訓練速度與模型精確度：\n\n| 精度類型 | 位元數 | 優點 | 缺點 | 建議使用情境 |\n|---------|-------|------|------|------------|\n| **FP32** | 32-bit | • 最高精度<br>• 最佳數值穩定性 | • 記憶體使用量大<br>• 運算速度慢 | • 小型模型<br>• 需要最高精度的科學計算 |\n| **FP16** | 16-bit | • 記憶體減半<br>• 2-4x 加速（支援硬體） | • 可能溢位/欠位<br>• 梯度消失風險 | • 影像分類<br>• 已調校良好的模型 |\n| **BF16** | 16-bit | • 與 FP32 相同的指數範圍<br>• 更好的數值穩定性<br>• 2-4x 加速（L4/A100） | • 精度略低於 FP32<br>• 需要新硬體支援 | • **推薦用於 NVIDIA L4**<br>• 大型語言模型<br>• 天文資料處理 |\n\n### NVIDIA L4 GPU 優化建議\n\nNVIDIA L4 GPU 特別優化了 BF16 運算，在保持數值穩定性的同時提供顯著的效能提升：\n\n```python\n# BF16 自動混合精度訓練（推薦）\nwith torch.autocast('cuda', dtype=torch.bfloat16):\n    outputs = model(inputs)\n    loss = criterion(outputs, targets)\nloss.backward()\noptimizer.step()\n```\n\n### 何時改回 FP32\n\n在以下情況建議改回 FP32 精度：\n\n1. **最終推論階段**：當需要最精確的預測結果時\n2. **小批次訓練**：batch_size < 8 時，加速效果不明顯\n3. **數值不穩定**：出現 NaN 或 Inf 時\n4. **精密科學計算**：例如軌道參數計算、凌日時間精確預測\n\n```python\n# 切換回 FP32 精度\nmodel.float()  # 將模型轉回 FP32\nwith torch.no_grad():\n    outputs = model(inputs.float())\n```\n\n### 外行星偵測特定優化\n\n對於外行星凌日偵測任務：\n- **特徵提取**：使用 FP32 以保持光曲線資料精度\n- **神經網路訓練**：使用 BF16 加速訓練過程\n- **BLS/TLS 週期搜尋**：維持 FP64 以獲得準確的週期估計\n- **最終機率校準**：使用 FP32 確保可靠度分數準確",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 持久化模型\n",
    "os.makedirs(\"/content/model\", exist_ok=True)\n",
    "import joblib, json\n",
    "joblib.dump(pipe, \"/content/model/ranker.joblib\")\n",
    "with open(\"/content/model/feature_schema.json\",\"w\") as f:\n",
    "    json.dump({\"features\": ['bls_period','bls_duration','bls_snr']}, f)\n",
    "print(\"Saved to /content/model/\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}