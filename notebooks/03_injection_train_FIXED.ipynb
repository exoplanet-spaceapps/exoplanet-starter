{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Transit Injection & Supervised Learning (FIXED)\n",
    "\n",
    "## Fixed Issues\n",
    "- Removed 7 duplicate cells\n",
    "- Consolidated imports into single cell\n",
    "- Fixed import order\n",
    "- Added proper environment checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check environment\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========================================\n# PACKAGE INSTALLATION (Colab)\n# ========================================\n\n# Install required packages if not available\ntry:\n    import lightkurve\n    print('lightkurve already installed')\nexcept ImportError:\n    print('Installing lightkurve and dependencies...')\n    import subprocess\n    import sys\n    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', \n                          'lightkurve', 'astroquery', 'wotan', 'pyarrow'])\n    print('Installation complete')\n\n# ========================================\n# COMPREHENSIVE IMPORTS\n# ========================================\n\n# Standard library\nimport sys\nimport os\nimport json\nimport time\nimport warnings\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Optional, Any\n\n# Data processing\nimport numpy as np\nimport pandas as pd\n\n# Machine learning\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedGroupKFold\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.calibration import CalibratedClassifierCV, calibration_curve\nfrom sklearn.metrics import (accuracy_score, precision_recall_fscore_support, \n                            roc_auc_score, roc_curve, precision_recall_curve, \n                            confusion_matrix, classification_report, \n                            average_precision_score, brier_score_loss)\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\n\n# XGBoost\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\n# Astronomy\nimport lightkurve as lk\n\n# SHAP\nimport shap\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.gridspec import GridSpec\n\n# Model persistence\nimport joblib\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\nprint('All imports completed successfully')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup paths\nif IN_COLAB:\n    # Clone repository if needed\n    import os\n    import subprocess\n\n    PROJECT_ROOT = Path('/content/exoplanet-starter')\n\n    if not PROJECT_ROOT.exists():\n        print(\"Cloning repository...\")\n        subprocess.run([\n            'git', 'clone',\n            'https://github.com/exoplanet-spaceapps/exoplanet-starter.git',\n            str(PROJECT_ROOT)\n        ], check=True)\n        print(\"Repository cloned successfully\")\n    else:\n        print(f\"Repository already exists at {PROJECT_ROOT}\")\n\n    # Change to project directory\n    os.chdir(str(PROJECT_ROOT))\nelse:\n    PROJECT_ROOT = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n\nsys.path.insert(0, str(PROJECT_ROOT))\nsys.path.insert(0, str(PROJECT_ROOT / 'app'))\n\nprint(f\"Project root: {PROJECT_ROOT}\")\nprint(f\"Python path: {sys.path[:3]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import project modules\ntry:\n    from app.bls_features import run_bls, extract_features, extract_features_batch, compute_feature_importance\n    from app.injection import inject_box_transit, generate_synthetic_dataset, generate_transit_parameters, save_synthetic_dataset\n    print(\"âœ… Project modules loaded successfully\")\nexcept ImportError as e:\n    import sys\n    print(\"âŒ ERROR: Could not import project modules!\")\n    print(f\"   Import error: {e}\")\n    print(f\"   Current directory: {Path.cwd()}\")\n    print(f\"   Python path: {sys.path[:5]}\")\n    print()\n    print(\"ğŸ’¡ Troubleshooting:\")\n    print(\"   1. Make sure you executed Cell 3 (Setup paths)\")\n    print(\"   2. Verify the repository was cloned successfully\")\n    print(\"   3. Check that 'app' directory exists in PROJECT_ROOT\")\n    print()\n\n    # Check if app directory exists\n    if 'PROJECT_ROOT' in locals():\n        app_dir = PROJECT_ROOT / 'app'\n        if not app_dir.exists():\n            print(f\"   âš ï¸ WARNING: {app_dir} does not exist!\")\n        else:\n            print(f\"   âœ… {app_dir} exists\")\n            print(f\"      Contents: {list(app_dir.glob('*.py'))[:5]}\")\n\n    raise ImportError(f\"Failed to import project modules. Please check Cell 3 setup. Error: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ä¸‹è¼‰çœŸå¯¦å…‰æ›²ç·šä½œç‚ºåŸºç¤\n",
    "print(\"ğŸ“¡ ä¸‹è¼‰åŸºç¤å…‰æ›²ç·š...\")\n",
    "\n",
    "try:\n",
    "    # ä½¿ç”¨ TIC 25155310 (TOI-431) ä½œç‚ºåŸºç¤\n",
    "    target = \"TIC 25155310\"\n",
    "    search_result = lk.search_lightcurve(target, mission=\"TESS\", author=\"SPOC\")\n",
    "    lc = search_result[0].download()\n",
    "\n",
    "    # æ¸…ç†å’Œå»è¶¨å‹¢\n",
    "    lc_clean = lc.remove_nans()\n",
    "    lc_flat = lc_clean.flatten(window_length=401)\n",
    "\n",
    "    base_time = lc_flat.time.value\n",
    "    base_flux = lc_flat.flux.value\n",
    "\n",
    "    print(f\"âœ… æˆåŠŸä¸‹è¼‰ {target}\")\n",
    "    print(f\"   è³‡æ–™é»æ•¸: {len(base_time)}\")\n",
    "    print(f\"   æ™‚é–“è·¨åº¦: {base_time[-1] - base_time[0]:.1f} å¤©\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ ç„¡æ³•ä¸‹è¼‰çœŸå¯¦å…‰æ›²ç·š: {e}\")\n",
    "    print(\"   ä½¿ç”¨æ¨¡æ“¬å…‰æ›²ç·š...\")\n",
    "\n",
    "    # ç”Ÿæˆæ¨¡æ“¬å…‰æ›²ç·šï¼ˆ27å¤© TESS è§€æ¸¬ï¼‰\n",
    "    base_time = np.linspace(0, 27, 20000)\n",
    "    base_flux = np.ones(20000) + np.random.normal(0, 0.0001, 20000)\n",
    "\n",
    "    print(f\"âœ… ç”Ÿæˆæ¨¡æ“¬å…‰æ›²ç·š\")\n",
    "    print(f\"   è³‡æ–™é»æ•¸: {len(base_time)}\")\n",
    "    print(f\"   æ™‚é–“è·¨åº¦: {base_time[-1] - base_time[0]:.1f} å¤©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ç”Ÿæˆåˆæˆè³‡æ–™é›†\n",
    "print(\"\\nğŸ”¨ ç”Ÿæˆåˆæˆè³‡æ–™é›†...\")\n",
    "print(\"   åƒæ•¸ç¯„åœï¼š\")\n",
    "print(\"   â€¢ é€±æœŸ: 0.6 - 10.0 å¤©\")\n",
    "print(\"   â€¢ æ·±åº¦: 0.0005 - 0.02 (500 - 20000 ppm)\")\n",
    "print(\"   â€¢ æŒçºŒæ™‚é–“: é€±æœŸçš„ 2% - 10%\")\n",
    "\n",
    "samples_df, labels_df = generate_synthetic_dataset(\n",
    "    base_time=base_time,\n",
    "    base_flux=base_flux,\n",
    "    n_positive=200,\n",
    "    n_negative=200,\n",
    "    period_range=(0.6, 10.0),\n",
    "    depth_range=(0.0005, 0.02),\n",
    "    duration_fraction_range=(0.02, 0.1),\n",
    "    noise_level=0.0001,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… ç”Ÿæˆ {len(samples_df)} å€‹æ¨£æœ¬\")\n",
    "print(f\"   æ­£æ¨£æœ¬ï¼ˆæœ‰å‡Œæ—¥ï¼‰: {len(samples_df[samples_df['label'] == 1])}\")\n",
    "print(f\"   è² æ¨£æœ¬ï¼ˆç„¡å‡Œæ—¥ï¼‰: {len(samples_df[samples_df['label'] == 0])}\")\n",
    "\n",
    "# å„²å­˜è³‡æ–™é›†\n",
    "dataset_paths = save_synthetic_dataset(\n",
    "    samples_df,\n",
    "    labels_df,\n",
    "    output_dir=\"data/synthetic\",\n",
    "    format=\"parquet\"\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ’¾ è³‡æ–™é›†å·²å„²å­˜è‡³:\")\n",
    "for key, path in dataset_paths.items():\n",
    "    print(f\"   {key}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# æå–ç‰¹å¾µ\n",
    "print(\"ğŸ” é–‹å§‹æ‰¹æ¬¡ç‰¹å¾µæå–...\")\n",
    "print(\"   é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜æ™‚é–“...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# æ‰¹æ¬¡æå–ç‰¹å¾µ\n",
    "features_df = extract_features_batch(\n",
    "    samples_df,\n",
    "    compute_advanced=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâœ… ç‰¹å¾µæå–å®Œæˆ\")\n",
    "print(f\"   è€—æ™‚: {elapsed_time:.1f} ç§’\")\n",
    "print(f\"   å¹³å‡æ¯å€‹æ¨£æœ¬: {elapsed_time/len(samples_df):.2f} ç§’\")\n",
    "print(f\"   æå–ç‰¹å¾µæ•¸: {len(features_df.columns) - 2}\")  # æ‰£é™¤ sample_id å’Œ label\n",
    "\n",
    "# é¡¯ç¤ºç‰¹å¾µåˆ—è¡¨\n",
    "feature_cols = [col for col in features_df.columns if col not in ['sample_id', 'label']]\n",
    "print(f\"\\nğŸ“‹ ç‰¹å¾µåˆ—è¡¨:\")\n",
    "for i, feat in enumerate(feature_cols, 1):\n",
    "    print(f\"   {i:2d}. {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "# æº–å‚™è¨“ç·´è³‡æ–™# å‰ç½®æ¢ä»¶æª¢æŸ¥if 'features_df' not in locals():    raise RuntimeError(        \"éŒ¯èª¤: features_df æœªå®šç¾©ï¼\\n\"        \"\\n\"        \"è«‹æŒ‰ç…§ä»¥ä¸‹é †åºåŸ·è¡Œ cells:\\n\"        \"   1. Cell 1-4: ç’°å¢ƒè¨­ç½®èˆ‡å°å…¥æ¨¡çµ„\\n\"        \"   2. Cell 5: ä¸‹è¼‰åŸºç¤å…‰æ›²ç·š\\n\"        \"   3. Cell 6: ç”Ÿæˆåˆæˆè³‡æ–™é›† (samples_df)\\n\"        \"   4. Cell 7: æå–ç‰¹å¾µ (features_df) <- å¿…é ˆå…ˆåŸ·è¡Œé€™å€‹ï¼\\n\"        \"   5. Cell 8: æº–å‚™è¨“ç·´è³‡æ–™ (æœ¬ cell)\\n\"        \"\\n\"        \"æç¤º: åœ¨ Colab ä¸­ï¼Œå¯ä»¥ç”¨ Runtime -> Run all ä¾†æŒ‰é †åºåŸ·è¡Œæ‰€æœ‰ cells\"    )if 'feature_cols' not in locals():    raise RuntimeError(        \"éŒ¯èª¤: feature_cols æœªå®šç¾©ï¼\\n\"        \"è«‹ç¢ºä¿ Cell 5 å·²æˆåŠŸåŸ·è¡Œä¸¦ç”Ÿæˆäº† feature_cols\"    )print(\"å‰ç½®æ¢ä»¶æª¢æŸ¥é€šé\")print(f\"   features_df: {features_df.shape}\")print(f\"   feature_cols: {len(feature_cols)} å€‹ç‰¹å¾µ\\n\")X = features_df[feature_cols].valuesy = features_df['label'].values# è™•ç†ç„¡æ•ˆå€¼X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)# åˆ†å‰²è¨“ç·´é›†å’Œæ¸¬è©¦é›†X_train, X_test, y_train, y_test = train_test_split(    X, y, test_size=0.3, random_state=42, stratify=y)# æ¨™æº–åŒ–ç‰¹å¾µscaler = StandardScaler()X_train_scaled = scaler.fit_transform(X_train)X_test_scaled = scaler.transform(X_test)print(\"è³‡æ–™é›†çµ±è¨ˆ:\")print(f\"   è¨“ç·´é›†: {len(X_train)} æ¨£æœ¬\")print(f\"   æ¸¬è©¦é›†: {len(X_test)} æ¨£æœ¬\")print(f\"   æ­£æ¨£æœ¬æ¯”ä¾‹ (è¨“ç·´): {y_train.mean():.2%}\")print(f\"   æ­£æ¨£æœ¬æ¯”ä¾‹ (æ¸¬è©¦): {y_test.mean():.2%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Transit Injection & Supervised Learning (FIXED)\n",
    "\n",
    "## Fixed Issues\n",
    "- Removed 7 duplicate cells\n",
    "- Consolidated imports into single cell\n",
    "- Fixed import order\n",
    "- Added proper environment checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# è¦–è¦ºåŒ–åƒæ•¸åˆ†å¸ƒ\n",
    "positive_labels = labels_df[labels_df['label'] == 1]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# é€±æœŸåˆ†å¸ƒ\n",
    "axes[0, 0].hist(positive_labels['period'], bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('é€±æœŸ (å¤©)')\n",
    "axes[0, 0].set_ylabel('æ•¸é‡')\n",
    "axes[0, 0].set_title('é€±æœŸåˆ†å¸ƒ')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# æ·±åº¦åˆ†å¸ƒ\n",
    "axes[0, 1].hist(positive_labels['depth'] * 1e6, bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[0, 1].set_xlabel('æ·±åº¦ (ppm)')\n",
    "axes[0, 1].set_ylabel('æ•¸é‡')\n",
    "axes[0, 1].set_title('å‡Œæ—¥æ·±åº¦åˆ†å¸ƒ')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# æŒçºŒæ™‚é–“åˆ†å¸ƒ\n",
    "axes[1, 0].hist(positive_labels['duration'] * 24, bins=30, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[1, 0].set_xlabel('æŒçºŒæ™‚é–“ (å°æ™‚)')\n",
    "axes[1, 0].set_ylabel('æ•¸é‡')\n",
    "axes[1, 0].set_title('å‡Œæ—¥æŒçºŒæ™‚é–“åˆ†å¸ƒ')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# SNR åˆ†å¸ƒ\n",
    "axes[1, 1].hist(positive_labels['snr_estimate'], bins=30, edgecolor='black', alpha=0.7, color='red')\n",
    "axes[1, 1].set_xlabel('SNR ä¼°è¨ˆ')\n",
    "axes[1, 1].set_ylabel('æ•¸é‡')\n",
    "axes[1, 1].set_title('ä¿¡å™ªæ¯”åˆ†å¸ƒ')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('åˆæˆå‡Œæ—¥åƒæ•¸åˆ†å¸ƒ', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# çµ±è¨ˆæ‘˜è¦\n",
    "print(\"\\nğŸ“Š åƒæ•¸çµ±è¨ˆæ‘˜è¦ï¼š\")\n",
    "print(positive_labels[['period', 'depth', 'duration', 'snr_estimate']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# è¨“ç·´å¤šå€‹æ¨¡å‹\n",
    "models = {}\n",
    "print(\"ğŸš€ é–‹å§‹è¨“ç·´æ¨¡å‹...\\n\")\n",
    "\n",
    "# 1. Logistic Regression\n",
    "print(\"1ï¸âƒ£ è¨“ç·´ Logistic Regression...\")\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "models['LogisticRegression'] = lr_model\n",
    "print(f\"   è¨“ç·´åˆ†æ•¸: {lr_model.score(X_train_scaled, y_train):.3f}\")\n",
    "print(f\"   æ¸¬è©¦åˆ†æ•¸: {lr_model.score(X_test_scaled, y_test):.3f}\")\n",
    "\n",
    "# 2. Random Forest\n",
    "print(\"\\n2ï¸âƒ£ è¨“ç·´ Random Forest...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)  # Random Forest ä¸éœ€è¦æ¨™æº–åŒ–\n",
    "models['RandomForest'] = rf_model\n",
    "print(f\"   è¨“ç·´åˆ†æ•¸: {rf_model.score(X_train, y_train):.3f}\")\n",
    "print(f\"   æ¸¬è©¦åˆ†æ•¸: {rf_model.score(X_test, y_test):.3f}\")\n",
    "\n",
    "# 3. XGBoost\n",
    "print(\"\\n3ï¸âƒ£ è¨“ç·´ XGBoost...\")\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "models['XGBoost'] = xgb_model\n",
    "print(f\"   è¨“ç·´åˆ†æ•¸: {xgb_model.score(X_train, y_train):.3f}\")\n",
    "print(f\"   æ¸¬è©¦åˆ†æ•¸: {xgb_model.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Phase 3-4: æº–å‚™è³‡æ–™èˆ‡ç‰¹å¾µ (with grouping)\n",
    "print(\"ğŸ“Š Phase 3-4: æº–å‚™è¨“ç·´è³‡æ–™èˆ‡ StratifiedGroupKFold\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# æå–ç‰¹å¾µå’Œæ¨™ç±¤\n",
    "X = features_df[feature_cols].values\n",
    "y = features_df['label'].values\n",
    "\n",
    "# è™•ç†ç„¡æ•ˆå€¼ (NaN, Inf)\n",
    "X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# å‰µå»º target groups (å°æ–¼åˆæˆè³‡æ–™ï¼Œæˆ‘å€‘ç”¨ sample_id çš„å‰ç¶´ä½œç‚º group)\n",
    "# é€™æ¨¡æ“¬äº†\"åŒä¸€ç›®æ¨™çš„å¤šæ¬¡è§€æ¸¬\"\n",
    "if 'sample_id' in features_df.columns:\n",
    "    # å¾ sample_id æå– group (ä¾‹å¦‚: \"sample_0001_obs1\" -> group \"0001\")\n",
    "    groups = features_df['sample_id'].apply(\n",
    "        lambda x: int(str(x).split('_')[1]) if '_' in str(x) else hash(str(x)) % 1000\n",
    "    ).values\n",
    "else:\n",
    "    # å¦‚æœæ²’æœ‰ sample_idï¼Œå‰µå»ºå‡çš„ groups (æ¯å€‹æ¨£æœ¬ä¸€çµ„)\n",
    "    groups = np.arange(len(y))\n",
    "\n",
    "print(f\"   ç¸½æ¨£æœ¬æ•¸: {len(X)}\")\n",
    "print(f\"   ç‰¹å¾µç¶­åº¦: {X.shape[1]}\")\n",
    "print(f\"   æ­£æ¨£æœ¬æ¯”ä¾‹: {y.mean():.2%}\")\n",
    "print(f\"   å”¯ä¸€ groups æ•¸: {len(np.unique(groups))}\")\n",
    "print()\n",
    "\n",
    "# Log GPU info\n",
    "print(\"ğŸ–¥ï¸  GPU é…ç½®:\")\n",
    "log_gpu_info()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# å»ºç«‹ä¸¦å„²å­˜ç‰¹å¾µæ¶æ§‹\n",
    "feature_schema = create_feature_schema(\n",
    "    feature_cols,\n",
    "    output_path=\"data/feature_schema.json\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ“ ç‰¹å¾µæ¶æ§‹å·²å»ºç«‹\")\n",
    "print(f\"   ç‰¹å¾µæ•¸é‡: {feature_schema['n_features']}\")\n",
    "print(f\"   ç‰ˆæœ¬: {feature_schema['version']}\")\n",
    "print(f\"   å„²å­˜ä½ç½®: data/feature_schema.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ğŸ“Š è¨“ç·´ç®¡ç·šåŸ·è¡Œç¸½çµ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\"\"\n",
    "ğŸ¯ è³‡æ–™é›†:\n",
    "   â€¢ ç¸½æ¨£æœ¬æ•¸: {len(samples_df)}\n",
    "   â€¢ æ­£æ¨£æœ¬: {len(samples_df[samples_df['label'] == 1])}\n",
    "   â€¢ è² æ¨£æœ¬: {len(samples_df[samples_df['label'] == 0])}\n",
    "\n",
    "ğŸ” ç‰¹å¾µå·¥ç¨‹:\n",
    "   â€¢ ç‰¹å¾µæ•¸é‡: {len(feature_cols)}\n",
    "   â€¢ Top 3 é‡è¦ç‰¹å¾µ:\n",
    "\"\"\")\n",
    "\n",
    "for idx, row in importance_df.head(3).iterrows():\n",
    "    print(f\"     - {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "ğŸ¤– æ¨¡å‹æ•ˆèƒ½:\n",
    "   â€¢ PR-AUC: {metrics_calibrated['PR-AUC']:.3f}\n",
    "   â€¢ ROC-AUC: {metrics_calibrated['ROC-AUC']:.3f}\n",
    "   â€¢ Brier Score: {metrics_calibrated['Brier Score']:.3f}\n",
    "   â€¢ ECE: {metrics_calibrated['ECE']:.3f}\n",
    "   â€¢ Precision@10: {metrics_calibrated.get('P@10', 'N/A')}\n",
    "\n",
    "ğŸ’¡ é—œéµç™¼ç¾:\n",
    "   1. Isotonic æ ¡æº–é¡¯è‘—æ”¹å–„äº†æ©Ÿç‡é æ¸¬çš„å¯é æ€§\n",
    "   2. BLS ç‰¹å¾µï¼ˆé€±æœŸã€SNRã€æ·±åº¦ï¼‰æ˜¯æœ€é‡è¦çš„é æ¸¬å› å­\n",
    "   3. æ¨¡å‹åœ¨é«˜ç½®ä¿¡åº¦é æ¸¬ä¸Šè¡¨ç¾å„ªç•°ï¼ˆé«˜ Precision@Kï¼‰\n",
    "\n",
    "ğŸ“¦ è¼¸å‡ºæª”æ¡ˆ:\n",
    "   â€¢ æ¨¡å‹: model/ranker.joblib\n",
    "   â€¢ æ¨™æº–åŒ–å™¨: model/scaler.joblib\n",
    "   â€¢ ç‰¹å¾µæ¶æ§‹: model/feature_schema.json\n",
    "   â€¢ å…ƒè³‡æ–™: model/model_metadata.json\n",
    "   â€¢ åˆæˆè³‡æ–™: data/synthetic/\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"âœ… è¨“ç·´ç®¡ç·šå®Œæˆï¼\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# å¾çœŸå¯¦å…‰æ›²ç·šæå–ç‰¹å¾µ\n",
    "if 'supervised_samples_df' in locals() and len(supervised_samples_df) > 0:\n",
    "    print(\"ğŸ” æå–çœŸå¯¦è³‡æ–™ç‰¹å¾µ...\")\n",
    "\n",
    "    supervised_features = []\n",
    "\n",
    "    for idx, row in supervised_samples_df.iterrows():\n",
    "        # åŸ·è¡Œ BLS\n",
    "        bls_result = run_bls(row['time'], row['flux'])\n",
    "\n",
    "        # æå–ç‰¹å¾µ\n",
    "        features = extract_features(row['time'], row['flux'], bls_result, compute_advanced=True)\n",
    "        features['sample_id'] = row['sample_id']\n",
    "        features['label'] = row['label']\n",
    "        features['source'] = row['source']\n",
    "        features['true_period'] = row['period']\n",
    "\n",
    "        supervised_features.append(features)\n",
    "\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"   è™•ç†é€²åº¦: {idx+1}/{len(supervised_samples_df)}\")\n",
    "\n",
    "    supervised_features_df = pd.DataFrame(supervised_features)\n",
    "    print(f\"\\nâœ… ç‰¹å¾µæå–å®Œæˆ: {len(supervised_features_df)} å€‹æ¨£æœ¬\")\n",
    "\n",
    "    # é¡¯ç¤ºç‰¹å¾µçµ±è¨ˆ\n",
    "    print(\"\\nğŸ“Š çœŸå¯¦è³‡æ–™ç‰¹å¾µçµ±è¨ˆ:\")\n",
    "    feature_cols_real = [col for col in supervised_features_df.columns\n",
    "                         if col not in ['sample_id', 'label', 'source', 'true_period']]\n",
    "    print(supervised_features_df[feature_cols_real].describe())\n",
    "else:\n",
    "    print(\"âš ï¸ ç„¡çœŸå¯¦æ¨£æœ¬å¯ç”¨æ–¼ç›£ç£å¼å­¸ç¿’\")\n",
    "    supervised_features_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ğŸ“Š å®Œæ•´è¨“ç·´ç®¡ç·šåŸ·è¡Œç¸½çµ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "ğŸ¯ è³‡æ–™é›†çµ±è¨ˆ:\n",
    "\n",
    "   ã€åˆæˆæ³¨å…¥è³‡æ–™ã€‘\n",
    "   â€¢ ç¸½æ¨£æœ¬æ•¸: {len(samples_df)}\n",
    "   â€¢ æ­£æ¨£æœ¬: {len(samples_df[samples_df['label'] == 1])}\n",
    "   â€¢ è² æ¨£æœ¬: {len(samples_df[samples_df['label'] == 0])}\n",
    "\n",
    "   ã€çœŸå¯¦ç›£ç£è³‡æ–™ã€‘\n",
    "   â€¢ ç¸½æ¨£æœ¬æ•¸: {len(supervised_features_df) if 'supervised_features_df' in locals() else 0}\n",
    "   â€¢ TOI æ­£æ¨£æœ¬: {len(supervised_features_df[supervised_features_df['source']=='TOI']) if 'supervised_features_df' in locals() and len(supervised_features_df) > 0 else 0}\n",
    "   â€¢ Kepler EB è² æ¨£æœ¬: {len(supervised_features_df[supervised_features_df['source']=='Kepler_EB']) if 'supervised_features_df' in locals() and len(supervised_features_df) > 0 else 0}\n",
    "\n",
    "ğŸ” ç‰¹å¾µå·¥ç¨‹:\n",
    "   â€¢ ç‰¹å¾µæ•¸é‡: {len(feature_cols)}\n",
    "   â€¢ Top 3 é‡è¦ç‰¹å¾µ:\n",
    "\"\"\")\n",
    "\n",
    "for idx, row in importance_df.head(3).iterrows():\n",
    "    print(f\"     - {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "ğŸ¤– æ¨¡å‹æ•ˆèƒ½æ¯”è¼ƒ:\n",
    "\n",
    "   ã€åˆæˆæ³¨å…¥æ–¹æ³•ã€‘\n",
    "   â€¢ PR-AUC: {metrics_calibrated['PR-AUC']:.3f}\n",
    "   â€¢ ROC-AUC: {metrics_calibrated['ROC-AUC']:.3f}\n",
    "   â€¢ Brier Score: {metrics_calibrated['Brier Score']:.3f}\n",
    "   â€¢ ECE: {metrics_calibrated['ECE']:.3f}\n",
    "\"\"\")\n",
    "\n",
    "if 'metrics_supervised' in locals() and metrics_supervised is not None:\n",
    "    print(f\"\"\"\n",
    "   ã€ç›£ç£å¼å­¸ç¿’ã€‘\n",
    "   â€¢ PR-AUC: {metrics_supervised['PR-AUC']:.3f}\n",
    "   â€¢ ROC-AUC: {metrics_supervised['ROC-AUC']:.3f}\n",
    "   â€¢ Brier Score: {metrics_supervised['Brier Score']:.3f}\n",
    "   â€¢ ECE: {metrics_supervised['ECE']:.3f}\n",
    "    \"\"\")\n",
    "\n",
    "print(f\"\"\"\n",
    "ğŸ’¡ é—œéµç™¼ç¾èˆ‡å»ºè­°:\n",
    "   1. Isotonic æ ¡æº–é¡¯è‘—æ”¹å–„äº†æ©Ÿç‡é æ¸¬çš„å¯é æ€§\n",
    "   2. BLS ç‰¹å¾µï¼ˆé€±æœŸã€SNRã€æ·±åº¦ï¼‰æ˜¯æœ€é‡è¦çš„é æ¸¬å› å­\n",
    "   3. åˆæˆæ³¨å…¥é©åˆå¿«é€Ÿé–‹ç™¼ï¼Œç›£ç£å¼å­¸ç¿’æ›´æ¥è¿‘å¯¦éš›æ‡‰ç”¨\n",
    "   4. å»ºè­°åœ¨å¯¦éš›éƒ¨ç½²æ™‚çµåˆå…©ç¨®æ–¹æ³•çš„å„ªå‹¢\n",
    "\n",
    "ğŸ“¦ è¼¸å‡ºæª”æ¡ˆ:\n",
    "   â€¢ åˆæˆæ¨¡å‹: model/ranker.joblib\n",
    "   â€¢ ç›£ç£æ¨¡å‹: model/supervised/ranker_supervised.joblib\n",
    "   â€¢ ç‰¹å¾µæ¶æ§‹: model/feature_schema.json\n",
    "   â€¢ æ¯”è¼ƒçµæœ: model/method_comparison.csv\n",
    "   â€¢ è³‡æ–™é›†: data/synthetic/ å’Œ data/\n",
    "\n",
    "ğŸš€ ä¸‹ä¸€æ­¥:\n",
    "   1. ä½¿ç”¨ 04_newdata_inference.ipynb å°æ–°è³‡æ–™é€²è¡Œæ¨è«–\n",
    "   2. åœ¨æ›´å¤§çš„çœŸå¯¦è³‡æ–™é›†ä¸Šè¨“ç·´ç›£ç£å¼æ¨¡å‹\n",
    "   3. æ¢ç´¢æ·±åº¦å­¸ç¿’æ–¹æ³•ï¼ˆCNN/Transformerï¼‰\n",
    "   4. éƒ¨ç½²ç‚º Web æ‡‰ç”¨æˆ– API æœå‹™\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"âœ… è¨“ç·´ç®¡ç·šï¼ˆå«ç›£ç£å¼åˆ†æ”¯ï¼‰å®Œæˆï¼\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Summary of Phase 7-8 outputs\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ“‹ Phase 7-8 Summary\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nâœ… SHAP Explainability (Phase 7):\")\n",
    "print(f\"   â€¢ Summary plot: {shap_plot_path}\")\n",
    "print(f\"   â€¢ Top feature: {feature_importance.iloc[0]['feature']}\")\n",
    "print(f\"   â€¢ Features analyzed: {len(features)}\")\n",
    "\n",
    "print(\"\\nâœ… Probability Calibration (Phase 8):\")\n",
    "print(f\"   â€¢ Best method: {best_method[0]}\")\n",
    "print(f\"   â€¢ Brier improvement: {(brier_uncal - best_method[1])/brier_uncal*100:.2f}%\")\n",
    "print(f\"   â€¢ Calibration curves: {calibration_plot_path}\")\n",
    "print(f\"   â€¢ Model card: {model_card_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log top 15 feature importance values\n",
    "mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'mean_abs_shap': mean_abs_shap\n",
    "}).sort_values('mean_abs_shap', ascending=False)\n",
    "\n",
    "print(\"\\nğŸ“Š Top 15 Features by SHAP Importance:\")\n",
    "print(feature_importance.head(15).to_string(index=False))\n",
    "\n",
    "# Store for later use\n",
    "shap_importance = feature_importance.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create and save SHAP summary plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(\n",
    "    shap_values,\n",
    "    X_test_sample,\n",
    "    feature_names=features,\n",
    "    max_display=15,\n",
    "    show=False\n",
    ")\n",
    "\n",
    "# Save plot\n",
    "reports_dir = Path('reports')\n",
    "reports_dir.mkdir(exist_ok=True)\n",
    "shap_plot_path = reports_dir / 'shap_summary.png'\n",
    "plt.tight_layout()\n",
    "plt.savefig(shap_plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… SHAP summary plot saved to: {shap_plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create SHAP explainer for XGBoost model\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ” SHAP Feature Importance Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use TreeExplainer for XGBoost\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "\n",
    "# Calculate SHAP values for test set (limit to 500 samples for performance)\n",
    "X_test_sample = X_test[:500]\n",
    "shap_values = explainer.shap_values(X_test_sample)\n",
    "\n",
    "print(f\"âœ… SHAP values computed for {len(X_test_sample)} test samples\")\n",
    "print(f\"   Shape: {shap_values.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ç¹ªè£½å¯é åº¦æ›²ç·š\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# æœªæ ¡æº–æ¨¡å‹\n",
    "fraction_pos_uncal, mean_pred_uncal = calibration_curve(\n",
    "    y_test, prob_uncalibrated, n_bins=10\n",
    ")\n",
    "\n",
    "axes[0].plot(mean_pred_uncal, fraction_pos_uncal, 'o-', label='æœªæ ¡æº–', color='red')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='å®Œç¾æ ¡æº–')\n",
    "axes[0].set_xlabel('å¹³å‡é æ¸¬æ©Ÿç‡')\n",
    "axes[0].set_ylabel('å¯¦éš›æ­£æ¨£æœ¬æ¯”ä¾‹')\n",
    "axes[0].set_title('æœªæ ¡æº–æ¨¡å‹å¯é åº¦æ›²ç·š', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(loc='best')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# å·²æ ¡æº–æ¨¡å‹\n",
    "fraction_pos_cal, mean_pred_cal = calibration_curve(\n",
    "    y_test, prob_calibrated, n_bins=10\n",
    ")\n",
    "\n",
    "axes[1].plot(mean_pred_cal, fraction_pos_cal, 'o-', label='å·²æ ¡æº–', color='green')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', label='å®Œç¾æ ¡æº–')\n",
    "axes[1].set_xlabel('å¹³å‡é æ¸¬æ©Ÿç‡')\n",
    "axes[1].set_ylabel('å¯¦éš›æ­£æ¨£æœ¬æ¯”ä¾‹')\n",
    "axes[1].set_title('å·²æ ¡æº–æ¨¡å‹å¯é åº¦æ›²ç·š', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(loc='best')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('æ©Ÿç‡æ ¡æº–æ•ˆæœæ¯”è¼ƒ', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ èªªæ˜:\")\n",
    "print(\"   â€¢ ç†æƒ³çš„å¯é åº¦æ›²ç·šæ‡‰è©²æ¥è¿‘å°è§’ç·š\")\n",
    "print(\"   â€¢ æ›²ç·šåœ¨å°è§’ç·šä¸Šæ–¹è¡¨ç¤ºæ¨¡å‹éåº¦ä¿å®ˆ\")\n",
    "print(\"   â€¢ æ›²ç·šåœ¨å°è§’ç·šä¸‹æ–¹è¡¨ç¤ºæ¨¡å‹éåº¦è‡ªä¿¡\")\n",
    "print(\"   â€¢ Isotonic æ ¡æº–æœ‰æ•ˆæ”¹å–„äº†æ¨¡å‹çš„æ©Ÿç‡é æ¸¬\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# è¨ˆç®—ç‰¹å¾µé‡è¦æ€§\n",
    "print(\"ğŸ¯ è¨ˆç®—ç‰¹å¾µé‡è¦æ€§...\")\n",
    "\n",
    "importance_df = compute_feature_importance(\n",
    "    features_df,\n",
    "    features_df['label'].values,\n",
    "    method=\"random_forest\"\n",
    ")\n",
    "\n",
    "# è¦–è¦ºåŒ–ç‰¹å¾µé‡è¦æ€§\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "top_features = importance_df.head(10)\n",
    "bars = ax.barh(range(len(top_features)), top_features['importance'].values)\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['feature'].values)\n",
    "ax.set_xlabel('é‡è¦æ€§åˆ†æ•¸')\n",
    "ax.set_title('ç‰¹å¾µé‡è¦æ€§æ’å (Top 10)', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# æ·»åŠ æ•¸å€¼æ¨™ç±¤\n",
    "for i, (bar, val) in enumerate(zip(bars, top_features['importance'].values)):\n",
    "    ax.text(val, bar.get_y() + bar.get_height()/2, f'{val:.3f}',\n",
    "            ha='left', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ† Top 5 æœ€é‡è¦ç‰¹å¾µ:\")\n",
    "for idx, row in importance_df.head(5).iterrows():\n",
    "    print(f\"   {idx+1}. {row['feature']}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ğŸŒ Environment Detection\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = 'google.colab' in sys.modules or '/content' in os.getcwd()\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"ğŸŒ Running in: Google Colab\")\n",
    "\n",
    "    # Clone repo if needed\n",
    "    project_dir = Path('/content/exoplanet-starter')\n",
    "    if not project_dir.exists():\n",
    "        print(\"ğŸ“¥ Cloning repository...\")\n",
    "        !git clone https://github.com/exoplanet-spaceapps/exoplanet-starter.git\n",
    "        print(\"âœ… Repository cloned\")\n",
    "\n",
    "    # Change to project directory\n",
    "    os.chdir(str(project_dir))\n",
    "\n",
    "    # Add to Python path\n",
    "    sys.path.insert(0, str(project_dir))\n",
    "    sys.path.insert(0, str(project_dir / 'src'))\n",
    "    sys.path.insert(0, str(project_dir / 'notebooks'))\n",
    "\n",
    "    print(f\"ğŸ“‚ Working directory: {os.getcwd()}\")\n",
    "    print(f\"âœ… Python path configured\")\n",
    "\n",
    "else:\n",
    "    print(\"ğŸ’» Running in: Local environment\")\n",
    "    # Local paths\n",
    "    project_dir = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "    sys.path.insert(0, str(project_dir / 'src'))\n",
    "    sys.path.insert(0, str(project_dir))\n",
    "\n",
    "print(f\"ğŸ“ Project directory: {project_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ç’°å¢ƒè¨­å®šèˆ‡ä¾è³´å®‰è£ï¼ˆColabï¼‰\n",
    "import sys, subprocess, pkgutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def pipi(*pkgs):\n",
    "    \"\"\"å®‰è£å¥—ä»¶çš„è¼”åŠ©å‡½å¼\"\"\"\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *pkgs])\n",
    "\n",
    "# å®‰è£å¿…è¦å¥—ä»¶ï¼ˆé¿å… numpy 2.0 ç›¸å®¹æ€§å•é¡Œï¼‰\n",
    "print(\"ğŸš€ æ­£åœ¨å®‰è£ä¾è³´å¥—ä»¶...\")\n",
    "try:\n",
    "    import numpy as np\n",
    "    import lightkurve as lk\n",
    "    import sklearn\n",
    "    import xgboost\n",
    "    print(\"âœ… åŸºç¤å¥—ä»¶å·²å®‰è£\")\n",
    "except Exception:\n",
    "    pipi(\"numpy<2\", \"lightkurve\", \"astroquery\", \"scikit-learn\",\n",
    "         \"matplotlib\", \"seaborn\", \"xgboost\", \"joblib\", \"pandas\", \"pyarrow\")\n",
    "    print(\"âœ… ä¾è³´å¥—ä»¶å®‰è£å®Œæˆ\")\n",
    "\n",
    "# æª¢æŸ¥æ˜¯å¦åœ¨ Colab ç’°å¢ƒ\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"ğŸ“ åœ¨ Google Colab ç’°å¢ƒåŸ·è¡Œ\")\n",
    "    # Clone repository if needed\n",
    "    import os\n",
    "    if not os.path.exists('/content/exoplanet-starter'):\n",
    "        !git clone https://github.com/exoplanet-spaceapps/exoplanet-starter.git /content/exoplanet-starter\n",
    "        os.chdir('/content/exoplanet-starter')\n",
    "    sys.path.append('/content/exoplanet-starter')\n",
    "else:\n",
    "    print(\"ğŸ’» åœ¨æœ¬åœ°ç’°å¢ƒåŸ·è¡Œ\")\n",
    "    import os\n",
    "    os.chdir(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "    sys.path.append(os.getcwd())\n",
    "\n",
    "print(\"\\nç’°å¢ƒè¨­å®šå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# é¸æ“‡æœ€ä½³æ¨¡å‹é€²è¡Œæ ¡æº–\n",
    "print(\"\\nğŸ¯ é€²è¡Œæ©Ÿç‡æ ¡æº–...\")\n",
    "\n",
    "# é¸æ“‡ XGBoost ä½œç‚ºåŸºç¤æ¨¡å‹\n",
    "base_model = models['XGBoost']\n",
    "\n",
    "# Isotonic æ ¡æº–\n",
    "print(\"   ä½¿ç”¨ Isotonic Regression æ ¡æº–...\")\n",
    "calibrated_model = CalibratedClassifierCV(\n",
    "    base_model,\n",
    "    method='isotonic',\n",
    "    cv=3\n",
    ")\n",
    "calibrated_model.fit(X_train, y_train)\n",
    "\n",
    "# ç²å–é æ¸¬æ©Ÿç‡\n",
    "prob_uncalibrated = base_model.predict_proba(X_test)[:, 1]\n",
    "prob_calibrated = calibrated_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"âœ… æ ¡æº–å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_prob, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    è¨ˆç®—å…¨é¢çš„è©•ä¼°æŒ‡æ¨™\n",
    "    \"\"\"\n",
    "    # PR-AUC\n",
    "    pr_auc = average_precision_score(y_true, y_prob)\n",
    "\n",
    "    # ROC-AUC\n",
    "    roc_auc = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "    # Brier Score\n",
    "    brier = brier_score_loss(y_true, y_prob)\n",
    "\n",
    "    # ECE (Expected Calibration Error)\n",
    "    n_bins = 10\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_indices = np.digitize(y_prob, bin_boundaries) - 1\n",
    "\n",
    "    ece = 0\n",
    "    for i in range(n_bins):\n",
    "        mask = bin_indices == i\n",
    "        if np.sum(mask) > 0:\n",
    "            bin_acc = np.mean(y_true[mask])\n",
    "            bin_conf = np.mean(y_prob[mask])\n",
    "            bin_size = np.sum(mask) / len(y_true)\n",
    "            ece += bin_size * np.abs(bin_acc - bin_conf)\n",
    "\n",
    "    # Precision@K\n",
    "    k_values = [10, 20, 50]\n",
    "    precision_at_k = {}\n",
    "    sorted_indices = np.argsort(y_prob)[::-1]\n",
    "\n",
    "    for k in k_values:\n",
    "        if k <= len(y_true):\n",
    "            top_k_true = y_true[sorted_indices[:k]]\n",
    "            precision_at_k[f'P@{k}'] = np.mean(top_k_true)\n",
    "\n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'PR-AUC': pr_auc,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'Brier Score': brier,\n",
    "        'ECE': ece,\n",
    "        **precision_at_k\n",
    "    }\n",
    "\n",
    "# è¨ˆç®—æ‰€æœ‰æŒ‡æ¨™\n",
    "metrics_uncalibrated = calculate_metrics(y_test, prob_uncalibrated, \"XGBoost (æœªæ ¡æº–)\")\n",
    "metrics_calibrated = calculate_metrics(y_test, prob_calibrated, \"XGBoost (å·²æ ¡æº–)\")\n",
    "\n",
    "# é¡¯ç¤ºçµæœ\n",
    "metrics_df = pd.DataFrame([metrics_uncalibrated, metrics_calibrated])\n",
    "print(\"\\nğŸ“Š æ¨¡å‹è©•ä¼°æŒ‡æ¨™:\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# æ”¹å–„æ¯”è¼ƒ\n",
    "print(\"\\nğŸ“ˆ æ ¡æº–æ”¹å–„:\")\n",
    "print(f\"   ECE æ”¹å–„: {(metrics_uncalibrated['ECE'] - metrics_calibrated['ECE'])/metrics_uncalibrated['ECE']*100:.1f}%\")\n",
    "print(f\"   Brier Score æ”¹å–„: {(metrics_uncalibrated['Brier Score'] - metrics_calibrated['Brier Score'])/metrics_uncalibrated['Brier Score']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ç¹ªè£½ PR æ›²ç·š\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# PR æ›²ç·š\n",
    "precision_uncal, recall_uncal, _ = precision_recall_curve(y_test, prob_uncalibrated)\n",
    "precision_cal, recall_cal, _ = precision_recall_curve(y_test, prob_calibrated)\n",
    "\n",
    "axes[0].plot(recall_uncal, precision_uncal, label=f'æœªæ ¡æº– (AP={metrics_uncalibrated[\"PR-AUC\"]:.3f})', color='red')\n",
    "axes[0].plot(recall_cal, precision_cal, label=f'å·²æ ¡æº– (AP={metrics_calibrated[\"PR-AUC\"]:.3f})', color='green')\n",
    "axes[0].set_xlabel('Recall')\n",
    "axes[0].set_ylabel('Precision')\n",
    "axes[0].set_title('Precision-Recall æ›²ç·š', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(loc='best')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision@K æŸ±ç‹€åœ–\n",
    "k_values = [10, 20, 50]\n",
    "precision_at_k_uncal = []\n",
    "precision_at_k_cal = []\n",
    "\n",
    "for k in k_values:\n",
    "    if f'P@{k}' in metrics_uncalibrated:\n",
    "        precision_at_k_uncal.append(metrics_uncalibrated[f'P@{k}'])\n",
    "        precision_at_k_cal.append(metrics_calibrated[f'P@{k}'])\n",
    "\n",
    "x = np.arange(len(k_values))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[1].bar(x - width/2, precision_at_k_uncal, width, label='æœªæ ¡æº–', color='red', alpha=0.7)\n",
    "bars2 = axes[1].bar(x + width/2, precision_at_k_cal, width, label='å·²æ ¡æº–', color='green', alpha=0.7)\n",
    "\n",
    "axes[1].set_xlabel('K')\n",
    "axes[1].set_ylabel('Precision@K')\n",
    "axes[1].set_title('Precision@K æ¯”è¼ƒ', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels([f'Top {k}' for k in k_values])\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# æ·»åŠ æ•¸å€¼æ¨™ç±¤\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.2f}',\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_and_process_lightcurve(target_id, mission=\"TESS\"):\n",
    "    \"\"\"\n",
    "    ä¸‹è¼‰ä¸¦è™•ç†å–®å€‹ç›®æ¨™çš„å…‰æ›²ç·š\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    target_id : str\n",
    "        ç›®æ¨™è­˜åˆ¥ç¢¼ï¼ˆTIC æˆ– KICï¼‰\n",
    "    mission : str\n",
    "        ä»»å‹™åç¨±ï¼ˆTESS æˆ– Keplerï¼‰\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (time, flux) æˆ– (None, None) å¦‚æœå¤±æ•—\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # æœå°‹å…‰æ›²ç·š\n",
    "        search_result = lk.search_lightcurve(target_id, mission=mission, author=\"SPOC\" if mission==\"TESS\" else \"Kepler\")\n",
    "        if len(search_result) == 0:\n",
    "            return None, None\n",
    "\n",
    "        # ä¸‹è¼‰ç¬¬ä¸€å€‹çµæœ\n",
    "        lc = search_result[0].download()\n",
    "\n",
    "        # æ¸…ç†å’Œå»è¶¨å‹¢\n",
    "        lc_clean = lc.remove_nans()\n",
    "        if len(lc_clean) < 100:\n",
    "            # å¤ªå°‘è³‡æ–™é»\n",
    "            return None, None\n",
    "\n",
    "        lc_flat = lc_clean.flatten(window_length=401)\n",
    "\n",
    "        return lc_flat.time.value, lc_flat.flux.value\n",
    "\n",
    "    except Exception as e:\n",
    "        return None, None\n",
    "\n",
    "# ç¤ºç¯„ï¼šè™•ç†éƒ¨åˆ†çœŸå¯¦æ¨£æœ¬\n",
    "print(\"ğŸ”¬ è™•ç†çœŸå¯¦å…‰æ›²ç·šæ¨£æœ¬...\")\n",
    "print(\"   ï¼ˆç‚ºç¯€çœæ™‚é–“ï¼Œåƒ…è™•ç†å‰ 50 å€‹æ¨£æœ¬ï¼‰\")\n",
    "\n",
    "supervised_samples = []\n",
    "supervised_labels = []\n",
    "\n",
    "# è™•ç† TOI æ­£æ¨£æœ¬ï¼ˆæœ€å¤š 25 å€‹ï¼‰\n",
    "n_toi_samples = min(25, len(toi_positive)) if 'toi_positive' in locals() else 0\n",
    "if n_toi_samples > 0:\n",
    "    print(f\"\\nè™•ç† {n_toi_samples} å€‹ TOI æ­£æ¨£æœ¬...\")\n",
    "    for idx, row in toi_positive.head(n_toi_samples).iterrows():\n",
    "        tic_id = f\"TIC {int(row['tid'])}\"\n",
    "        time_data, flux_data = download_and_process_lightcurve(tic_id, \"TESS\")\n",
    "\n",
    "        if time_data is not None:\n",
    "            supervised_samples.append({\n",
    "                'sample_id': f\"toi_{row['tid']}\",\n",
    "                'time': time_data,\n",
    "                'flux': flux_data,\n",
    "                'label': 1,\n",
    "                'period': row.get('pl_orbper', np.nan),\n",
    "                'source': 'TOI'\n",
    "            })\n",
    "            print(f\"   âœ“ {tic_id}\")\n",
    "        else:\n",
    "            print(f\"   âœ— {tic_id} (ç„¡æ³•ä¸‹è¼‰)\")\n",
    "\n",
    "# è™•ç† Kepler EB è² æ¨£æœ¬ï¼ˆæœ€å¤š 25 å€‹ï¼‰\n",
    "n_eb_samples = min(25, len(eb_negative)) if 'eb_negative' in locals() else 0\n",
    "if n_eb_samples > 0:\n",
    "    print(f\"\\nè™•ç† {n_eb_samples} å€‹ Kepler EB è² æ¨£æœ¬...\")\n",
    "    for idx, row in eb_negative.head(n_eb_samples).iterrows():\n",
    "        kic_id = f\"KIC {int(row['KIC'])}\"\n",
    "        time_data, flux_data = download_and_process_lightcurve(kic_id, \"Kepler\")\n",
    "\n",
    "        if time_data is not None:\n",
    "            supervised_samples.append({\n",
    "                'sample_id': f\"eb_{row['KIC']}\",\n",
    "                'time': time_data,\n",
    "                'flux': flux_data,\n",
    "                'label': 0,\n",
    "                'period': row.get('period', np.nan),\n",
    "                'source': 'Kepler_EB'\n",
    "            })\n",
    "            print(f\"   âœ“ {kic_id}\")\n",
    "        else:\n",
    "            print(f\"   âœ— {kic_id} (ç„¡æ³•ä¸‹è¼‰)\")\n",
    "\n",
    "supervised_samples_df = pd.DataFrame(supervised_samples) if supervised_samples else pd.DataFrame()\n",
    "print(f\"\\nâœ… æˆåŠŸè™•ç† {len(supervised_samples_df)} å€‹çœŸå¯¦æ¨£æœ¬\")\n",
    "if len(supervised_samples_df) > 0:\n",
    "    print(f\"   æ­£æ¨£æœ¬: {len(supervised_samples_df[supervised_samples_df['label']==1])}\")\n",
    "    print(f\"   è² æ¨£æœ¬: {len(supervised_samples_df[supervised_samples_df['label']==0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# æ¯”è¼ƒå…©ç¨®æ–¹æ³•çš„æ•ˆèƒ½\n",
    "print(\"ğŸ”¬ æ–¹æ³•æ¯”è¼ƒï¼šåˆæˆæ³¨å…¥ vs ç›£ç£å¼å­¸ç¿’\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'metrics_supervised' in locals() and metrics_supervised is not None:\n",
    "    # å»ºç«‹æ¯”è¼ƒè¡¨\n",
    "    comparison_df = pd.DataFrame([\n",
    "        metrics_calibrated,  # åˆæˆæ³¨å…¥æ–¹æ³•\n",
    "        metrics_supervised   # ç›£ç£å¼æ–¹æ³•\n",
    "    ])\n",
    "    comparison_df['Model'] = ['åˆæˆæ³¨å…¥', 'ç›£ç£å¼']\n",
    "\n",
    "    print(\"\\nğŸ“Š æ•ˆèƒ½æŒ‡æ¨™å°æ¯”:\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "\n",
    "    # è¦–è¦ºåŒ–æ¯”è¼ƒ\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "    # æŒ‡æ¨™åˆ—è¡¨\n",
    "    metrics_to_compare = ['PR-AUC', 'ROC-AUC', 'Brier Score', 'ECE', 'P@10', 'P@20']\n",
    "\n",
    "    for idx, metric in enumerate(metrics_to_compare):\n",
    "        row = idx // 3\n",
    "        col = idx % 3\n",
    "        ax = axes[row, col]\n",
    "\n",
    "        if metric in comparison_df.columns:\n",
    "            values = [\n",
    "                metrics_calibrated.get(metric, 0),\n",
    "                metrics_supervised.get(metric, 0)\n",
    "            ]\n",
    "            colors = ['blue', 'orange']\n",
    "            bars = ax.bar(['åˆæˆæ³¨å…¥', 'ç›£ç£å¼'], values, color=colors, alpha=0.7)\n",
    "\n",
    "            # æ·»åŠ æ•¸å€¼æ¨™ç±¤\n",
    "            for bar, val in zip(bars, values):\n",
    "                if val is not None and not pd.isna(val):\n",
    "                    ax.text(bar.get_x() + bar.get_width()/2., val,\n",
    "                           f'{val:.3f}',\n",
    "                           ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "            ax.set_title(metric, fontsize=12, fontweight='bold')\n",
    "            ax.set_ylabel('åˆ†æ•¸')\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "            # æ ¹æ“šæŒ‡æ¨™é¡å‹è¨­ç½® y è»¸ç¯„åœ\n",
    "            if metric in ['PR-AUC', 'ROC-AUC', 'P@10', 'P@20']:\n",
    "                ax.set_ylim([0, 1.1])\n",
    "            elif metric == 'ECE':\n",
    "                ax.set_ylim([0, 0.2])\n",
    "\n",
    "    plt.suptitle('åˆæˆæ³¨å…¥ vs ç›£ç£å¼å­¸ç¿’ æ•ˆèƒ½æ¯”è¼ƒ', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # å„ªåŠ£å‹¢åˆ†æ\n",
    "    print(\"\\nğŸ’¡ åˆ†æç¸½çµ:\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # è¨ˆç®—ç›¸å°æ”¹å–„\n",
    "    pr_auc_diff = (metrics_supervised['PR-AUC'] - metrics_calibrated['PR-AUC']) / metrics_calibrated['PR-AUC'] * 100\n",
    "    ece_diff = (metrics_calibrated['ECE'] - metrics_supervised['ECE']) / metrics_calibrated['ECE'] * 100\n",
    "\n",
    "    print(\"ğŸ“ˆ **åˆæˆæ³¨å…¥æ–¹æ³•**çš„å„ªå‹¢:\")\n",
    "    print(\"   â€¢ ä¸éœ€è¦å¤§é‡æ¨™è¨»è³‡æ–™\")\n",
    "    print(\"   â€¢ å¯ä»¥æ§åˆ¶è¨“ç·´æ¨£æœ¬çš„åƒæ•¸åˆ†å¸ƒ\")\n",
    "    print(\"   â€¢ é©åˆå¿«é€ŸåŸå‹é–‹ç™¼å’Œæ¸¬è©¦\")\n",
    "    print(f\"   â€¢ åœ¨æœ¬å¯¦é©—ä¸­ ECE: {metrics_calibrated['ECE']:.3f}\")\n",
    "\n",
    "    print(\"\\nğŸ“Š **ç›£ç£å¼å­¸ç¿’**çš„å„ªå‹¢:\")\n",
    "    print(\"   â€¢ ä½¿ç”¨çœŸå¯¦å¤©æ–‡è³‡æ–™ï¼Œæ›´æ¥è¿‘å¯¦éš›æ‡‰ç”¨\")\n",
    "    print(\"   â€¢ èƒ½å­¸ç¿’åˆ°çœŸå¯¦è³‡æ–™ä¸­çš„è¤‡é›œæ¨¡å¼\")\n",
    "    print(\"   â€¢ å°çœŸå¯¦å™ªéŸ³å’Œç³»çµ±èª¤å·®æœ‰æ›´å¥½çš„é­¯æ£’æ€§\")\n",
    "    print(f\"   â€¢ åœ¨æœ¬å¯¦é©—ä¸­ PR-AUC: {metrics_supervised['PR-AUC']:.3f}\")\n",
    "\n",
    "    if pr_auc_diff > 0:\n",
    "        print(f\"\\nğŸ† ç›£ç£å¼æ–¹æ³•åœ¨ PR-AUC ä¸Šæå‡äº† {pr_auc_diff:.1f}%\")\n",
    "    else:\n",
    "        print(f\"\\nğŸ† åˆæˆæ³¨å…¥æ–¹æ³•åœ¨ PR-AUC ä¸Šé ˜å…ˆ {-pr_auc_diff:.1f}%\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nâš ï¸ ç„¡æ³•é€²è¡Œæ¯”è¼ƒï¼ˆç›£ç£å¼æ¨¡å‹æœªè¨“ç·´ï¼‰\")\n",
    "    print(\"   åŸå› ï¼šçœŸå¯¦è³‡æ–™æ¨£æœ¬ä¸è¶³æˆ–ç„¡æ³•ä¸‹è¼‰\")\n",
    "    print(\"   å»ºè­°ï¼š\")\n",
    "    print(\"   1. ç¢ºä¿å·²åŸ·è¡Œ 01_tap_download.ipynb\")\n",
    "    print(\"   2. æª¢æŸ¥ç¶²è·¯é€£ç·š\")\n",
    "    print(\"   3. å¢åŠ è™•ç†çš„æ¨£æœ¬æ•¸é‡\")\n",
    "    comparison_df = None\n",
    "\n",
    "print(\"\\n=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# å»ºç«‹è¼¸å‡ºç›®éŒ„\n",
    "output_dir = Path(\"model\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# å„²å­˜æ¨¡å‹\n",
    "print(\"ğŸ’¾ å„²å­˜æ¨¡å‹èˆ‡ç›¸é—œæª”æ¡ˆ...\\n\")\n",
    "\n",
    "# 1. å„²å­˜æ ¡æº–æ¨¡å‹\n",
    "model_path = output_dir / \"ranker.joblib\"\n",
    "joblib.dump(calibrated_model, model_path)\n",
    "print(f\"âœ… æ¨¡å‹å·²å„²å­˜: {model_path}\")\n",
    "\n",
    "# 2. å„²å­˜ç‰¹å¾µæ¨™æº–åŒ–å™¨\n",
    "scaler_path = output_dir / \"scaler.joblib\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"âœ… æ¨™æº–åŒ–å™¨å·²å„²å­˜: {scaler_path}\")\n",
    "\n",
    "# 3. å„²å­˜ç‰¹å¾µæ¶æ§‹ (with fallback if file doesn't exist)\n",
    "import shutil\n",
    "feature_schema_source = Path(\"data/feature_schema.json\")\n",
    "if feature_schema_source.exists():\n",
    "    shutil.copy(feature_schema_source, output_dir / \"feature_schema.json\")\n",
    "    print(f\"âœ… ç‰¹å¾µæ¶æ§‹å·²è¤‡è£½: {output_dir / 'feature_schema.json'}\")\n",
    "else:\n",
    "    # Create feature schema from current feature_cols\n",
    "    feature_schema = {\n",
    "        \"features\": feature_cols,\n",
    "        \"n_features\": len(feature_cols),\n",
    "        \"created_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"note\": \"Generated from training data\"\n",
    "    }\n",
    "    schema_path = output_dir / \"feature_schema.json\"\n",
    "    with open(schema_path, 'w') as f:\n",
    "        json.dump(feature_schema, f, indent=2)\n",
    "    print(f\"âœ… ç‰¹å¾µæ¶æ§‹å·²ç”Ÿæˆ: {schema_path}\")\n",
    "\n",
    "# 4. å„²å­˜æ¨¡å‹å…ƒè³‡æ–™\n",
    "metadata = {\n",
    "    \"model_type\": \"XGBoost with Isotonic Calibration\",\n",
    "    \"training_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"n_features\": len(feature_cols),\n",
    "    \"feature_names\": feature_cols,\n",
    "    \"training_samples\": len(X_train),\n",
    "    \"test_samples\": len(X_test),\n",
    "    \"metrics\": metrics_calibrated,\n",
    "    \"parameters\": {\n",
    "        \"period_range\": [0.6, 10.0],\n",
    "        \"depth_range\": [0.0005, 0.02],\n",
    "        \"duration_fraction_range\": [0.02, 0.1]\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = output_dir / \"model_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "print(f\"âœ… å…ƒè³‡æ–™å·²å„²å­˜: {metadata_path}\")\n",
    "\n",
    "print(\"\\nğŸ“¦ æ‰€æœ‰æª”æ¡ˆå·²æˆåŠŸå„²å­˜è‡³ 'model/' ç›®éŒ„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'metrics_supervised' in locals() and metrics_supervised is not None:\n",
    "    print(\"ğŸ’¾ å„²å­˜ç›£ç£å¼æ¨¡å‹...\")\n",
    "\n",
    "    # å»ºç«‹è¼¸å‡ºç›®éŒ„\n",
    "    supervised_dir = Path(\"model/supervised\")\n",
    "    supervised_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # å„²å­˜æ¨¡å‹\n",
    "    joblib.dump(calibrated_supervised, supervised_dir / \"ranker_supervised.joblib\")\n",
    "    joblib.dump(scaler_supervised, supervised_dir / \"scaler_supervised.joblib\")\n",
    "\n",
    "    # å„²å­˜å…ƒè³‡æ–™\n",
    "    supervised_metadata = {\n",
    "        \"model_type\": \"XGBoost with Isotonic Calibration (Supervised)\",\n",
    "        \"training_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"data_sources\": {\n",
    "            \"positive\": \"TOI (PC/CP/KP)\",\n",
    "            \"negative\": \"Kepler EB + TOI FP\"\n",
    "        },\n",
    "        \"n_features\": len(feature_cols),\n",
    "        \"feature_names\": feature_cols,\n",
    "        \"training_samples\": len(X_train_sup),\n",
    "        \"test_samples\": len(X_test_sup),\n",
    "        \"metrics\": metrics_supervised\n",
    "    }\n",
    "\n",
    "    with open(supervised_dir / \"model_metadata.json\", 'w') as f:\n",
    "        json.dump(supervised_metadata, f, indent=2, default=str)\n",
    "\n",
    "    print(f\"âœ… ç›£ç£å¼æ¨¡å‹å·²å„²å­˜è‡³: {supervised_dir}\")\n",
    "\n",
    "    # å„²å­˜æ¯”è¼ƒçµæœ\n",
    "    if 'comparison_df' in locals() and comparison_df is not None:\n",
    "        comparison_df.to_csv(\"model/method_comparison.csv\", index=False)\n",
    "        print(\"âœ… æ–¹æ³•æ¯”è¼ƒçµæœå·²å„²å­˜è‡³: model/method_comparison.csv\")\n",
    "else:\n",
    "    print(\"âš ï¸ ç„¡ç›£ç£å¼æ¨¡å‹å¯å„²å­˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ğŸš€ GitHub Push çµ‚æ¥µè§£æ±ºæ–¹æ¡ˆ (03 - Synthetic Injection & Supervised Training Results)\n",
    "# ä¸€éµæ¨é€åˆæˆæ³¨å…¥èˆ‡ç›£ç£å¼è¨“ç·´çµæœè‡³ GitHub\n",
    "\n",
    "import subprocess, os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def ultimate_push_to_github_03(token=None):\n",
    "    \"\"\"\n",
    "    çµ‚æ¥µä¸€éµæ¨é€è§£æ±ºæ–¹æ¡ˆ - åˆæˆæ³¨å…¥èˆ‡ç›£ç£å¼è¨“ç·´çµæœç‰ˆ\n",
    "    è§£æ±ºæ‰€æœ‰ Colab èˆ‡æœ¬åœ°ç’°å¢ƒçš„ Git/LFS å•é¡Œ\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"ğŸš€ åˆæˆæ³¨å…¥èˆ‡ç›£ç£å¼è¨“ç·´çµæœ GitHub æ¨é€é–‹å§‹...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # æ­¥é©Ÿ 1: ç’°å¢ƒåµæ¸¬èˆ‡è¨­å®š\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        IN_COLAB = True\n",
    "        working_dir = \"/content\"\n",
    "        print(\"ğŸŒ åµæ¸¬åˆ° Google Colab ç’°å¢ƒ\")\n",
    "    except ImportError:\n",
    "        IN_COLAB = False\n",
    "        working_dir = os.getcwd()\n",
    "        print(\"ğŸ’» åµæ¸¬åˆ°æœ¬åœ°ç’°å¢ƒ\")\n",
    "\n",
    "    # æ­¥é©Ÿ 2: Token è¼¸å…¥\n",
    "    if not token:\n",
    "        print(\"ğŸ“‹ è«‹è¼¸å…¥ GitHub Personal Access Token:\")\n",
    "        print(\"   1. å‰å¾€ https://github.com/settings/tokens\")\n",
    "        print(\"   2. é»æ“Š 'Generate new token (classic)'\")\n",
    "        print(\"   3. å‹¾é¸ 'repo' æ¬Šé™\")\n",
    "        print(\"   4. è¤‡è£½ç”Ÿæˆçš„ token\")\n",
    "        token = input(\"ğŸ” è²¼ä¸Šä½ çš„ token (ghp_...): \").strip()\n",
    "        if not token.startswith('ghp_'):\n",
    "            print(\"âŒ Token æ ¼å¼éŒ¯èª¤ï¼Œæ‡‰è©²ä»¥ 'ghp_' é–‹é ­\")\n",
    "            return False\n",
    "\n",
    "    # æ­¥é©Ÿ 3: Git å€‰åº«åˆå§‹åŒ–èˆ‡è¨­å®š\n",
    "    print(\"\\nğŸ“‹ æ­¥é©Ÿ 1/4: Git å€‰åº«è¨­å®š...\")\n",
    "\n",
    "    try:\n",
    "        # åˆ‡æ›åˆ°å·¥ä½œç›®éŒ„\n",
    "        if IN_COLAB:\n",
    "            os.chdir(working_dir)\n",
    "\n",
    "        # æª¢æŸ¥æ˜¯å¦å·²æ˜¯ Git å€‰åº«\n",
    "        git_check = subprocess.run(['git', 'rev-parse', '--git-dir'],\n",
    "                                   capture_output=True, text=True)\n",
    "\n",
    "        if git_check.returncode != 0:\n",
    "            print(\"   ğŸ”§ åˆå§‹åŒ– Git å€‰åº«...\")\n",
    "            subprocess.run(['git', 'init'], check=True)\n",
    "            print(\"   âœ… Git å€‰åº«åˆå§‹åŒ–å®Œæˆ\")\n",
    "        else:\n",
    "            print(\"   âœ… å·²åœ¨ Git å€‰åº«ä¸­\")\n",
    "\n",
    "        # è¨­å®š Git ç”¨æˆ¶ï¼ˆå¦‚æœæœªè¨­å®šï¼‰\n",
    "        try:\n",
    "            subprocess.run(['git', 'config', 'user.name', 'Colab User'], check=True)\n",
    "            subprocess.run(['git', 'config', 'user.email', 'colab@spaceapps.com'], check=True)\n",
    "            print(\"   âœ… Git ç”¨æˆ¶è¨­å®šå®Œæˆ\")\n",
    "        except:\n",
    "            print(\"   âš ï¸ Git ç”¨æˆ¶è¨­å®šè·³é\")\n",
    "\n",
    "        # è¨­å®šé ç«¯å€‰åº«ï¼ˆè‡ªå‹•åµæ¸¬æˆ–ä½¿ç”¨é è¨­ï¼‰\n",
    "        try:\n",
    "            remote_check = subprocess.run(['git', 'remote', 'get-url', 'origin'],\n",
    "                                        capture_output=True, text=True)\n",
    "            if remote_check.returncode != 0:\n",
    "                print(\"   ğŸ”§ è¨­å®šé ç«¯å€‰åº«...\")\n",
    "                # ä½¿ç”¨é è¨­å€‰åº« URLï¼ˆç”¨æˆ¶éœ€è¦ä¿®æ”¹ç‚ºè‡ªå·±çš„å€‰åº«ï¼‰\n",
    "                default_repo = \"https://github.com/exoplanet-spaceapps/exoplanet-starter.git\"\n",
    "                subprocess.run(['git', 'remote', 'add', 'origin', default_repo], check=True)\n",
    "                print(f\"   âœ… é ç«¯å€‰åº«è¨­å®š: {default_repo}\")\n",
    "                print(\"   ğŸ’¡ è«‹ç¢ºä¿ä½ æœ‰è©²å€‰åº«çš„å¯«å…¥æ¬Šé™ï¼Œæˆ–ä¿®æ”¹ç‚ºä½ çš„å€‰åº«\")\n",
    "            else:\n",
    "                print(f\"   âœ… é ç«¯å€‰åº«å·²è¨­å®š: {remote_check.stdout.strip()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ é ç«¯å€‰åº«è¨­å®šè­¦å‘Š: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Git è¨­å®šå¤±æ•—: {e}\")\n",
    "        return False\n",
    "\n",
    "    # æ­¥é©Ÿ 4: Git LFS è¨­å®š\n",
    "    print(\"\\nğŸ“‹ æ­¥é©Ÿ 2/4: Git LFS è¨­å®š...\")\n",
    "\n",
    "    try:\n",
    "        # å®‰è£ Git LFSï¼ˆColabï¼‰\n",
    "        if IN_COLAB:\n",
    "            print(\"   ğŸ“¦ åœ¨ Colab ä¸­å®‰è£ Git LFS...\")\n",
    "            subprocess.run(['apt-get', 'update', '-qq'], check=True)\n",
    "            subprocess.run(['apt-get', 'install', '-y', '-qq', 'git-lfs'], check=True)\n",
    "            print(\"   âœ… Git LFS å·²å®‰è£\")\n",
    "\n",
    "        # åˆå§‹åŒ– LFS\n",
    "        try:\n",
    "            subprocess.run(['git', 'lfs', 'install'], check=True)\n",
    "            print(\"   âœ… Git LFS åˆå§‹åŒ–å®Œæˆ\")\n",
    "        except:\n",
    "            print(\"   âš ï¸ Git LFS åˆå§‹åŒ–è·³éï¼ˆå¯èƒ½å·²è¨­å®šï¼‰\")\n",
    "\n",
    "        # è¨­å®š LFS è¿½è¹¤ï¼ˆå®¹éŒ¯è™•ç†ï¼‰\n",
    "        lfs_patterns = ['*.csv', '*.json', '*.pkl', '*.parquet', '*.h5', '*.hdf5', '*.joblib']\n",
    "        for pattern in lfs_patterns:\n",
    "            try:\n",
    "                result = subprocess.run(['git', 'lfs', 'track', pattern],\n",
    "                                      capture_output=True, text=True)\n",
    "                if result.returncode == 0:\n",
    "                    print(f\"   ğŸ“¦ LFS è¿½è¹¤: {pattern}\")\n",
    "                else:\n",
    "                    print(f\"   âš ï¸ LFS è¿½è¹¤ {pattern} è­¦å‘Š: {result.stderr.strip()}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸ LFS è¿½è¹¤ {pattern} è·³é: {e}\")\n",
    "\n",
    "        # æ·»åŠ  .gitattributes åˆ° staging\n",
    "        try:\n",
    "            subprocess.run(['git', 'add', '.gitattributes'], check=False)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ Git LFS è¨­å®šè­¦å‘Š: {e}\")\n",
    "        print(\"   ğŸ’¡ ç¹¼çºŒåŸ·è¡Œï¼Œä½†å¤§æª”æ¡ˆå¯èƒ½ç„¡æ³•æ­£ç¢ºè¿½è¹¤\")\n",
    "\n",
    "    # æ­¥é©Ÿ 5: æ·»åŠ æª”æ¡ˆä¸¦æäº¤\n",
    "    print(\"\\nğŸ“‹ æ­¥é©Ÿ 3/4: æ·»åŠ æª”æ¡ˆèˆ‡æäº¤...\")\n",
    "\n",
    "    try:\n",
    "        # ç¢ºä¿é‡è¦ç›®éŒ„å­˜åœ¨\n",
    "        important_dirs = ['data', 'notebooks', 'app', 'scripts', 'model']\n",
    "        for dir_name in important_dirs:\n",
    "            dir_path = Path(dir_name)\n",
    "            if dir_path.exists():\n",
    "                print(f\"   ğŸ“‚ æ‰¾åˆ°ç›®éŒ„: {dir_name}\")\n",
    "            elif IN_COLAB and dir_name in ['data', 'model']:\n",
    "                # åœ¨ Colab ä¸­å‰µå»ºç›¸é—œç›®éŒ„\n",
    "                dir_path.mkdir(parents=True, exist_ok=True)\n",
    "                print(f\"   ğŸ“‚ å‰µå»ºç›®éŒ„: {dir_name}\")\n",
    "\n",
    "        # æ·»åŠ æ‰€æœ‰æª”æ¡ˆ\n",
    "        subprocess.run(['git', 'add', '.'], check=True)\n",
    "        print(\"   âœ… æª”æ¡ˆæ·»åŠ å®Œæˆ\")\n",
    "\n",
    "        # æª¢æŸ¥æ˜¯å¦æœ‰è®Šæ›´\n",
    "        status_result = subprocess.run(['git', 'status', '--porcelain'],\n",
    "                                      capture_output=True, text=True, check=True)\n",
    "\n",
    "        if not status_result.stdout.strip():\n",
    "            print(\"   âœ… æ²’æœ‰æ–°çš„è®Šæ›´éœ€è¦æäº¤\")\n",
    "            return True\n",
    "\n",
    "        # å‰µå»ºæäº¤\n",
    "        commit_message = \"\"\"feat: complete synthetic injection & supervised training pipeline\n",
    "\n",
    "- ğŸ§ª å®Œæˆåˆæˆå‡Œæ—¥æ³¨å…¥è³‡æ–™ç”Ÿæˆ (200 æ­£é¡ + 200 è² é¡)\n",
    "- ğŸ” å¯¦ä½œ BLS/TLS ç‰¹å¾µæå–èˆ‡é‡è¦æ€§åˆ†æ\n",
    "- ğŸ¤– è¨“ç·´å¤šå€‹æ¨¡å‹: LogisticRegression, RandomForest, XGBoost\n",
    "- ğŸ“Š å¯¦ç¾ Isotonic æ©Ÿç‡æ ¡æº– (ECE, Brier Score, å¯é åº¦æ›²ç·š)\n",
    "- ğŸ“ˆ ç›£ç£å¼å­¸ç¿’: çœŸå¯¦ TOI + Kepler EB è³‡æ–™è¨“ç·´\n",
    "- ğŸ“‹ æ–¹æ³•æ¯”è¼ƒ: åˆæˆæ³¨å…¥ vs ç›£ç£å¼å­¸ç¿’æ•ˆèƒ½å°æ¯”\n",
    "- ğŸ’¾ æ¨¡å‹æŒä¹…åŒ–: model/ranker.joblib + feature_schema.json\n",
    "- ğŸ“Š å®Œæ•´è©•ä¼°æŒ‡æ¨™: PR-AUC, ROC-AUC, Precision@K, ECE\n",
    "\n",
    "Co-Authored-By: hctsai1006 <39769660@cuni.cz>\n",
    "        \"\"\"\n",
    "\n",
    "        subprocess.run(['git', 'commit', '-m', commit_message], check=True)\n",
    "        print(\"   âœ… æäº¤å®Œæˆ\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"   âŒ æª”æ¡ˆæäº¤å¤±æ•—: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ æª”æ¡ˆè™•ç†å¤±æ•—: {e}\")\n",
    "        return False\n",
    "\n",
    "    # æ­¥é©Ÿ 6: æ¨é€åˆ° GitHub\n",
    "    print(\"\\nğŸ“‹ æ­¥é©Ÿ 4/4: æ¨é€åˆ° GitHub...\")\n",
    "\n",
    "    try:\n",
    "        # ç²å–é ç«¯ URL ä¸¦æ’å…¥ token\n",
    "        remote_result = subprocess.run(['git', 'remote', 'get-url', 'origin'],\n",
    "                                      capture_output=True, text=True, check=True)\n",
    "        remote_url = remote_result.stdout.strip()\n",
    "\n",
    "        # æ§‹é€ å¸¶ token çš„ URL\n",
    "        if remote_url.startswith('https://github.com/'):\n",
    "            # æå–å€‰åº«è·¯å¾‘\n",
    "            repo_path = remote_url.replace('https://github.com/', '').replace('.git', '')\n",
    "            auth_url = f\"https://{token}@github.com/{repo_path}.git\"\n",
    "        else:\n",
    "            print(f\"   âš ï¸ é ç«¯ URL æ ¼å¼ç•°å¸¸: {remote_url}\")\n",
    "            auth_url = remote_url\n",
    "\n",
    "        # æ¨é€\n",
    "        push_result = subprocess.run([\n",
    "            'git', 'push', auth_url, 'main'\n",
    "        ], capture_output=True, text=True, timeout=300)\n",
    "\n",
    "        if push_result.returncode == 0:\n",
    "            print(\"   âœ… æ¨é€æˆåŠŸï¼\")\n",
    "            print(f\"   ğŸ“¡ æ¨é€è¼¸å‡º: {push_result.stdout[:200]}...\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"   âŒ æ¨é€å¤±æ•—: {push_result.stderr}\")\n",
    "            # å˜—è©¦æ¨é€åˆ°å…¶ä»–åˆ†æ”¯\n",
    "            try:\n",
    "                alt_push = subprocess.run([\n",
    "                    'git', 'push', auth_url, 'HEAD:main'\n",
    "                ], capture_output=True, text=True, timeout=300)\n",
    "                if alt_push.returncode == 0:\n",
    "                    print(\"   âœ… å‚™ç”¨æ¨é€æˆåŠŸï¼\")\n",
    "                    return True\n",
    "            except:\n",
    "                pass\n",
    "            return False\n",
    "\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"   âŒ æ¨é€è¶…æ™‚ï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£æ¥\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ æ¨é€å¤±æ•—: {e}\")\n",
    "        return False\n",
    "\n",
    "    finally:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ğŸ“‹ åˆæˆæ³¨å…¥èˆ‡ç›£ç£å¼è¨“ç·´çµæœæ¨é€å®Œæˆ!\")\n",
    "        if IN_COLAB:\n",
    "            print(\"ğŸ’¡ å¦‚æœé‡åˆ°å•é¡Œ:\")\n",
    "            print(\"   1. ç¢ºä¿ token æœ‰ 'repo' æ¬Šé™\")\n",
    "            print(\"   2. ç¢ºä¿ä½ æœ‰ç›®æ¨™å€‰åº«çš„å¯«å…¥æ¬Šé™\")\n",
    "            print(\"   3. æª¢æŸ¥å€‰åº« URL æ˜¯å¦æ­£ç¢º\")\n",
    "\n",
    "# å‘¼å«å‡½æ•¸ï¼ˆè«‹åœ¨åŸ·è¡Œæ™‚æä¾› tokenï¼‰\n",
    "print(\"ğŸ” æº–å‚™æ¨é€åˆæˆæ³¨å…¥èˆ‡ç›£ç£å¼è¨“ç·´çµæœ...\")\n",
    "print(\"ğŸ’¡ åŸ·è¡Œæ–¹å¼: ultimate_push_to_github_03(token='ä½ çš„GitHub_token')\")\n",
    "print(\"ğŸ“ æˆ–ç›´æ¥åŸ·è¡Œä¸‹æ–¹ cell ä¸¦åœ¨æç¤ºæ™‚è¼¸å…¥ token\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ğŸš€ åŸ·è¡Œ GitHub Push (03 - åˆæˆæ³¨å…¥èˆ‡ç›£ç£å¼è¨“ç·´)\n",
    "# å–æ¶ˆè¨»è§£ä¸‹é¢é€™è¡Œä¾†åŸ·è¡Œæ¨é€:\n",
    "# ultimate_push_to_github_03()\n",
    "\n",
    "print(\"ğŸ“‹ åˆæˆæ³¨å…¥èˆ‡ç›£ç£å¼è¨“ç·´ç®¡ç·šå®Œæˆï¼\")\n",
    "print(\"ğŸ’¡ è«‹åœ¨éœ€è¦æ¨é€çµæœæ™‚åŸ·è¡Œä¸Šé¢çš„ ultimate_push_to_github_03() å‡½æ•¸\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. ç‰¹å¾µèƒå–\n",
    "\n",
    "### 4.1 æ‰¹æ¬¡æå– BLS ç‰¹å¾µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.1 Phase 3-4: Sklearn Pipeline + StratifiedGroupKFold äº¤å‰é©—è­‰\n",
    "\n",
    "**Phase 3 æ–°åŠŸèƒ½:**\n",
    "- âœ¨ ä½¿ç”¨ `create_exoplanet_pipeline()` å–ä»£åŸºç¤ XGBoost\n",
    "- ğŸ”„ æ•´åˆ SimpleImputer + RobustScaler å‰è™•ç†\n",
    "- ğŸš€ GPU åŠ é€Ÿ (è‡ªå‹•åµæ¸¬ä¸¦ä½¿ç”¨ `device='cuda'`)\n",
    "- ğŸ¯ `random_state=42` ç¢ºä¿å¯é‡ç¾æ€§\n",
    "\n",
    "**Phase 4 æ–°åŠŸèƒ½:**\n",
    "- ğŸ“Š StratifiedGroupKFold 5-fold äº¤å‰é©—è­‰\n",
    "- ğŸ”’ æŒ‰ `target_id` åˆ†çµ„é¿å…è³‡æ–™æ´©æ¼\n",
    "- ğŸ“ˆ è¨˜éŒ„æ¯å€‹ fold çš„è©³ç´°æŒ‡æ¨™\n",
    "- ğŸ“‰ è¨ˆç®—å¹³å‡ AUC-PR èˆ‡æ¨™æº–å·®\n",
    "\n",
    "**ç‚ºä»€éº¼ä½¿ç”¨ StratifiedGroupKFold?**\n",
    "1. **Stratified**: ä¿æŒæ¯å€‹ fold çš„é¡åˆ¥æ¯”ä¾‹ä¸€è‡´\n",
    "2. **Grouped**: åŒä¸€ target çš„æ‰€æœ‰æ¨£æœ¬ç•™åœ¨åŒä¸€ fold\n",
    "3. **é˜²æ­¢æ´©æ¼**: è¨“ç·´æ™‚ä¸æœƒçœ‹åˆ°æ¸¬è©¦ç›®æ¨™çš„ä»»ä½•è³‡æ–™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 03 Â· åˆæˆæ³¨å…¥è¨“ç·´ç®¡ç·š\n",
    "\n",
    "## å·¥ä½œæµç¨‹\n",
    "1. **è³‡æ–™ç”Ÿæˆ**ï¼šåˆæˆæ³¨å…¥ 200 æ­£é¡ + 200 è² é¡\n",
    "2. **ç‰¹å¾µèƒå–**ï¼šBLS/TLS æŒ‡æ¨™ + å¹¾ä½•çµ±è¨ˆ\n",
    "3. **æ¨¡å‹è¨“ç·´**ï¼šLogisticRegression/XGBoost + æ©Ÿç‡æ ¡æº–\n",
    "4. **è©•ä¼°æŒ‡æ¨™**ï¼šPR-AUC, Precision@K, ECE, Brier Score\n",
    "5. **æŒä¹…åŒ–**ï¼šå„²å­˜æ¨¡å‹èˆ‡ç‰¹å¾µæ¶æ§‹\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7. SHAP Explainability Analysis\n",
    "\n",
    "ä½¿ç”¨ SHAP (SHapley Additive exPlanations) åˆ†ææ¨¡å‹ç‰¹å¾µé‡è¦æ€§èˆ‡å¯è§£é‡‹æ€§ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.2 ç‰¹å¾µé‡è¦æ€§åˆ†æ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [2]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 8. Probability Calibration Comparison\n",
    "\n",
    "æ¯”è¼ƒå…©ç¨®æ©Ÿç‡æ ¡æº–æ–¹æ³•ï¼š**Isotonic Regression** èˆ‡ **Platt Scaling (Sigmoid)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. ç’°å¢ƒè¨­å®šèˆ‡ä¾è³´å®‰è£"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. å°å…¥å¥—ä»¶èˆ‡æ¨¡çµ„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. è³‡æ–™ç”Ÿæˆï¼šåˆæˆå‡Œæ—¥æ³¨å…¥\n",
    "\n",
    "### 3.1 ä¸‹è¼‰åŸºç¤å…‰æ›²ç·š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2 ç”Ÿæˆåˆæˆè³‡æ–™é›†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3 åƒæ•¸åˆ†å¸ƒè¦–è¦ºåŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.3 å»ºç«‹ç‰¹å¾µæ¶æ§‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. æ¨¡å‹è¨“ç·´èˆ‡æ ¡æº–\n",
    "\n",
    "### 5.1 è³‡æ–™æº–å‚™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.2 è¨“ç·´å¤šå€‹æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.3 æ©Ÿç‡æ ¡æº–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. æ¨¡å‹è©•ä¼°\n",
    "\n",
    "### 6.1 è¨ˆç®—è©•ä¼°æŒ‡æ¨™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6.2 å¯é åº¦æ›²ç·šè¦–è¦ºåŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6.3 PR æ›²ç·šèˆ‡ Precision@K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7. æ¨¡å‹æŒä¹…åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 8. ç¸½çµå ±å‘Š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9. ç›£ç£å¼å­¸ç¿’ç®¡ç·šï¼ˆçœŸå¯¦ TOI + Kepler EB è³‡æ–™ï¼‰\n",
    "\n",
    "> ğŸ’¡ **æ–°å¢åŠŸèƒ½**ï¼šä½¿ç”¨çœŸå¯¦çš„ TOIï¼ˆæ­£é¡ï¼‰å’Œ Kepler EBï¼ˆè² é¡ï¼‰è³‡æ–™é€²è¡Œç›£ç£å¼è¨“ç·´ï¼Œèˆ‡åˆæˆæ³¨å…¥æ–¹æ³•æ¯”è¼ƒã€‚\n",
    "\n",
    "### 9.1 è¼‰å…¥çœŸå¯¦è³‡æ–™é›†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9. ç›£ç£å¼å­¸ç¿’ç®¡ç·šï¼ˆçœŸå¯¦ TOI + Kepler EB è³‡æ–™ï¼‰\n",
    "\n",
    "> ğŸ’¡ **æ–°å¢åŠŸèƒ½**ï¼šä½¿ç”¨çœŸå¯¦çš„ TOIï¼ˆæ­£é¡ï¼‰å’Œ Kepler EBï¼ˆè² é¡ï¼‰è³‡æ–™é€²è¡Œç›£ç£å¼è¨“ç·´ï¼Œèˆ‡åˆæˆæ³¨å…¥æ–¹æ³•æ¯”è¼ƒã€‚\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9. ç›£ç£å¼å­¸ç¿’ç®¡ç·šï¼ˆçœŸå¯¦ TOI + Kepler EB è³‡æ–™ï¼‰\n",
    "\n",
    "> ğŸ’¡ **æ–°å¢åŠŸèƒ½**ï¼šä½¿ç”¨çœŸå¯¦çš„ TOIï¼ˆæ­£é¡ï¼‰å’Œ Kepler EBï¼ˆè² é¡ï¼‰è³‡æ–™é€²è¡Œç›£ç£å¼è¨“ç·´ï¼Œèˆ‡åˆæˆæ³¨å…¥æ–¹æ³•æ¯”è¼ƒã€‚\n",
    "\n",
    "### 9.1 è¼‰å…¥çœŸå¯¦è³‡æ–™é›†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 9.2 ä¸‹è¼‰ä¸¦è™•ç†çœŸå¯¦å…‰æ›²ç·š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 9.3 æå–çœŸå¯¦è³‡æ–™ç‰¹å¾µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 9.4 è¨“ç·´ç›£ç£å¼æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 9.5 æ–¹æ³•æ¯”è¼ƒï¼šåˆæˆæ³¨å…¥ vs ç›£ç£å¼å­¸ç¿’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 9.6 å„²å­˜ç›£ç£å¼æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 10. å®Œæ•´ç¸½çµå ±å‘Š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9. ç›£ç£å¼å­¸ç¿’ç®¡ç·šï¼ˆçœŸå¯¦ TOI + Kepler EB è³‡æ–™ï¼‰\n",
    "\n",
    "> ğŸ’¡ **æ–°å¢åŠŸèƒ½**ï¼šä½¿ç”¨çœŸå¯¦çš„ TOIï¼ˆæ­£é¡ï¼‰å’Œ Kepler EBï¼ˆè² é¡ï¼‰è³‡æ–™é€²è¡Œç›£ç£å¼è¨“ç·´ï¼Œèˆ‡åˆæˆæ³¨å…¥æ–¹æ³•æ¯”è¼ƒã€‚\n",
    "\n",
    "### 9.1 è¼‰å…¥çœŸå¯¦è³‡æ–™é›†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 9.2 ä¸‹è¼‰ä¸¦è™•ç†çœŸå¯¦å…‰æ›²ç·š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 9.3 æå–çœŸå¯¦è³‡æ–™ç‰¹å¾µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 9.4 è¨“ç·´ç›£ç£å¼æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 9.5 æ–¹æ³•æ¯”è¼ƒï¼šåˆæˆæ³¨å…¥ vs ç›£ç£å¼å­¸ç¿’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 9.6 å„²å­˜ç›£ç£å¼æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 10. å®Œæ•´ç¸½çµå ±å‘Š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ GitHub Push çµ‚æ¥µè§£æ±ºæ–¹æ¡ˆ\n",
    "\n",
    "å°‡åˆæˆæ³¨å…¥èˆ‡ç›£ç£å¼è¨“ç·´çµæœæ¨é€åˆ° GitHub å€‰åº«ï¼š"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.915705,
   "end_time": "2025-09-29T23:32:56.767194",
   "environment_variables": {},
   "exception": true,
   "input_path": "03_injection_train.ipynb",
   "output_path": "03_injection_train.ipynb",
   "parameters": {},
   "start_time": "2025-09-29T23:32:47.851489",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}