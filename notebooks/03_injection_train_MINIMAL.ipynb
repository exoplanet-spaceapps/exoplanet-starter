{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Injection Training - MINIMAL VERSION\n",
    "\n",
    "**Purpose**: Train supervised exoplanet detection model with proper cell execution order.\n",
    "\n",
    "**Fixed Issues**:\n",
    "- ‚úÖ All imports in one cell\n",
    "- ‚úÖ `feature_cols` defined BEFORE use\n",
    "- ‚úÖ Proper DataFrame indexing with `.iloc[]`\n",
    "- ‚úÖ Removed broken calibration cells\n",
    "- ‚úÖ Removed visualization cells with undefined variables\n",
    "\n",
    "**Workflow**: Load Data ‚Üí Extract Features ‚Üí Train ‚Üí Save Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Installation (Colab Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell ONLY if on Google Colab\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"üì¶ Installing dependencies for Colab...\")\n",
    "    !pip install -q numpy==1.26.4 'scipy<1.13' astropy lightkurve pandas scikit-learn xgboost joblib\n",
    "    !pip install -q transitleastsquares\n",
    "    print(\"‚úÖ Installation complete\")\n",
    "    print(\"‚ö†Ô∏è RESTART RUNTIME NOW, then continue from Cell 2\")\n",
    "else:\n",
    "    print(\"‚úÖ Running locally, skipping installation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: All Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# Astronomy\n",
    "import lightkurve as lk\n",
    "from astropy.timeseries import BoxLeastSquares\n",
    "from transitleastsquares import transitleastsquares\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All imports successful\")\n",
    "print(f\"   NumPy: {np.__version__}\")\n",
    "print(f\"   Pandas: {pd.__version__}\")\n",
    "print(f\"   XGBoost: {xgb.__version__}\")\n",
    "print(f\"   Lightkurve: {lk.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Setup Paths and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    BASE_DIR = Path('/content/drive/MyDrive/spaceapps-exoplanet')\n",
    "    BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "else:\n",
    "    BASE_DIR = Path(os.getcwd()).parent\n",
    "\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "MODEL_DIR = BASE_DIR / 'models'\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Paths configured:\")\n",
    "print(f\"   Base: {BASE_DIR}\")\n",
    "print(f\"   Data: {DATA_DIR}\")\n",
    "print(f\"   Models: {MODEL_DIR}\")\n",
    "\n",
    "# Helper: Get XGBoost GPU params\n",
    "def get_xgboost_gpu_params() -> Dict:\n",
    "    \"\"\"Check GPU availability and return XGBoost parameters.\"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"‚úÖ GPU detected, using tree_method='hist' with GPU\")\n",
    "            return {'tree_method': 'hist', 'device': 'cuda'}\n",
    "    except ImportError:\n",
    "        pass\n",
    "    print(\"‚ÑπÔ∏è No GPU, using tree_method='hist' with CPU\")\n",
    "    return {'tree_method': 'hist'}\n",
    "\n",
    "# Helper: Create pipeline\n",
    "def create_exoplanet_pipeline(\n",
    "    numerical_features: List[str],\n",
    "    xgb_params: Dict = None,\n",
    "    n_estimators: int = 100,\n",
    "    max_depth: int = 6,\n",
    "    learning_rate: float = 0.1,\n",
    "    random_state: int = 42\n",
    ") -> Pipeline:\n",
    "    \"\"\"Create sklearn Pipeline with preprocessing and XGBoost.\"\"\"\n",
    "    if xgb_params is None:\n",
    "        xgb_params = {}\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', xgb.XGBClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            random_state=random_state,\n",
    "            eval_metric='logloss',\n",
    "            **xgb_params\n",
    "        ))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "print(\"\\n‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load supervised dataset\nsupervised_path = DATA_DIR / 'supervised_dataset.csv'\n\nif not supervised_path.exists():\n    raise FileNotFoundError(f\"‚ùå Dataset not found: {supervised_path}\")\n\nsamples_df = pd.read_csv(supervised_path)\n\nprint(f\"‚úÖ Loaded supervised dataset: {supervised_path}\")\nprint(f\"   Total samples: {len(samples_df)}\")\nprint(f\"   Original columns: {list(samples_df.columns)}\")\n\n# ==== COLUMN MAPPING FIX ====\n# Map existing columns to expected schema\nprint(\"\\nüîß Applying column mappings...\")\n\n# Map tid ‚Üí tic_id (TIC ID is in 'tid' or 'target_id' column)\nif 'tic_id' not in samples_df.columns:\n    if 'tid' in samples_df.columns:\n        samples_df['tic_id'] = samples_df['tid']\n        print(\"   ‚úÖ Mapped 'tid' ‚Üí 'tic_id'\")\n    elif 'target_id' in samples_df.columns:\n        samples_df['tic_id'] = samples_df['target_id']\n        print(\"   ‚úÖ Mapped 'target_id' ‚Üí 'tic_id'\")\n\n# Generate sample_id from index\nif 'sample_id' not in samples_df.columns:\n    samples_df['sample_id'] = [f\"SAMPLE_{i:06d}\" for i in range(len(samples_df))]\n    print(\"   ‚úÖ Generated 'sample_id' from index\")\n\n# Set default sector (TESS observing sector)\nif 'sector' not in samples_df.columns:\n    samples_df['sector'] = 1  # Default to sector 1, can be refined later\n    print(\"   ‚úÖ Set default 'sector' = 1\")\n\n# Calculate epoch (transit mid-point, approximate as period/2 if not available)\nif 'epoch' not in samples_df.columns:\n    if 'period' in samples_df.columns:\n        samples_df['epoch'] = samples_df['period'] * 0.5\n        print(\"   ‚úÖ Calculated 'epoch' from period\")\n    else:\n        samples_df['epoch'] = 0.0\n        print(\"   ‚ö†Ô∏è Set default 'epoch' = 0.0\")\n\nprint(f\"\\n‚úÖ Column mapping complete\")\nprint(f\"   Updated columns: {list(samples_df.columns)}\")\nprint(f\"\\nFirst few rows after mapping:\")\nprint(samples_df[['sample_id', 'tic_id', 'sector', 'period', 'duration', 'epoch', 'depth', 'label']].head())\n\n# Verify required columns\nrequired_cols = ['sample_id', 'tic_id', 'sector', 'period', 'duration', 'epoch', 'depth', 'label']\nmissing_cols = [col for col in required_cols if col not in samples_df.columns]\nif missing_cols:\n    print(f\"\\n‚ùå Still missing columns: {missing_cols}\")\n    raise ValueError(f\"Missing required columns after mapping: {missing_cols}\")\nelse:\n    print(f\"\\n‚úÖ All required columns present\")\n    print(f\"   Positive samples: {samples_df['label'].sum()} ({samples_df['label'].mean():.2%})\")\n    print(f\"   Negative samples: {(~samples_df['label'].astype(bool)).sum()} ({(~samples_df['label'].astype(bool)).mean():.2%})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Feature Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_features_from_lightcurve(\n    time: np.ndarray,\n    flux: np.ndarray,\n    period: float,\n    duration: float,\n    epoch: float,\n    depth: float\n) -> Dict[str, float]:\n    \"\"\"Extract BLS/TLS features from light curve.\"\"\"\n    features = {}\n    \n    # Basic statistics\n    features['flux_mean'] = np.nanmean(flux)\n    features['flux_std'] = np.nanstd(flux)\n    features['flux_median'] = np.nanmedian(flux)\n    features['flux_mad'] = np.nanmedian(np.abs(flux - np.nanmedian(flux)))\n    \n    # Input parameters\n    features['input_period'] = period\n    features['input_duration'] = duration\n    features['input_depth'] = depth\n    features['input_epoch'] = epoch\n    \n    # BLS analysis\n    try:\n        bls = BoxLeastSquares(time, flux)\n        periods = np.linspace(0.5, 15.0, 5000)\n        durations = np.linspace(0.05, 0.5, 10)\n        bls_result = bls.power(periods, durations)\n        \n        features['bls_power'] = float(bls_result.power.max())\n        features['bls_period'] = float(bls_result.period[np.argmax(bls_result.power)])\n        features['bls_duration'] = float(bls_result.duration[np.argmax(bls_result.power)])\n        features['bls_depth'] = float(bls_result.depth[np.argmax(bls_result.power)])\n    except Exception as e:\n        print(f\"‚ö†Ô∏è BLS failed: {e}\")\n        features['bls_power'] = 0.0\n        features['bls_period'] = period\n        features['bls_duration'] = duration\n        features['bls_depth'] = depth\n    \n    # TLS analysis\n    try:\n        tls_result = transitleastsquares(time, flux)\n        tls_power = tls_result.power(period_min=0.5, period_max=15.0, n_transits_min=2)\n        \n        features['tls_power'] = float(tls_power['power'].max())\n        features['tls_period'] = float(tls_power['period'])\n        features['tls_duration'] = float(tls_power['duration'])\n        features['tls_depth'] = float(tls_power['depth'])\n        features['tls_snr'] = float(tls_power.get('snr', 0.0))\n    except Exception as e:\n        print(f\"‚ö†Ô∏è TLS failed: {e}\")\n        features['tls_power'] = 0.0\n        features['tls_period'] = period\n        features['tls_duration'] = duration\n        features['tls_depth'] = depth\n        features['tls_snr'] = 0.0\n    \n    return features\n\ndef extract_features_batch(\n    samples_df: pd.DataFrame,\n    max_samples: Optional[int] = None\n) -> pd.DataFrame:\n    \"\"\"Extract features for batch of samples.\"\"\"\n    if max_samples:\n        samples_df = samples_df.head(max_samples)\n    \n    all_features = []\n    \n    for idx, row in samples_df.iterrows():\n        try:\n            # FIX: Convert tic_id to integer\n            tic_id = int(float(row['tic_id']))\n            \n            # CRITICAL FIX: Search ANY available sector, not just sector=1\n            search_result = lk.search_lightcurve(f\"TIC {tic_id}\", author='SPOC')\n            \n            if search_result is None or len(search_result) == 0:\n                print(f\"‚ö†Ô∏è No data for TIC {tic_id} (any sector)\")\n                continue\n            \n            # Download FIRST available light curve\n            lc_collection = search_result.download_all()\n            \n            if lc_collection is None or len(lc_collection) == 0:\n                print(f\"‚ö†Ô∏è Download failed for TIC {tic_id}\")\n                continue\n            \n            lc = lc_collection[0].remove_nans().normalize()\n            time = lc.time.value\n            flux = lc.flux.value\n            \n            # Extract features\n            features = extract_features_from_lightcurve(\n                time, flux,\n                row['period'], row['duration'],\n                row['epoch'], row['depth']\n            )\n            \n            features['sample_id'] = row['sample_id']\n            features['label'] = row['label']\n            all_features.append(features)\n            \n            if len(all_features) % 10 == 0:\n                print(f\"‚úì Processed {len(all_features)} samples\")\n                \n        except Exception as e:\n            print(f\"‚ùå Error processing {row['sample_id']}: {e}\")\n            continue\n    \n    features_df = pd.DataFrame(all_features)\n    print(f\"\\n‚úÖ Feature extraction complete: {len(features_df)} samples\")\n    return features_df\n\nprint(\"‚úÖ Feature extraction functions defined (ANY sector strategy)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Extract Features (CRITICAL - Defines feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract features from samples - STRATIFIED SAMPLING TO ENSURE BOTH CLASSES\nprint(\"üöÄ Starting feature extraction...\")\nprint(\"‚ö†Ô∏è Using 30 samples with stratified sampling (15 pos + 15 neg)\")\nprint()\n\n# STRATIFIED SAMPLING: Select equal numbers of positive and negative samples\npos_samples = samples_df[samples_df['label'] == 1].head(15)\nneg_samples = samples_df[samples_df['label'] == 0].head(15)\nbalanced_samples = pd.concat([pos_samples, neg_samples], ignore_index=True)\n\nprint(f\"‚úÖ Balanced sampling:\")\nprint(f\"   Positive samples: {(balanced_samples['label'] == 1).sum()}\")\nprint(f\"   Negative samples: {(balanced_samples['label'] == 0).sum()}\")\nprint()\n\n# Extract features - 30 BALANCED SAMPLES\nfeatures_df = extract_features_batch(balanced_samples, max_samples=None)\n\n# CRITICAL: Define feature_cols IMMEDIATELY after extraction\nfeature_cols = [col for col in features_df.columns if col not in ['sample_id', 'label']]\n\nprint(f\"\\n‚úÖ Features extracted and feature_cols defined:\")\nprint(f\"   Total samples: {len(features_df)}\")\nprint(f\"   Positive samples: {(features_df['label'] == 1).sum()}\")\nprint(f\"   Negative samples: {(features_df['label'] == 0).sum()}\")\nprint(f\"   Total features: {len(feature_cols)}\")\nprint(f\"   Feature columns: {feature_cols[:5]}... (showing first 5)\")\nprint(f\"\\nFeature statistics:\")\nprint(features_df[feature_cols].describe())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "X = features_df[feature_cols].values\n",
    "y = features_df['label'].values\n",
    "\n",
    "# Handle invalid values\n",
    "X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# Create groups for cross-validation\n",
    "if 'sample_id' in features_df.columns:\n",
    "    groups = features_df['sample_id'].apply(lambda x: hash(str(x)) % 10000).values\n",
    "    print(f\"‚úÖ Created groups from sample_id\")\n",
    "else:\n",
    "    groups = np.arange(len(y))\n",
    "    print(f\"‚ö†Ô∏è No sample_id, using individual groups\")\n",
    "\n",
    "print(f\"\\nüìä Training data prepared:\")\n",
    "print(f\"   X shape: {X.shape}\")\n",
    "print(f\"   y shape: {y.shape}\")\n",
    "print(f\"   Positive ratio: {y.mean():.2%}\")\n",
    "print(f\"   Unique groups: {len(np.unique(groups))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Cross-Validation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get GPU params\ngpu_params = get_xgboost_gpu_params()\nprint(f\"XGBoost params: {gpu_params}\")\nprint()\n\n# Setup cross-validation (REDUCED TO 3 FOLDS FOR SMALL DATASET)\nn_splits = 3\nsgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\nfold_results = []\nfold_models = []\n\nprint(f\"üöÄ Starting {n_splits}-Fold Cross-Validation\")\nprint(\"=\" * 60)\n\nfor fold_idx, (train_idx, test_idx) in enumerate(sgkf.split(X, y, groups), 1):\n    print(f\"\\nFold {fold_idx}/{n_splits}\")\n    print(\"-\" * 40)\n    \n    # Split data using .iloc[] for proper indexing\n    X_train, X_test = X[train_idx], X[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    \n    print(f\"   Train samples: {len(train_idx)} (pos: {y_train.sum()}, neg: {len(y_train) - y_train.sum()})\")\n    print(f\"   Test samples:  {len(test_idx)} (pos: {y_test.sum()}, neg: {len(y_test) - y_test.sum()})\")\n    \n    # Create and train pipeline\n    pipeline = create_exoplanet_pipeline(\n        numerical_features=feature_cols,\n        xgb_params=gpu_params,\n        n_estimators=100,\n        max_depth=6,\n        learning_rate=0.1,\n        random_state=42 + fold_idx\n    )\n    \n    print(f\"   Training...\")\n    pipeline.fit(X_train, y_train)\n    \n    # Predict\n    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n    \n    # Calculate metrics\n    ap_score = average_precision_score(y_test, y_pred_proba)\n    auc_score = roc_auc_score(y_test, y_pred_proba)\n    \n    fold_results.append({\n        'fold': fold_idx,\n        'train_size': len(train_idx),\n        'test_size': len(test_idx),\n        'test_pos_ratio': y_test.mean(),\n        'auc_pr': ap_score,\n        'auc_roc': auc_score\n    })\n    \n    fold_models.append(pipeline)\n    \n    print(f\"   AUC-PR:  {ap_score:.4f}\")\n    print(f\"   AUC-ROC: {auc_score:.4f}\")\n\n# Summary results\nprint(\"\\n\" + \"=\" * 60)\nprint(\"üìä Cross-Validation Summary\")\nprint(\"=\" * 60)\n\nfold_df = pd.DataFrame(fold_results)\nprint(f\"\\nAUC-PR:  {fold_df['auc_pr'].mean():.4f} +/- {fold_df['auc_pr'].std():.4f}\")\nprint(f\"AUC-ROC: {fold_df['auc_roc'].mean():.4f} +/- {fold_df['auc_roc'].std():.4f}\")\nprint(f\"\\nDetailed results per fold:\")\nprint(fold_df.to_string(index=False))\n\n# Select best model\nbest_fold_idx = fold_df['auc_pr'].idxmax()\nbest_model = fold_models[best_fold_idx]\n\nprint(f\"\\n‚úÖ Best model: Fold {best_fold_idx + 1} (AUC-PR: {fold_df.loc[best_fold_idx, 'auc_pr']:.4f})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "model_path = MODEL_DIR / 'exoplanet_xgboost_pipeline.pkl'\n",
    "joblib.dump(best_model, model_path)\n",
    "\n",
    "print(f\"‚úÖ Model saved: {model_path}\")\n",
    "print(f\"   Model size: {model_path.stat().st_size / 1024:.2f} KB\")\n",
    "\n",
    "# Save feature columns\n",
    "feature_cols_path = MODEL_DIR / 'feature_columns.txt'\n",
    "with open(feature_cols_path, 'w') as f:\n",
    "    f.write('\\n'.join(feature_cols))\n",
    "\n",
    "print(f\"‚úÖ Feature columns saved: {feature_cols_path}\")\n",
    "\n",
    "# Save metrics\n",
    "metrics_path = MODEL_DIR / 'training_metrics.csv'\n",
    "fold_df.to_csv(metrics_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Metrics saved: {metrics_path}\")\n",
    "\n",
    "# Save summary\n",
    "summary_path = MODEL_DIR / 'training_summary.txt'\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(f\"Exoplanet Detection Model Training Summary\\n\")\n",
    "    f.write(f\"=========================================\\n\\n\")\n",
    "    f.write(f\"Training Date: {pd.Timestamp.now()}\\n\")\n",
    "    f.write(f\"Total Samples: {len(X)}\\n\")\n",
    "    f.write(f\"Total Features: {len(feature_cols)}\\n\")\n",
    "    f.write(f\"Positive Ratio: {y.mean():.2%}\\n\\n\")\n",
    "    f.write(f\"Cross-Validation Results (n={n_splits} folds):\\n\")\n",
    "    f.write(f\"  AUC-PR:  {fold_df['auc_pr'].mean():.4f} +/- {fold_df['auc_pr'].std():.4f}\\n\")\n",
    "    f.write(f\"  AUC-ROC: {fold_df['auc_roc'].mean():.4f} +/- {fold_df['auc_roc'].std():.4f}\\n\\n\")\n",
    "    f.write(f\"Best Model: Fold {best_fold_idx + 1}\\n\")\n",
    "    f.write(f\"  AUC-PR: {fold_df.loc[best_fold_idx, 'auc_pr']:.4f}\\n\")\n",
    "    f.write(f\"  AUC-ROC: {fold_df.loc[best_fold_idx, 'auc_roc']:.4f}\\n\")\n",
    "\n",
    "print(f\"‚úÖ Summary saved: {summary_path}\")\n",
    "\n",
    "print(f\"\\nüéâ Training complete! All files saved to {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Complete! üéâ\n",
    "\n",
    "### What was saved:\n",
    "1. **Model**: `exoplanet_xgboost_pipeline.pkl` - Trained XGBoost pipeline\n",
    "2. **Features**: `feature_columns.txt` - List of feature names\n",
    "3. **Metrics**: `training_metrics.csv` - Cross-validation results\n",
    "4. **Summary**: `training_summary.txt` - Training overview\n",
    "\n",
    "### Next Steps:\n",
    "- Use `04_newdata_inference.ipynb` to make predictions on new data\n",
    "- Use `05_metrics_dashboard.ipynb` to analyze model performance\n",
    "\n",
    "### Issues Fixed:\n",
    "- ‚úÖ Cell execution order corrected\n",
    "- ‚úÖ `feature_cols` defined before use\n",
    "- ‚úÖ DataFrame indexing uses `.iloc[]`\n",
    "- ‚úÖ Removed broken calibration code\n",
    "- ‚úÖ Removed undefined variable references"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}