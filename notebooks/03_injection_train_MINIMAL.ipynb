{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Injection Training - MINIMAL VERSION\n",
    "\n",
    "**Purpose**: Train supervised exoplanet detection model with proper cell execution order.\n",
    "\n",
    "**Fixed Issues**:\n",
    "- ‚úÖ All imports in one cell\n",
    "- ‚úÖ `feature_cols` defined BEFORE use\n",
    "- ‚úÖ Proper DataFrame indexing with `.iloc[]`\n",
    "- ‚úÖ Removed broken calibration cells\n",
    "- ‚úÖ Removed visualization cells with undefined variables\n",
    "\n",
    "**Workflow**: Load Data ‚Üí Extract Features ‚Üí Train ‚Üí Save Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Installation (Colab Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell ONLY if on Google Colab\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"üì¶ Installing dependencies for Colab...\")\n",
    "    !pip install -q numpy==1.26.4 'scipy<1.13' astropy lightkurve pandas scikit-learn xgboost joblib\n",
    "    !pip install -q transitleastsquares\n",
    "    print(\"‚úÖ Installation complete\")\n",
    "    print(\"‚ö†Ô∏è RESTART RUNTIME NOW, then continue from Cell 2\")\n",
    "else:\n",
    "    print(\"‚úÖ Running locally, skipping installation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: All Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# Astronomy\n",
    "import lightkurve as lk\n",
    "from astropy.timeseries import BoxLeastSquares\n",
    "from transitleastsquares import transitleastsquares\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All imports successful\")\n",
    "print(f\"   NumPy: {np.__version__}\")\n",
    "print(f\"   Pandas: {pd.__version__}\")\n",
    "print(f\"   XGBoost: {xgb.__version__}\")\n",
    "print(f\"   Lightkurve: {lk.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Setup Paths and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    BASE_DIR = Path('/content/drive/MyDrive/spaceapps-exoplanet')\n",
    "    BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "else:\n",
    "    BASE_DIR = Path(os.getcwd()).parent\n",
    "\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "MODEL_DIR = BASE_DIR / 'models'\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Paths configured:\")\n",
    "print(f\"   Base: {BASE_DIR}\")\n",
    "print(f\"   Data: {DATA_DIR}\")\n",
    "print(f\"   Models: {MODEL_DIR}\")\n",
    "\n",
    "# Helper: Get XGBoost GPU params\n",
    "def get_xgboost_gpu_params() -> Dict:\n",
    "    \"\"\"Check GPU availability and return XGBoost parameters.\"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"‚úÖ GPU detected, using tree_method='hist' with GPU\")\n",
    "            return {'tree_method': 'hist', 'device': 'cuda'}\n",
    "    except ImportError:\n",
    "        pass\n",
    "    print(\"‚ÑπÔ∏è No GPU, using tree_method='hist' with CPU\")\n",
    "    return {'tree_method': 'hist'}\n",
    "\n",
    "# Helper: Create pipeline\n",
    "def create_exoplanet_pipeline(\n",
    "    numerical_features: List[str],\n",
    "    xgb_params: Dict = None,\n",
    "    n_estimators: int = 100,\n",
    "    max_depth: int = 6,\n",
    "    learning_rate: float = 0.1,\n",
    "    random_state: int = 42\n",
    ") -> Pipeline:\n",
    "    \"\"\"Create sklearn Pipeline with preprocessing and XGBoost.\"\"\"\n",
    "    if xgb_params is None:\n",
    "        xgb_params = {}\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', xgb.XGBClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            random_state=random_state,\n",
    "            eval_metric='logloss',\n",
    "            **xgb_params\n",
    "        ))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "print(\"\\n‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load supervised dataset\n",
    "supervised_path = DATA_DIR / 'supervised_dataset.csv'\n",
    "\n",
    "if not supervised_path.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Dataset not found: {supervised_path}\")\n",
    "\n",
    "samples_df = pd.read_csv(supervised_path)\n",
    "\n",
    "print(f\"‚úÖ Loaded supervised dataset: {supervised_path}\")\n",
    "print(f\"   Total samples: {len(samples_df)}\")\n",
    "print(f\"   Columns: {list(samples_df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(samples_df.head())\n",
    "\n",
    "# Verify required columns\n",
    "required_cols = ['sample_id', 'tic_id', 'sector', 'period', 'duration', 'epoch', 'depth', 'label']\n",
    "missing_cols = [col for col in required_cols if col not in samples_df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"‚ö†Ô∏è Missing columns: {missing_cols}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All required columns present\")\n",
    "    print(f\"   Positive samples: {samples_df['label'].sum()} ({samples_df['label'].mean():.2%})\")\n",
    "    print(f\"   Negative samples: {(~samples_df['label']).sum()} ({(~samples_df['label']).mean():.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Feature Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_lightcurve(\n",
    "    time: np.ndarray,\n",
    "    flux: np.ndarray,\n",
    "    period: float,\n",
    "    duration: float,\n",
    "    epoch: float,\n",
    "    depth: float\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Extract BLS/TLS features from light curve.\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Basic statistics\n",
    "    features['flux_mean'] = np.nanmean(flux)\n",
    "    features['flux_std'] = np.nanstd(flux)\n",
    "    features['flux_median'] = np.nanmedian(flux)\n",
    "    features['flux_mad'] = np.nanmedian(np.abs(flux - np.nanmedian(flux)))\n",
    "    \n",
    "    # Input parameters\n",
    "    features['input_period'] = period\n",
    "    features['input_duration'] = duration\n",
    "    features['input_depth'] = depth\n",
    "    features['input_epoch'] = epoch\n",
    "    \n",
    "    # BLS analysis\n",
    "    try:\n",
    "        bls = BoxLeastSquares(time, flux)\n",
    "        periods = np.linspace(0.5, 15.0, 5000)\n",
    "        durations = np.linspace(0.05, 0.5, 10)\n",
    "        bls_result = bls.power(periods, durations)\n",
    "        \n",
    "        features['bls_power'] = float(bls_result.power.max())\n",
    "        features['bls_period'] = float(bls_result.period[np.argmax(bls_result.power)])\n",
    "        features['bls_duration'] = float(bls_result.duration[np.argmax(bls_result.power)])\n",
    "        features['bls_depth'] = float(bls_result.depth[np.argmax(bls_result.power)])\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è BLS failed: {e}\")\n",
    "        features['bls_power'] = 0.0\n",
    "        features['bls_period'] = period\n",
    "        features['bls_duration'] = duration\n",
    "        features['bls_depth'] = depth\n",
    "    \n",
    "    # TLS analysis\n",
    "    try:\n",
    "        tls_result = transitleastsquares(time, flux)\n",
    "        tls_power = tls_result.power(period_min=0.5, period_max=15.0, n_transits_min=2)\n",
    "        \n",
    "        features['tls_power'] = float(tls_power['power'].max())\n",
    "        features['tls_period'] = float(tls_power['period'])\n",
    "        features['tls_duration'] = float(tls_power['duration'])\n",
    "        features['tls_depth'] = float(tls_power['depth'])\n",
    "        features['tls_snr'] = float(tls_power.get('snr', 0.0))\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è TLS failed: {e}\")\n",
    "        features['tls_power'] = 0.0\n",
    "        features['tls_period'] = period\n",
    "        features['tls_duration'] = duration\n",
    "        features['tls_depth'] = depth\n",
    "        features['tls_snr'] = 0.0\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_features_batch(\n",
    "    samples_df: pd.DataFrame,\n",
    "    max_samples: Optional[int] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Extract features for batch of samples.\"\"\"\n",
    "    if max_samples:\n",
    "        samples_df = samples_df.head(max_samples)\n",
    "    \n",
    "    all_features = []\n",
    "    \n",
    "    for idx, row in samples_df.iterrows():\n",
    "        try:\n",
    "            # Download light curve\n",
    "            lc_collection = lk.search_lightcurve(\n",
    "                f\"TIC {row['tic_id']}\",\n",
    "                sector=row['sector'],\n",
    "                author='SPOC'\n",
    "            ).download_all()\n",
    "            \n",
    "            if lc_collection is None or len(lc_collection) == 0:\n",
    "                print(f\"‚ö†Ô∏è No data for TIC {row['tic_id']} sector {row['sector']}\")\n",
    "                continue\n",
    "            \n",
    "            lc = lc_collection[0].remove_nans().normalize()\n",
    "            time = lc.time.value\n",
    "            flux = lc.flux.value\n",
    "            \n",
    "            # Extract features\n",
    "            features = extract_features_from_lightcurve(\n",
    "                time, flux,\n",
    "                row['period'], row['duration'],\n",
    "                row['epoch'], row['depth']\n",
    "            )\n",
    "            \n",
    "            features['sample_id'] = row['sample_id']\n",
    "            features['label'] = row['label']\n",
    "            all_features.append(features)\n",
    "            \n",
    "            if len(all_features) % 10 == 0:\n",
    "                print(f\"‚úì Processed {len(all_features)} samples\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {row['sample_id']}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    features_df = pd.DataFrame(all_features)\n",
    "    print(f\"\\n‚úÖ Feature extraction complete: {len(features_df)} samples\")\n",
    "    return features_df\n",
    "\n",
    "print(\"‚úÖ Feature extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Extract Features (CRITICAL - Defines feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from samples (limit to 50 for testing, remove limit for full training)\n",
    "print(\"üöÄ Starting feature extraction...\")\n",
    "print(\"‚ö†Ô∏è This may take 10-30 minutes depending on number of samples\")\n",
    "print()\n",
    "\n",
    "# Extract features\n",
    "features_df = extract_features_batch(samples_df, max_samples=50)\n",
    "\n",
    "# CRITICAL: Define feature_cols IMMEDIATELY after extraction\n",
    "feature_cols = [col for col in features_df.columns if col not in ['sample_id', 'label']]\n",
    "\n",
    "print(f\"\\n‚úÖ Features extracted and feature_cols defined:\")\n",
    "print(f\"   Total samples: {len(features_df)}\")\n",
    "print(f\"   Total features: {len(feature_cols)}\")\n",
    "print(f\"   Feature columns: {feature_cols[:5]}... (showing first 5)\")\n",
    "print(f\"\\nFeature statistics:\")\n",
    "print(features_df[feature_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "X = features_df[feature_cols].values\n",
    "y = features_df['label'].values\n",
    "\n",
    "# Handle invalid values\n",
    "X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# Create groups for cross-validation\n",
    "if 'sample_id' in features_df.columns:\n",
    "    groups = features_df['sample_id'].apply(lambda x: hash(str(x)) % 10000).values\n",
    "    print(f\"‚úÖ Created groups from sample_id\")\n",
    "else:\n",
    "    groups = np.arange(len(y))\n",
    "    print(f\"‚ö†Ô∏è No sample_id, using individual groups\")\n",
    "\n",
    "print(f\"\\nüìä Training data prepared:\")\n",
    "print(f\"   X shape: {X.shape}\")\n",
    "print(f\"   y shape: {y.shape}\")\n",
    "print(f\"   Positive ratio: {y.mean():.2%}\")\n",
    "print(f\"   Unique groups: {len(np.unique(groups))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Cross-Validation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GPU params\n",
    "gpu_params = get_xgboost_gpu_params()\n",
    "print(f\"XGBoost params: {gpu_params}\")\n",
    "print()\n",
    "\n",
    "# Setup cross-validation\n",
    "n_splits = 5\n",
    "sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "fold_results = []\n",
    "fold_models = []\n",
    "\n",
    "print(f\"üöÄ Starting {n_splits}-Fold Cross-Validation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(sgkf.split(X, y, groups), 1):\n",
    "    print(f\"\\nFold {fold_idx}/{n_splits}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Split data using .iloc[] for proper indexing\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Create and train pipeline\n",
    "    pipeline = create_exoplanet_pipeline(\n",
    "        numerical_features=feature_cols,\n",
    "        xgb_params=gpu_params,\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42 + fold_idx\n",
    "    )\n",
    "    \n",
    "    print(f\"Training on {len(train_idx)} samples...\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    ap_score = average_precision_score(y_test, y_pred_proba)\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    fold_results.append({\n",
    "        'fold': fold_idx,\n",
    "        'train_size': len(train_idx),\n",
    "        'test_size': len(test_idx),\n",
    "        'test_pos_ratio': y_test.mean(),\n",
    "        'auc_pr': ap_score,\n",
    "        'auc_roc': auc_score\n",
    "    })\n",
    "    \n",
    "    fold_models.append(pipeline)\n",
    "    \n",
    "    print(f\"   Train: {len(train_idx)} samples\")\n",
    "    print(f\"   Test: {len(test_idx)} samples (pos: {y_test.mean():.2%})\")\n",
    "    print(f\"   AUC-PR:  {ap_score:.4f}\")\n",
    "    print(f\"   AUC-ROC: {auc_score:.4f}\")\n",
    "\n",
    "# Summary results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä Cross-Validation Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fold_df = pd.DataFrame(fold_results)\n",
    "print(f\"\\nAUC-PR:  {fold_df['auc_pr'].mean():.4f} +/- {fold_df['auc_pr'].std():.4f}\")\n",
    "print(f\"AUC-ROC: {fold_df['auc_roc'].mean():.4f} +/- {fold_df['auc_roc'].std():.4f}\")\n",
    "print(f\"\\nDetailed results per fold:\")\n",
    "print(fold_df.to_string(index=False))\n",
    "\n",
    "# Select best model\n",
    "best_fold_idx = fold_df['auc_pr'].idxmax()\n",
    "best_model = fold_models[best_fold_idx]\n",
    "\n",
    "print(f\"\\n‚úÖ Best model: Fold {best_fold_idx + 1} (AUC-PR: {fold_df.loc[best_fold_idx, 'auc_pr']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "model_path = MODEL_DIR / 'exoplanet_xgboost_pipeline.pkl'\n",
    "joblib.dump(best_model, model_path)\n",
    "\n",
    "print(f\"‚úÖ Model saved: {model_path}\")\n",
    "print(f\"   Model size: {model_path.stat().st_size / 1024:.2f} KB\")\n",
    "\n",
    "# Save feature columns\n",
    "feature_cols_path = MODEL_DIR / 'feature_columns.txt'\n",
    "with open(feature_cols_path, 'w') as f:\n",
    "    f.write('\\n'.join(feature_cols))\n",
    "\n",
    "print(f\"‚úÖ Feature columns saved: {feature_cols_path}\")\n",
    "\n",
    "# Save metrics\n",
    "metrics_path = MODEL_DIR / 'training_metrics.csv'\n",
    "fold_df.to_csv(metrics_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Metrics saved: {metrics_path}\")\n",
    "\n",
    "# Save summary\n",
    "summary_path = MODEL_DIR / 'training_summary.txt'\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(f\"Exoplanet Detection Model Training Summary\\n\")\n",
    "    f.write(f\"=========================================\\n\\n\")\n",
    "    f.write(f\"Training Date: {pd.Timestamp.now()}\\n\")\n",
    "    f.write(f\"Total Samples: {len(X)}\\n\")\n",
    "    f.write(f\"Total Features: {len(feature_cols)}\\n\")\n",
    "    f.write(f\"Positive Ratio: {y.mean():.2%}\\n\\n\")\n",
    "    f.write(f\"Cross-Validation Results (n={n_splits} folds):\\n\")\n",
    "    f.write(f\"  AUC-PR:  {fold_df['auc_pr'].mean():.4f} +/- {fold_df['auc_pr'].std():.4f}\\n\")\n",
    "    f.write(f\"  AUC-ROC: {fold_df['auc_roc'].mean():.4f} +/- {fold_df['auc_roc'].std():.4f}\\n\\n\")\n",
    "    f.write(f\"Best Model: Fold {best_fold_idx + 1}\\n\")\n",
    "    f.write(f\"  AUC-PR: {fold_df.loc[best_fold_idx, 'auc_pr']:.4f}\\n\")\n",
    "    f.write(f\"  AUC-ROC: {fold_df.loc[best_fold_idx, 'auc_roc']:.4f}\\n\")\n",
    "\n",
    "print(f\"‚úÖ Summary saved: {summary_path}\")\n",
    "\n",
    "print(f\"\\nüéâ Training complete! All files saved to {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Complete! üéâ\n",
    "\n",
    "### What was saved:\n",
    "1. **Model**: `exoplanet_xgboost_pipeline.pkl` - Trained XGBoost pipeline\n",
    "2. **Features**: `feature_columns.txt` - List of feature names\n",
    "3. **Metrics**: `training_metrics.csv` - Cross-validation results\n",
    "4. **Summary**: `training_summary.txt` - Training overview\n",
    "\n",
    "### Next Steps:\n",
    "- Use `04_newdata_inference.ipynb` to make predictions on new data\n",
    "- Use `05_metrics_dashboard.ipynb` to analyze model performance\n",
    "\n",
    "### Issues Fixed:\n",
    "- ‚úÖ Cell execution order corrected\n",
    "- ‚úÖ `feature_cols` defined before use\n",
    "- ‚úÖ DataFrame indexing uses `.iloc[]`\n",
    "- ‚úÖ Removed broken calibration code\n",
    "- ‚úÖ Removed undefined variable references"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}