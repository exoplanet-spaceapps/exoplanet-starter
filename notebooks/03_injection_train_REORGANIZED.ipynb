{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef56ebc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 步驟 0: 安裝套件與修復 NumPy 2.0 相容性 (Colab 環境)\n",
    "# ⚠️ 重要: 若在 Google Colab，執行此 cell 後請手動重啟 Runtime (Runtime → Restart runtime)\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"📍 偵測到 Google Colab 環境\")\n",
    "    print(\"🔧 安裝相容版本套件...\")\n",
    "    !pip install -q numpy==1.26.4 pandas astropy scipy'<1.13' matplotlib scikit-learn\n",
    "    !pip install -q lightkurve astroquery transitleastsquares wotan joblib\n",
    "    !pip install -q 'xgboost>=2.0.0' shap plotly pyyaml\n",
    "    print(\"✅ 套件安裝完成!\")\n",
    "    print(\"⚠️ 請現在手動重啟 Runtime: Runtime → Restart runtime\")\n",
    "    print(\"   然後從下一個 cell 繼續執行\")\n",
    "else:\n",
    "    print(\"💻 本地環境，跳過套件安裝\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec14ec8f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 步驟 0: 安裝套件與修復 NumPy 2.0 相容性 (Colab 環境)\n",
    "# ⚠️ 重要: 若在 Google Colab，執行此 cell 後請手動重啟 Runtime (Runtime → Restart runtime)\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"📍 偵測到 Google Colab 環境\")\n",
    "    print(\"🔧 安裝相容版本套件...\")\n",
    "    !pip install -q numpy==1.26.4 pandas astropy scipy'<1.13' matplotlib scikit-learn\n",
    "    !pip install -q lightkurve astroquery transitleastsquares wotan xgboost joblib\n",
    "    print(\"✅ 套件安裝完成!\")\n",
    "    print(\"⚠️ 請現在手動重啟 Runtime: Runtime → Restart runtime\")\n",
    "    print(\"   然後從下一個 cell 繼續執行\")\n",
    "else:\n",
    "    print(\"💻 本地環境，跳過套件安裝\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991ecdb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T23:32:51.444787Z",
     "iopub.status.busy": "2025-09-29T23:32:51.444436Z",
     "iopub.status.idle": "2025-09-29T23:32:53.554847Z",
     "shell.execute_reply": "2025-09-29T23:32:53.554388Z"
    },
    "papermill": {
     "duration": 2.123565,
     "end_time": "2025-09-29T23:32:53.555783",
     "exception": false,
     "start_time": "2025-09-29T23:32:51.432218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Phase 3-4 imports loaded successfully\n",
      "  - Pipeline, SimpleImputer, RobustScaler\n",
      "  - StratifiedGroupKFold\n",
      "  - create_exoplanet_pipeline, get_xgboost_gpu_params\n"
     ]
    }
   ],
   "source": [
    "# Phase 3-4 新增導入\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "# 導入自訂模組 (Phase 3)\n",
    "import sys\n",
    "import os\n",
    "if '/content' in os.getcwd():  # Colab 環境\n",
    "    sys.path.append('/content/exoplanet-starter/src')\n",
    "else:  # 本地環境\n",
    "    sys.path.append(os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "from models.pipeline import create_exoplanet_pipeline\n",
    "from utils.gpu_utils import get_xgboost_gpu_params, log_gpu_info\n",
    "\n",
    "print(\"✅ Phase 3-4 imports loaded successfully\")\n",
    "print(\"  - Pipeline, SimpleImputer, RobustScaler\")\n",
    "print(\"  - StratifiedGroupKFold\")\n",
    "print(\"  - create_exoplanet_pipeline, get_xgboost_gpu_params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12dafd0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import calibration visualization utility\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from utils.calibration_viz import plot_calibration_curves\n",
    "\n",
    "# Create calibration curves plot\n",
    "predictions_dict = {\n",
    "    'Uncalibrated': y_pred_uncal,\n",
    "    'Isotonic': y_pred_isotonic,\n",
    "    'Platt': y_pred_platt\n",
    "}\n",
    "\n",
    "calibration_plot_path = reports_dir / 'calibration_curves.png'\n",
    "plot_calibration_curves(\n",
    "    y_true=y_test,\n",
    "    predictions=predictions_dict,\n",
    "    output_path=calibration_plot_path,\n",
    "    n_bins=10\n",
    ")\n",
    "\n",
    "print(f\"✅ Calibration curves visualization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21377783",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 標準函式庫\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# 數據處理\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 天文資料\n",
    "import lightkurve as lk\n",
    "\n",
    "# 機器學習\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    "    brier_score_loss,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# 視覺化\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# 設定視覺化風格\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# 導入自定義模組\n",
    "from app.injection import (\n",
    "    inject_box_transit,\n",
    "    generate_synthetic_dataset,\n",
    "    save_synthetic_dataset,\n",
    "    generate_transit_parameters\n",
    ")\n",
    "\n",
    "from app.bls_features import (\n",
    "    run_bls,\n",
    "    extract_features,\n",
    "    extract_features_batch,\n",
    "    compute_feature_importance,\n",
    "    create_feature_schema\n",
    ")\n",
    "\n",
    "print(\"📚 套件導入完成\")\n",
    "print(f\"   NumPy 版本: {np.__version__}\")\n",
    "print(f\"   Pandas 版本: {pd.__version__}\")\n",
    "print(f\"   Scikit-learn 版本: {sklearn.__version__}\")\n",
    "print(f\"   XGBoost 版本: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c37b76",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"📊 Probability Calibration Training\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get uncalibrated predictions\n",
    "y_pred_uncal = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Method 1: Isotonic Regression\n",
    "print(\"\\n1️⃣ Training Isotonic Calibration...\")\n",
    "calibrated_isotonic = CalibratedClassifierCV(\n",
    "    xgb_model, \n",
    "    method='isotonic', \n",
    "    cv='prefit'\n",
    ")\n",
    "calibrated_isotonic.fit(X_train, y_train)\n",
    "y_pred_isotonic = calibrated_isotonic.predict_proba(X_test)[:, 1]\n",
    "print(\"   ✅ Isotonic calibration complete\")\n",
    "\n",
    "# Method 2: Platt Scaling (Sigmoid)\n",
    "print(\"\\n2️⃣ Training Platt Scaling (Sigmoid)...\")\n",
    "calibrated_platt = CalibratedClassifierCV(\n",
    "    xgb_model, \n",
    "    method='sigmoid', \n",
    "    cv='prefit'\n",
    ")\n",
    "calibrated_platt.fit(X_train, y_train)\n",
    "y_pred_platt = calibrated_platt.predict_proba(X_test)[:, 1]\n",
    "print(\"   ✅ Platt calibration complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e213d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 環境設定與依賴安裝（Colab）\n",
    "import sys, subprocess, pkgutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def pipi(*pkgs):\n",
    "    \"\"\"安裝套件的輔助函式\"\"\"\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *pkgs])\n",
    "\n",
    "# 安裝必要套件（避免 numpy 2.0 相容性問題）\n",
    "print(\"🚀 正在安裝依賴套件...\")\n",
    "try:\n",
    "    import numpy as np\n",
    "    import lightkurve as lk\n",
    "    import sklearn\n",
    "    import xgboost\n",
    "    print(\"✅ 基礎套件已安裝\")\n",
    "except Exception:\n",
    "    pipi(\"numpy<2\", \"lightkurve\", \"astroquery\", \"scikit-learn\", \n",
    "         \"matplotlib\", \"seaborn\", \"xgboost\", \"joblib\", \"pandas\", \"pyarrow\")\n",
    "    print(\"✅ 依賴套件安裝完成\")\n",
    "\n",
    "# 檢查是否在 Colab 環境\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"📍 在 Google Colab 環境執行\")\n",
    "    # Clone repository if needed\n",
    "    import os\n",
    "    if not os.path.exists('/content/exoplanet-starter'):\n",
    "        !git clone https://github.com/exoplanet-spaceapps/exoplanet-starter.git /content/exoplanet-starter\n",
    "        os.chdir('/content/exoplanet-starter')\n",
    "    sys.path.append('/content/exoplanet-starter')\n",
    "else:\n",
    "    print(\"💻 在本地環境執行\")\n",
    "    import os\n",
    "    os.chdir(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "    sys.path.append(os.getcwd())\n",
    "\n",
    "print(\"\\n環境設定完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48130317",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 提取特徵\n",
    "print(\"🔍 開始批次特徵提取...\")\n",
    "print(\"   這可能需要幾分鐘時間...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 批次提取特徵\n",
    "features_df = extract_features_batch(\n",
    "    samples_df,\n",
    "    compute_advanced=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✅ 特徵提取完成\")\n",
    "print(f\"   耗時: {elapsed_time:.1f} 秒\")\n",
    "print(f\"   平均每個樣本: {elapsed_time/len(samples_df):.2f} 秒\")\n",
    "print(f\"   提取特徵數: {len(features_df.columns) - 2}\")  # 扣除 sample_id 和 label\n",
    "\n",
    "# 顯示特徵列表\n",
    "feature_cols = [col for col in features_df.columns if col not in ['sample_id', 'label']]\n",
    "print(f\"\\n📋 特徵列表:\")\n",
    "for i, feat in enumerate(feature_cols, 1):\n",
    "    print(f\"   {i:2d}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf3bcdf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 4. 特徵萃取\n",
    "\n",
    "### 4.1 批次提取 BLS 特徵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918c7713",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 5.1 Phase 3-4: Sklearn Pipeline + StratifiedGroupKFold 交叉驗證\n",
    "\n",
    "**Phase 3 新功能:**\n",
    "- ✨ 使用 `create_exoplanet_pipeline()` 取代基礎 XGBoost\n",
    "- 🔄 整合 SimpleImputer + RobustScaler 前處理\n",
    "- 🚀 GPU 加速 (自動偵測並使用 `device='cuda'`)\n",
    "- 🎯 `random_state=42` 確保可重現性\n",
    "\n",
    "**Phase 4 新功能:**\n",
    "- 📊 StratifiedGroupKFold 5-fold 交叉驗證\n",
    "- 🔒 按 `target_id` 分組避免資料洩漏\n",
    "- 📈 記錄每個 fold 的詳細指標\n",
    "- 📉 計算平均 AUC-PR 與標準差\n",
    "\n",
    "**為什麼使用 StratifiedGroupKFold?**\n",
    "1. **Stratified**: 保持每個 fold 的類別比例一致\n",
    "2. **Grouped**: 同一 target 的所有樣本留在同一 fold\n",
    "3. **防止洩漏**: 訓練時不會看到測試目標的任何資料"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9af972",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 03 · 合成注入訓練管線\n",
    "\n",
    "## 工作流程\n",
    "1. **資料生成**：合成注入 200 正類 + 200 負類\n",
    "2. **特徵萃取**：BLS/TLS 指標 + 幾何統計\n",
    "3. **模型訓練**：LogisticRegression/XGBoost + 機率校準\n",
    "4. **評估指標**：PR-AUC, Precision@K, ECE, Brier Score\n",
    "5. **持久化**：儲存模型與特徵架構\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c2e3ed",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 7. SHAP Explainability Analysis\n",
    "\n",
    "使用 SHAP (SHapley Additive exPlanations) 分析模型特徵重要性與可解釋性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aefd111",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 4.2 特徵重要性分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c02a61",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 4.3 建立特徵架構"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4182e455",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 9.3 提取真實資料特徵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43a3a6b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 9.3 提取真實資料特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57429325",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T23:32:53.572653Z",
     "iopub.status.busy": "2025-09-29T23:32:53.572234Z",
     "iopub.status.idle": "2025-09-29T23:32:55.765732Z",
     "shell.execute_reply": "2025-09-29T23:32:55.764842Z"
    },
    "papermill": {
     "duration": 2.202275,
     "end_time": "2025-09-29T23:32:55.767019",
     "exception": true,
     "start_time": "2025-09-29T23:32:53.564744",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Phase 3: 創建 Sklearn Pipeline with Preprocessing\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost 參數: {'tree_method': 'hist', 'device': 'cuda'}\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'feature_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 創建 pipeline (Phase 3)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m create_exoplanet_pipeline(\n\u001b[1;32m---> 12\u001b[0m     numerical_features\u001b[38;5;241m=\u001b[39m\u001b[43mfeature_cols\u001b[49m,\n\u001b[0;32m     13\u001b[0m     xgb_params\u001b[38;5;241m=\u001b[39mgpu_params,\n\u001b[0;32m     14\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     15\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m,\n\u001b[0;32m     16\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     17\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Pipeline 創建成功:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   步驟 1: SimpleImputer (median)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_cols' is not defined"
     ]
    }
   ],
   "source": [
    "# Phase 3-4: 使用 Pipeline + StratifiedGroupKFold 訓練\n",
    "print(\"🚀 Phase 3: 創建 Sklearn Pipeline with Preprocessing\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 獲取 GPU 參數\n",
    "gpu_params = get_xgboost_gpu_params()\n",
    "print(f\"XGBoost 參數: {gpu_params}\")\n",
    "print()\n",
    "\n",
    "# 創建 pipeline (Phase 3)\n",
    "pipeline = create_exoplanet_pipeline(\n",
    "    numerical_features=feature_cols,\n",
    "    xgb_params=gpu_params,\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"✅ Pipeline 創建成功:\")\n",
    "print(\"   步驟 1: SimpleImputer (median)\")\n",
    "print(\"   步驟 2: RobustScaler (robust to outliers)\")\n",
    "print(\"   步驟 3: XGBClassifier (GPU-accelerated)\")\n",
    "print()\n",
    "\n",
    "# Phase 4: StratifiedGroupKFold 交叉驗證\n",
    "print(\"📊 Phase 4: StratifiedGroupKFold 5-Fold Cross-Validation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "n_splits = 5\n",
    "sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# 儲存每個 fold 的結果\n",
    "fold_results = []\n",
    "fold_models = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(sgkf.split(X, y, groups), 1):\n",
    "    print(f\"\\n🔄 Fold {fold_idx}/{n_splits}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # 分割資料\n",
    "    X_train_fold, X_test_fold = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train_fold, y_test_fold = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    # 訓練 pipeline (包含前處理 + XGBoost)\n",
    "    pipeline_fold = create_exoplanet_pipeline(\n",
    "        numerical_features=feature_cols,\n",
    "        xgb_params=gpu_params,\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42 + fold_idx  # 每個 fold 不同的 seed\n",
    "    )\n",
    "    \n",
    "    pipeline_fold.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # 預測\n",
    "    y_pred_proba_fold = pipeline_fold.predict_proba(X_test_fold)[:, 1]\n",
    "    \n",
    "    # 計算指標\n",
    "    from sklearn.metrics import (\n",
    "        average_precision_score, \n",
    "        roc_auc_score, \n",
    "        precision_score, \n",
    "        recall_score\n",
    "    )\n",
    "    \n",
    "    ap_score = average_precision_score(y_test_fold, y_pred_proba_fold)\n",
    "    auc_score = roc_auc_score(y_test_fold, y_pred_proba_fold)\n",
    "    \n",
    "    # 使用 threshold=0.5 計算 precision/recall\n",
    "    y_pred_binary = (y_pred_proba_fold >= 0.5).astype(int)\n",
    "    precision = precision_score(y_test_fold, y_pred_binary, zero_division=0)\n",
    "    recall = recall_score(y_test_fold, y_pred_binary)\n",
    "    \n",
    "    fold_results.append({\n",
    "        'fold': fold_idx,\n",
    "        'train_size': len(train_idx),\n",
    "        'test_size': len(test_idx),\n",
    "        'test_pos_ratio': y_test_fold.mean(),\n",
    "        'auc_pr': ap_score,\n",
    "        'auc_roc': auc_score,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    })\n",
    "    \n",
    "    fold_models.append(pipeline_fold)\n",
    "    \n",
    "    print(f\"   訓練集: {len(train_idx)} 樣本\")\n",
    "    print(f\"   測試集: {len(test_idx)} 樣本 (正樣本: {y_test_fold.mean():.2%})\")\n",
    "    print(f\"   AUC-PR:  {ap_score:.4f}\")\n",
    "    print(f\"   AUC-ROC: {auc_score:.4f}\")\n",
    "    print(f\"   Precision@0.5: {precision:.4f}\")\n",
    "    print(f\"   Recall@0.5: {recall:.4f}\")\n",
    "\n",
    "# 匯總結果\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📈 Phase 4: Cross-Validation 匯總結果\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fold_df = pd.DataFrame(fold_results)\n",
    "\n",
    "print(f\"\\nAUC-PR:  {fold_df['auc_pr'].mean():.4f} ± {fold_df['auc_pr'].std():.4f}\")\n",
    "print(f\"AUC-ROC: {fold_df['auc_roc'].mean():.4f} ± {fold_df['auc_roc'].std():.4f}\")\n",
    "print(f\"Precision@0.5: {fold_df['precision'].mean():.4f} ± {fold_df['precision'].std():.4f}\")\n",
    "print(f\"Recall@0.5: {fold_df['recall'].mean():.4f} ± {fold_df['recall'].std():.4f}\")\n",
    "\n",
    "print(\"\\n📊 各 Fold 詳細結果:\")\n",
    "print(fold_df.to_string(index=False))\n",
    "\n",
    "# 保存最佳模型 (根據 AUC-PR)\n",
    "best_fold_idx = fold_df['auc_pr'].idxmax()\n",
    "best_model = fold_models[best_fold_idx]\n",
    "print(f\"\\n✅ 最佳模型: Fold {best_fold_idx + 1} (AUC-PR: {fold_df.loc[best_fold_idx, 'auc_pr']:.4f})\")\n",
    "\n",
    "# 將最佳模型保存到 models dict (以便後續使用)\n",
    "models = {'XGBoost_Pipeline_CV': best_model}\n",
    "print(\"   已保存為 models['XGBoost_Pipeline_CV']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318feb78",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'supervised_features_df' in locals() and len(supervised_features_df) > 10:    print(\"Phase 3-4: Training supervised model (Pipeline + CV)...\")    print(\"=\"*60)    # Prepare data    X_supervised = supervised_features_df[feature_cols].values    y_supervised = supervised_features_df['label'].values    # Handle invalid values    X_supervised = np.nan_to_num(X_supervised, nan=0.0, posinf=0.0, neginf=0.0)    # Phase 4: Create target groups (from target_id or TIC_ID)    if 'target_id' in supervised_features_df.columns:        groups_supervised = supervised_features_df['target_id'].values        print(f\"Using target_id for grouping\")    elif 'tic_id' in supervised_features_df.columns:        groups_supervised = supervised_features_df['tic_id'].values        print(f\"Using tic_id for grouping\")    elif 'sample_id' in supervised_features_df.columns:        groups_supervised = supervised_features_df['sample_id'].apply(            lambda x: hash(str(x)) % 10000        ).values        print(f\"Generated groups from sample_id\")    else:        groups_supervised = np.arange(len(y_supervised))        print(f\"No grouping column found, using individual groups\")    print(f\"Supervised Dataset:\")    print(f\"   Total samples: {len(X_supervised)}\")    print(f\"   Features: {X_supervised.shape[1]}\")    print(f\"   Positive ratio: {y_supervised.mean():.2%}\")    print(f\"   Unique groups: {len(np.unique(groups_supervised))}\")    print()    # Phase 3: Use Pipeline + GPU    gpu_params_sup = get_xgboost_gpu_params()    print(f\"XGBoost GPU params: {gpu_params_sup}\")    print()    # Phase 4: StratifiedGroupKFold cross-validation    print(\"Phase 4: StratifiedGroupKFold 5-Fold Cross-Validation\")    print(\"-\" * 60)    n_splits_sup = 5    sgkf_sup = StratifiedGroupKFold(n_splits=n_splits_sup, shuffle=True, random_state=42)    fold_results_sup = []    fold_models_sup = []    for fold_idx, (train_idx, test_idx) in enumerate(sgkf_sup.split(X_supervised, y_supervised, groups_supervised), 1):        print(f\"Fold {fold_idx}/{n_splits_sup}\")        print(\"-\" * 40)        # Split data        X_train_fold_sup, X_test_fold_sup = X_supervised[train_idx], X_supervised[test_idx]        y_train_fold_sup, y_test_fold_sup = y_supervised[train_idx], y_supervised[test_idx]        # Train pipeline (Phase 3)        pipeline_fold_sup = create_exoplanet_pipeline(            numerical_features=feature_cols,            xgb_params=gpu_params_sup,            n_estimators=100,            max_depth=6,            learning_rate=0.1,            random_state=42 + fold_idx        )        pipeline_fold_sup.fit(X_train_fold_sup, y_train_fold_sup)        # Predict        y_pred_proba_fold_sup = pipeline_fold_sup.predict_proba(X_test_fold_sup)[:, 1]        # Calculate metrics        from sklearn.metrics import average_precision_score, roc_auc_score        ap_score_sup = average_precision_score(y_test_fold_sup, y_pred_proba_fold_sup)        auc_score_sup = roc_auc_score(y_test_fold_sup, y_pred_proba_fold_sup)        fold_results_sup.append({            'fold': fold_idx,            'train_size': len(train_idx),            'test_size': len(test_idx),            'test_pos_ratio': y_test_fold_sup.mean(),            'auc_pr': ap_score_sup,            'auc_roc': auc_score_sup        })        fold_models_sup.append(pipeline_fold_sup)        print(f\"   Train: {len(train_idx)} samples\")        print(f\"   Test: {len(test_idx)} samples (pos: {y_test_fold_sup.mean():.2%})\")        print(f\"   AUC-PR:  {ap_score_sup:.4f}\")        print(f\"   AUC-ROC: {auc_score_sup:.4f}\")    # Summary results    print(\"\" + \"=\"*60)    print(\"Phase 4: Cross-Validation Summary Results\")    print(\"=\"*60)    fold_df_sup = pd.DataFrame(fold_results_sup)    print(f\"AUC-PR:  {fold_df_sup['auc_pr'].mean():.4f} +/- {fold_df_sup['auc_pr'].std():.4f}\")    print(f\"AUC-ROC: {fold_df_sup['auc_roc'].mean():.4f} +/- {fold_df_sup['auc_roc'].std():.4f}\")    print(\"Detailed results per fold:\")    print(fold_df_sup.to_string(index=False))    # Save best model    best_fold_idx_sup = fold_df_sup['auc_pr'].idxmax()    best_model_supervised = fold_models_sup[best_fold_idx_sup]    print(f\"Best supervised model: Fold {best_fold_idx_sup + 1} (AUC-PR: {fold_df_sup.loc[best_fold_idx_sup, 'auc_pr']:.4f})\")    # Save to models dict    models['XGBoost_Supervised_Pipeline_CV'] = best_model_supervised    metrics_supervised = fold_results_sup[best_fold_idx_sup]    print(\"   Saved as models['XGBoost_Supervised_Pipeline_CV']\")else:    print(\"supervised_features_df not found or insufficient samples, skipping supervised training\")    metrics_supervised = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d855ff5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'supervised_features_df' in locals() and len(supervised_features_df) > 10:    print(\"Phase 3-4: Training supervised model (Pipeline + CV)...\")    print(\"=\"*60)    # Prepare data    X_supervised = supervised_features_df[feature_cols].values    y_supervised = supervised_features_df['label'].values    # Handle invalid values    X_supervised = np.nan_to_num(X_supervised, nan=0.0, posinf=0.0, neginf=0.0)    # Phase 4: Create target groups (from target_id or TIC_ID)    if 'target_id' in supervised_features_df.columns:        groups_supervised = supervised_features_df['target_id'].values        print(f\"Using target_id for grouping\")    elif 'tic_id' in supervised_features_df.columns:        groups_supervised = supervised_features_df['tic_id'].values        print(f\"Using tic_id for grouping\")    elif 'sample_id' in supervised_features_df.columns:        groups_supervised = supervised_features_df['sample_id'].apply(            lambda x: hash(str(x)) % 10000        ).values        print(f\"Generated groups from sample_id\")    else:        groups_supervised = np.arange(len(y_supervised))        print(f\"No grouping column found, using individual groups\")    print(f\"Supervised Dataset:\")    print(f\"   Total samples: {len(X_supervised)}\")    print(f\"   Features: {X_supervised.shape[1]}\")    print(f\"   Positive ratio: {y_supervised.mean():.2%}\")    print(f\"   Unique groups: {len(np.unique(groups_supervised))}\")    print()    # Phase 3: Use Pipeline + GPU    gpu_params_sup = get_xgboost_gpu_params()    print(f\"XGBoost GPU params: {gpu_params_sup}\")    print()    # Phase 4: StratifiedGroupKFold cross-validation    print(\"Phase 4: StratifiedGroupKFold 5-Fold Cross-Validation\")    print(\"-\" * 60)    n_splits_sup = 5    sgkf_sup = StratifiedGroupKFold(n_splits=n_splits_sup, shuffle=True, random_state=42)    fold_results_sup = []    fold_models_sup = []    for fold_idx, (train_idx, test_idx) in enumerate(sgkf_sup.split(X_supervised, y_supervised, groups_supervised), 1):        print(f\"Fold {fold_idx}/{n_splits_sup}\")        print(\"-\" * 40)        # Split data        X_train_fold_sup, X_test_fold_sup = X_supervised[train_idx], X_supervised[test_idx]        y_train_fold_sup, y_test_fold_sup = y_supervised[train_idx], y_supervised[test_idx]        # Train pipeline (Phase 3)        pipeline_fold_sup = create_exoplanet_pipeline(            numerical_features=feature_cols,            xgb_params=gpu_params_sup,            n_estimators=100,            max_depth=6,            learning_rate=0.1,            random_state=42 + fold_idx        )        pipeline_fold_sup.fit(X_train_fold_sup, y_train_fold_sup)        # Predict        y_pred_proba_fold_sup = pipeline_fold_sup.predict_proba(X_test_fold_sup)[:, 1]        # Calculate metrics        from sklearn.metrics import average_precision_score, roc_auc_score        ap_score_sup = average_precision_score(y_test_fold_sup, y_pred_proba_fold_sup)        auc_score_sup = roc_auc_score(y_test_fold_sup, y_pred_proba_fold_sup)        fold_results_sup.append({            'fold': fold_idx,            'train_size': len(train_idx),            'test_size': len(test_idx),            'test_pos_ratio': y_test_fold_sup.mean(),            'auc_pr': ap_score_sup,            'auc_roc': auc_score_sup        })        fold_models_sup.append(pipeline_fold_sup)        print(f\"   Train: {len(train_idx)} samples\")        print(f\"   Test: {len(test_idx)} samples (pos: {y_test_fold_sup.mean():.2%})\")        print(f\"   AUC-PR:  {ap_score_sup:.4f}\")        print(f\"   AUC-ROC: {auc_score_sup:.4f}\")    # Summary results    print(\"\" + \"=\"*60)    print(\"Phase 4: Cross-Validation Summary Results\")    print(\"=\"*60)    fold_df_sup = pd.DataFrame(fold_results_sup)    print(f\"AUC-PR:  {fold_df_sup['auc_pr'].mean():.4f} +/- {fold_df_sup['auc_pr'].std():.4f}\")    print(f\"AUC-ROC: {fold_df_sup['auc_roc'].mean():.4f} +/- {fold_df_sup['auc_roc'].std():.4f}\")    print(\"Detailed results per fold:\")    print(fold_df_sup.to_string(index=False))    # Save best model    best_fold_idx_sup = fold_df_sup['auc_pr'].idxmax()    best_model_supervised = fold_models_sup[best_fold_idx_sup]    print(f\"Best supervised model: Fold {best_fold_idx_sup + 1} (AUC-PR: {fold_df_sup.loc[best_fold_idx_sup, 'auc_pr']:.4f})\")    # Save to models dict    models['XGBoost_Supervised_Pipeline_CV'] = best_model_supervised    metrics_supervised = fold_results_sup[best_fold_idx_sup]    print(\"   Saved as models['XGBoost_Supervised_Pipeline_CV']\")else:    print(\"supervised_features_df not found or insufficient samples, skipping supervised training\")    metrics_supervised = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c55c5f9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Phase 3-4: 準備資料與特徵 (with grouping)\n",
    "print(\"📊 Phase 3-4: 準備訓練資料與 StratifiedGroupKFold\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 提取特徵和標籤\n",
    "X = features_df[feature_cols].values\n",
    "y = features_df['label'].values\n",
    "\n",
    "# 處理無效值 (NaN, Inf)\n",
    "X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# 創建 target groups (對於合成資料，我們用 sample_id 的前綴作為 group)\n",
    "# 這模擬了\"同一目標的多次觀測\"\n",
    "if 'sample_id' in features_df.columns:\n",
    "    # 從 sample_id 提取 group (例如: \"sample_0001_obs1\" -> group \"0001\")\n",
    "    groups = features_df['sample_id'].apply(\n",
    "        lambda x: int(str(x).split('_')[1]) if '_' in str(x) else hash(str(x)) % 1000\n",
    "    ).values\n",
    "else:\n",
    "    # 如果沒有 sample_id，創建假的 groups (每個樣本一組)\n",
    "    groups = np.arange(len(y))\n",
    "\n",
    "print(f\"   總樣本數: {len(X)}\")\n",
    "print(f\"   特徵維度: {X.shape[1]}\")\n",
    "print(f\"   正樣本比例: {y.mean():.2%}\")\n",
    "print(f\"   唯一 groups 數: {len(np.unique(groups))}\")\n",
    "print()\n",
    "\n",
    "# Log GPU info\n",
    "print(\"🖥️  GPU 配置:\")\n",
    "log_gpu_info()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c886c9e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 5. 模型訓練與校準\n",
    "\n",
    "### 5.1 資料準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feaf896",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 5.2 訓練多個模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d27e30",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 9. 監督式學習管線（真實 TOI + Kepler EB 資料）\n",
    "\n",
    "> 💡 **新增功能**：使用真實的 TOI（正類）和 Kepler EB（負類）資料進行監督式訓練，與合成注入方法比較。\n",
    "\n",
    "### 9.1 載入真實資料集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aba9aa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 9. 監督式學習管線（真實 TOI + Kepler EB 資料）\n",
    "\n",
    "> 💡 **新增功能**：使用真實的 TOI（正類）和 Kepler EB（負類）資料進行監督式訓練，與合成注入方法比較。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6298eddd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 9. 監督式學習管線（真實 TOI + Kepler EB 資料）\n",
    "\n",
    "> 💡 **新增功能**：使用真實的 TOI（正類）和 Kepler EB（負類）資料進行監督式訓練，與合成注入方法比較。\n",
    "\n",
    "### 9.1 載入真實資料集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e8313b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 9.4 訓練監督式模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180169a6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 9. 監督式學習管線（真實 TOI + Kepler EB 資料）\n",
    "\n",
    "> 💡 **新增功能**：使用真實的 TOI（正類）和 Kepler EB（負類）資料進行監督式訓練，與合成注入方法比較。\n",
    "\n",
    "### 9.1 載入真實資料集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea138779",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 9.4 訓練監督式模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67af8223",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 🚀 GitHub Push 終極解決方案\n",
    "\n",
    "將合成注入與監督式訓練結果推送到 GitHub 倉庫："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c708a3a2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 建立輸出目錄\n",
    "output_dir = Path(\"model\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 儲存模型\n",
    "print(\"💾 儲存模型與相關檔案...\\n\")\n",
    "\n",
    "# 1. 儲存校準模型\n",
    "model_path = output_dir / \"ranker.joblib\"\n",
    "joblib.dump(calibrated_model, model_path)\n",
    "print(f\"✅ 模型已儲存: {model_path}\")\n",
    "\n",
    "# 2. 儲存特徵標準化器\n",
    "scaler_path = output_dir / \"scaler.joblib\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"✅ 標準化器已儲存: {scaler_path}\")\n",
    "\n",
    "# 3. 儲存特徵架構 (with fallback if file doesn't exist)\n",
    "import shutil\n",
    "feature_schema_source = Path(\"data/feature_schema.json\")\n",
    "if feature_schema_source.exists():\n",
    "    shutil.copy(feature_schema_source, output_dir / \"feature_schema.json\")\n",
    "    print(f\"✅ 特徵架構已複製: {output_dir / 'feature_schema.json'}\")\n",
    "else:\n",
    "    # Create feature schema from current feature_cols\n",
    "    feature_schema = {\n",
    "        \"features\": feature_cols,\n",
    "        \"n_features\": len(feature_cols),\n",
    "        \"created_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"note\": \"Generated from training data\"\n",
    "    }\n",
    "    schema_path = output_dir / \"feature_schema.json\"\n",
    "    with open(schema_path, 'w') as f:\n",
    "        json.dump(feature_schema, f, indent=2)\n",
    "    print(f\"✅ 特徵架構已生成: {schema_path}\")\n",
    "\n",
    "# 4. 儲存模型元資料\n",
    "metadata = {\n",
    "    \"model_type\": \"XGBoost with Isotonic Calibration\",\n",
    "    \"training_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"n_features\": len(feature_cols),\n",
    "    \"feature_names\": feature_cols,\n",
    "    \"training_samples\": len(X_train),\n",
    "    \"test_samples\": len(X_test),\n",
    "    \"metrics\": metrics_calibrated,\n",
    "    \"parameters\": {\n",
    "        \"period_range\": [0.6, 10.0],\n",
    "        \"depth_range\": [0.0005, 0.02],\n",
    "        \"duration_fraction_range\": [0.02, 0.1]\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = output_dir / \"model_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "print(f\"✅ 元資料已儲存: {metadata_path}\")\n",
    "\n",
    "print(\"\\n📦 所有檔案已成功儲存至 'model/' 目錄\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f040bcc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"📊 訓練管線執行總結\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\"\"\n",
    "🎯 資料集:\n",
    "   • 總樣本數: {len(samples_df)}\n",
    "   • 正樣本: {len(samples_df[samples_df['label'] == 1])}\n",
    "   • 負樣本: {len(samples_df[samples_df['label'] == 0])}\n",
    "   \n",
    "🔍 特徵工程:\n",
    "   • 特徵數量: {len(feature_cols)}\n",
    "   • Top 3 重要特徵:\n",
    "\"\"\")\n",
    "\n",
    "for idx, row in importance_df.head(3).iterrows():\n",
    "    print(f\"     - {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "🤖 模型效能:\n",
    "   • PR-AUC: {metrics_calibrated['PR-AUC']:.3f}\n",
    "   • ROC-AUC: {metrics_calibrated['ROC-AUC']:.3f}\n",
    "   • Brier Score: {metrics_calibrated['Brier Score']:.3f}\n",
    "   • ECE: {metrics_calibrated['ECE']:.3f}\n",
    "   • Precision@10: {metrics_calibrated.get('P@10', 'N/A')}\n",
    "   \n",
    "💡 關鍵發現:\n",
    "   1. Isotonic 校準顯著改善了機率預測的可靠性\n",
    "   2. BLS 特徵（週期、SNR、深度）是最重要的預測因子\n",
    "   3. 模型在高置信度預測上表現優異（高 Precision@K）\n",
    "   \n",
    "📦 輸出檔案:\n",
    "   • 模型: model/ranker.joblib\n",
    "   • 標準化器: model/scaler.joblib\n",
    "   • 特徵架構: model/feature_schema.json\n",
    "   • 元資料: model/model_metadata.json\n",
    "   • 合成資料: data/synthetic/\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"✅ 訓練管線完成！\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de2250",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'metrics_supervised' in locals() and metrics_supervised is not None:\n",
    "    print(\"💾 儲存監督式模型...\")\n",
    "\n",
    "    # 建立輸出目錄\n",
    "    supervised_dir = Path(\"model/supervised\")\n",
    "    supervised_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 儲存模型\n",
    "    joblib.dump(calibrated_supervised, supervised_dir / \"ranker_supervised.joblib\")\n",
    "    joblib.dump(scaler_supervised, supervised_dir / \"scaler_supervised.joblib\")\n",
    "\n",
    "    # 儲存元資料\n",
    "    supervised_metadata = {\n",
    "        \"model_type\": \"XGBoost with Isotonic Calibration (Supervised)\",\n",
    "        \"training_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"data_sources\": {\n",
    "            \"positive\": \"TOI (PC/CP/KP)\",\n",
    "            \"negative\": \"Kepler EB + TOI FP\"\n",
    "        },\n",
    "        \"n_features\": len(feature_cols),\n",
    "        \"feature_names\": feature_cols,\n",
    "        \"training_samples\": len(X_train_sup),\n",
    "        \"test_samples\": len(X_test_sup),\n",
    "        \"metrics\": metrics_supervised\n",
    "    }\n",
    "\n",
    "    with open(supervised_dir / \"model_metadata.json\", 'w') as f:\n",
    "        json.dump(supervised_metadata, f, indent=2, default=str)\n",
    "\n",
    "    print(f\"✅ 監督式模型已儲存至: {supervised_dir}\")\n",
    "\n",
    "    # 儲存比較結果\n",
    "    if 'comparison_df' in locals() and comparison_df is not None:\n",
    "        comparison_df.to_csv(\"model/method_comparison.csv\", index=False)\n",
    "        print(\"✅ 方法比較結果已儲存至: model/method_comparison.csv\")\n",
    "else:\n",
    "    print(\"⚠️ 無監督式模型可儲存\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c485c54",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"📊 完整訓練管線執行總結\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "🎯 資料集統計:\n",
    "\n",
    "   【合成注入資料】\n",
    "   • 總樣本數: {len(samples_df)}\n",
    "   • 正樣本: {len(samples_df[samples_df['label'] == 1])}\n",
    "   • 負樣本: {len(samples_df[samples_df['label'] == 0])}\n",
    "\n",
    "   【真實監督資料】\n",
    "   • 總樣本數: {len(supervised_features_df) if 'supervised_features_df' in locals() else 0}\n",
    "   • TOI 正樣本: {len(supervised_features_df[supervised_features_df['source']=='TOI']) if 'supervised_features_df' in locals() and len(supervised_features_df) > 0 else 0}\n",
    "   • Kepler EB 負樣本: {len(supervised_features_df[supervised_features_df['source']=='Kepler_EB']) if 'supervised_features_df' in locals() and len(supervised_features_df) > 0 else 0}\n",
    "\n",
    "🔍 特徵工程:\n",
    "   • 特徵數量: {len(feature_cols)}\n",
    "   • Top 3 重要特徵:\n",
    "\"\"\")\n",
    "\n",
    "for idx, row in importance_df.head(3).iterrows():\n",
    "    print(f\"     - {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "🤖 模型效能比較:\n",
    "\n",
    "   【合成注入方法】\n",
    "   • PR-AUC: {metrics_calibrated['PR-AUC']:.3f}\n",
    "   • ROC-AUC: {metrics_calibrated['ROC-AUC']:.3f}\n",
    "   • Brier Score: {metrics_calibrated['Brier Score']:.3f}\n",
    "   • ECE: {metrics_calibrated['ECE']:.3f}\n",
    "\"\"\")\n",
    "\n",
    "if 'metrics_supervised' in locals() and metrics_supervised is not None:\n",
    "    print(f\"\"\"\n",
    "   【監督式學習】\n",
    "   • PR-AUC: {metrics_supervised['PR-AUC']:.3f}\n",
    "   • ROC-AUC: {metrics_supervised['ROC-AUC']:.3f}\n",
    "   • Brier Score: {metrics_supervised['Brier Score']:.3f}\n",
    "   • ECE: {metrics_supervised['ECE']:.3f}\n",
    "    \"\"\")\n",
    "\n",
    "print(f\"\"\"\n",
    "💡 關鍵發現與建議:\n",
    "   1. Isotonic 校準顯著改善了機率預測的可靠性\n",
    "   2. BLS 特徵（週期、SNR、深度）是最重要的預測因子\n",
    "   3. 合成注入適合快速開發，監督式學習更接近實際應用\n",
    "   4. 建議在實際部署時結合兩種方法的優勢\n",
    "\n",
    "📦 輸出檔案:\n",
    "   • 合成模型: model/ranker.joblib\n",
    "   • 監督模型: model/supervised/ranker_supervised.joblib\n",
    "   • 特徵架構: model/feature_schema.json\n",
    "   • 比較結果: model/method_comparison.csv\n",
    "   • 資料集: data/synthetic/ 和 data/\n",
    "\n",
    "🚀 下一步:\n",
    "   1. 使用 04_newdata_inference.ipynb 對新資料進行推論\n",
    "   2. 在更大的真實資料集上訓練監督式模型\n",
    "   3. 探索深度學習方法（CNN/Transformer）\n",
    "   4. 部署為 Web 應用或 API 服務\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"✅ 訓練管線（含監督式分支）完成！\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa5a99",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'metrics_supervised' in locals() and metrics_supervised is not None:\n",
    "    print(\"💾 儲存監督式模型...\")\n",
    "\n",
    "    # 建立輸出目錄\n",
    "    supervised_dir = Path(\"model/supervised\")\n",
    "    supervised_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 儲存模型\n",
    "    joblib.dump(calibrated_supervised, supervised_dir / \"ranker_supervised.joblib\")\n",
    "    joblib.dump(scaler_supervised, supervised_dir / \"scaler_supervised.joblib\")\n",
    "\n",
    "    # 儲存元資料\n",
    "    supervised_metadata = {\n",
    "        \"model_type\": \"XGBoost with Isotonic Calibration (Supervised)\",\n",
    "        \"training_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"data_sources\": {\n",
    "            \"positive\": \"TOI (PC/CP/KP)\",\n",
    "            \"negative\": \"Kepler EB + TOI FP\"\n",
    "        },\n",
    "        \"n_features\": len(feature_cols),\n",
    "        \"feature_names\": feature_cols,\n",
    "        \"training_samples\": len(X_train_sup),\n",
    "        \"test_samples\": len(X_test_sup),\n",
    "        \"metrics\": metrics_supervised\n",
    "    }\n",
    "\n",
    "    with open(supervised_dir / \"model_metadata.json\", 'w') as f:\n",
    "        json.dump(supervised_metadata, f, indent=2, default=str)\n",
    "\n",
    "    print(f\"✅ 監督式模型已儲存至: {supervised_dir}\")\n",
    "\n",
    "    # 儲存比較結果\n",
    "    if 'comparison_df' in locals() and comparison_df is not None:\n",
    "        comparison_df.to_csv(\"model/method_comparison.csv\", index=False)\n",
    "        print(\"✅ 方法比較結果已儲存至: model/method_comparison.csv\")\n",
    "else:\n",
    "    print(\"⚠️ 無監督式模型可儲存\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5306b8c3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"📊 完整訓練管線執行總結\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "🎯 資料集統計:\n",
    "\n",
    "   【合成注入資料】\n",
    "   • 總樣本數: {len(samples_df)}\n",
    "   • 正樣本: {len(samples_df[samples_df['label'] == 1])}\n",
    "   • 負樣本: {len(samples_df[samples_df['label'] == 0])}\n",
    "\n",
    "   【真實監督資料】\n",
    "   • 總樣本數: {len(supervised_features_df) if 'supervised_features_df' in locals() else 0}\n",
    "   • TOI 正樣本: {len(supervised_features_df[supervised_features_df['source']=='TOI']) if 'supervised_features_df' in locals() and len(supervised_features_df) > 0 else 0}\n",
    "   • Kepler EB 負樣本: {len(supervised_features_df[supervised_features_df['source']=='Kepler_EB']) if 'supervised_features_df' in locals() and len(supervised_features_df) > 0 else 0}\n",
    "\n",
    "🔍 特徵工程:\n",
    "   • 特徵數量: {len(feature_cols)}\n",
    "   • Top 3 重要特徵:\n",
    "\"\"\")\n",
    "\n",
    "for idx, row in importance_df.head(3).iterrows():\n",
    "    print(f\"     - {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "🤖 模型效能比較:\n",
    "\n",
    "   【合成注入方法】\n",
    "   • PR-AUC: {metrics_calibrated['PR-AUC']:.3f}\n",
    "   • ROC-AUC: {metrics_calibrated['ROC-AUC']:.3f}\n",
    "   • Brier Score: {metrics_calibrated['Brier Score']:.3f}\n",
    "   • ECE: {metrics_calibrated['ECE']:.3f}\n",
    "\"\"\")\n",
    "\n",
    "if 'metrics_supervised' in locals() and metrics_supervised is not None:\n",
    "    print(f\"\"\"\n",
    "   【監督式學習】\n",
    "   • PR-AUC: {metrics_supervised['PR-AUC']:.3f}\n",
    "   • ROC-AUC: {metrics_supervised['ROC-AUC']:.3f}\n",
    "   • Brier Score: {metrics_supervised['Brier Score']:.3f}\n",
    "   • ECE: {metrics_supervised['ECE']:.3f}\n",
    "    \"\"\")\n",
    "\n",
    "print(f\"\"\"\n",
    "💡 關鍵發現與建議:\n",
    "   1. Isotonic 校準顯著改善了機率預測的可靠性\n",
    "   2. BLS 特徵（週期、SNR、深度）是最重要的預測因子\n",
    "   3. 合成注入適合快速開發，監督式學習更接近實際應用\n",
    "   4. 建議在實際部署時結合兩種方法的優勢\n",
    "\n",
    "📦 輸出檔案:\n",
    "   • 合成模型: model/ranker.joblib\n",
    "   • 監督模型: model/supervised/ranker_supervised.joblib\n",
    "   • 特徵架構: model/feature_schema.json\n",
    "   • 比較結果: model/method_comparison.csv\n",
    "   • 資料集: data/synthetic/ 和 data/\n",
    "\n",
    "🚀 下一步:\n",
    "   1. 使用 04_newdata_inference.ipynb 對新資料進行推論\n",
    "   2. 在更大的真實資料集上訓練監督式模型\n",
    "   3. 探索深度學習方法（CNN/Transformer）\n",
    "   4. 部署為 Web 應用或 API 服務\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"✅ 訓練管線（含監督式分支）完成！\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc71ccd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Summary of Phase 7-8 outputs\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"📋 Phase 7-8 Summary\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n✅ SHAP Explainability (Phase 7):\")\n",
    "print(f\"   • Summary plot: {shap_plot_path}\")\n",
    "print(f\"   • Top feature: {feature_importance.iloc[0]['feature']}\")\n",
    "print(f\"   • Features analyzed: {len(features)}\")\n",
    "\n",
    "print(\"\\n✅ Probability Calibration (Phase 8):\")\n",
    "print(f\"   • Best method: {best_method[0]}\")\n",
    "print(f\"   • Brier improvement: {(brier_uncal - best_method[1])/brier_uncal*100:.2f}%\")\n",
    "print(f\"   • Calibration curves: {calibration_plot_path}\")\n",
    "print(f\"   • Model card: {model_card_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed6ca6a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compare Brier scores\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"📈 Calibration Method Comparison (Brier Score - Lower is Better)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "brier_uncal = brier_score_loss(y_test, y_pred_uncal)\n",
    "brier_isotonic = brier_score_loss(y_test, y_pred_isotonic)\n",
    "brier_platt = brier_score_loss(y_test, y_pred_platt)\n",
    "\n",
    "print(f\"\\n   Uncalibrated:  {brier_uncal:.6f}\")\n",
    "print(f\"   Isotonic:      {brier_isotonic:.6f} (Δ {(brier_uncal - brier_isotonic)/brier_uncal*100:+.2f}%)\")\n",
    "print(f\"   Platt:         {brier_platt:.6f} (Δ {(brier_uncal - brier_platt)/brier_uncal*100:+.2f}%)\")\n",
    "\n",
    "# Determine best method\n",
    "best_method = min(\n",
    "    [('Uncalibrated', brier_uncal), ('Isotonic', brier_isotonic), ('Platt', brier_platt)],\n",
    "    key=lambda x: x[1]\n",
    ")\n",
    "print(f\"\\n   🏆 Best method: {best_method[0]} (Brier: {best_method[1]:.6f})\")\n",
    "\n",
    "# Store metrics\n",
    "calibration_metrics = {\n",
    "    'brier_uncalibrated': float(brier_uncal),\n",
    "    'brier_isotonic': float(brier_isotonic),\n",
    "    'brier_platt': float(brier_platt),\n",
    "    'best_method': best_method[0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9356ed",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log top 15 feature importance values\n",
    "mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'mean_abs_shap': mean_abs_shap\n",
    "}).sort_values('mean_abs_shap', ascending=False)\n",
    "\n",
    "print(\"\\n📊 Top 15 Features by SHAP Importance:\")\n",
    "print(feature_importance.head(15).to_string(index=False))\n",
    "\n",
    "# Store for later use\n",
    "shap_importance = feature_importance.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67327ee",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create and save SHAP summary plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(\n",
    "    shap_values, \n",
    "    X_test_sample, \n",
    "    feature_names=features,\n",
    "    max_display=15,\n",
    "    show=False\n",
    ")\n",
    "\n",
    "# Save plot\n",
    "reports_dir = Path('reports')\n",
    "reports_dir.mkdir(exist_ok=True)\n",
    "shap_plot_path = reports_dir / 'shap_summary.png'\n",
    "plt.tight_layout()\n",
    "plt.savefig(shap_plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✅ SHAP summary plot saved to: {shap_plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc974ea",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create SHAP explainer for XGBoost model\n",
    "print(\"=\" * 70)\n",
    "print(\"🔍 SHAP Feature Importance Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use TreeExplainer for XGBoost\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "\n",
    "# Calculate SHAP values for test set (limit to 500 samples for performance)\n",
    "X_test_sample = X_test[:500]\n",
    "shap_values = explainer.shap_values(X_test_sample)\n",
    "\n",
    "print(f\"✅ SHAP values computed for {len(X_test_sample)} test samples\")\n",
    "print(f\"   Shape: {shap_values.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c894363",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 繪製可靠度曲線\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# 未校準模型\n",
    "fraction_pos_uncal, mean_pred_uncal = calibration_curve(\n",
    "    y_test, prob_uncalibrated, n_bins=10\n",
    ")\n",
    "\n",
    "axes[0].plot(mean_pred_uncal, fraction_pos_uncal, 'o-', label='未校準', color='red')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='完美校準')\n",
    "axes[0].set_xlabel('平均預測機率')\n",
    "axes[0].set_ylabel('實際正樣本比例')\n",
    "axes[0].set_title('未校準模型可靠度曲線', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(loc='best')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 已校準模型\n",
    "fraction_pos_cal, mean_pred_cal = calibration_curve(\n",
    "    y_test, prob_calibrated, n_bins=10\n",
    ")\n",
    "\n",
    "axes[1].plot(mean_pred_cal, fraction_pos_cal, 'o-', label='已校準', color='green')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', label='完美校準')\n",
    "axes[1].set_xlabel('平均預測機率')\n",
    "axes[1].set_ylabel('實際正樣本比例')\n",
    "axes[1].set_title('已校準模型可靠度曲線', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(loc='best')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('機率校準效果比較', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 說明:\")\n",
    "print(\"   • 理想的可靠度曲線應該接近對角線\")\n",
    "print(\"   • 曲線在對角線上方表示模型過度保守\")\n",
    "print(\"   • 曲線在對角線下方表示模型過度自信\")\n",
    "print(\"   • Isotonic 校準有效改善了模型的機率預測\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5327a3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 選擇最佳模型進行校準\n",
    "print(\"\\n🎯 進行機率校準...\")\n",
    "\n",
    "# 選擇 XGBoost 作為基礎模型\n",
    "base_model = models['XGBoost']\n",
    "\n",
    "# Isotonic 校準\n",
    "print(\"   使用 Isotonic Regression 校準...\")\n",
    "calibrated_model = CalibratedClassifierCV(\n",
    "    base_model,\n",
    "    method='isotonic',\n",
    "    cv=3\n",
    ")\n",
    "calibrated_model.fit(X_train, y_train)\n",
    "\n",
    "# 獲取預測機率\n",
    "prob_uncalibrated = base_model.predict_proba(X_test)[:, 1]\n",
    "prob_calibrated = calibrated_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"✅ 校準完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7350d9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 6. 模型評估\n",
    "\n",
    "### 6.1 計算評估指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01c82de",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_prob, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    計算全面的評估指標\n",
    "    \"\"\"\n",
    "    # PR-AUC\n",
    "    pr_auc = average_precision_score(y_true, y_prob)\n",
    "    \n",
    "    # ROC-AUC\n",
    "    roc_auc = roc_auc_score(y_true, y_prob)\n",
    "    \n",
    "    # Brier Score\n",
    "    brier = brier_score_loss(y_true, y_prob)\n",
    "    \n",
    "    # ECE (Expected Calibration Error)\n",
    "    n_bins = 10\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_indices = np.digitize(y_prob, bin_boundaries) - 1\n",
    "    \n",
    "    ece = 0\n",
    "    for i in range(n_bins):\n",
    "        mask = bin_indices == i\n",
    "        if np.sum(mask) > 0:\n",
    "            bin_acc = np.mean(y_true[mask])\n",
    "            bin_conf = np.mean(y_prob[mask])\n",
    "            bin_size = np.sum(mask) / len(y_true)\n",
    "            ece += bin_size * np.abs(bin_acc - bin_conf)\n",
    "    \n",
    "    # Precision@K\n",
    "    k_values = [10, 20, 50]\n",
    "    precision_at_k = {}\n",
    "    sorted_indices = np.argsort(y_prob)[::-1]\n",
    "    \n",
    "    for k in k_values:\n",
    "        if k <= len(y_true):\n",
    "            top_k_true = y_true[sorted_indices[:k]]\n",
    "            precision_at_k[f'P@{k}'] = np.mean(top_k_true)\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'PR-AUC': pr_auc,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'Brier Score': brier,\n",
    "        'ECE': ece,\n",
    "        **precision_at_k\n",
    "    }\n",
    "\n",
    "# 計算所有指標\n",
    "metrics_uncalibrated = calculate_metrics(y_test, prob_uncalibrated, \"XGBoost (未校準)\")\n",
    "metrics_calibrated = calculate_metrics(y_test, prob_calibrated, \"XGBoost (已校準)\")\n",
    "\n",
    "# 顯示結果\n",
    "metrics_df = pd.DataFrame([metrics_uncalibrated, metrics_calibrated])\n",
    "print(\"\\n📊 模型評估指標:\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# 改善比較\n",
    "print(\"\\n📈 校準改善:\")\n",
    "print(f\"   ECE 改善: {(metrics_uncalibrated['ECE'] - metrics_calibrated['ECE'])/metrics_uncalibrated['ECE']*100:.1f}%\")\n",
    "print(f\"   Brier Score 改善: {(metrics_uncalibrated['Brier Score'] - metrics_calibrated['Brier Score'])/metrics_uncalibrated['Brier Score']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d80be4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 繪製 PR 曲線\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# PR 曲線\n",
    "precision_uncal, recall_uncal, _ = precision_recall_curve(y_test, prob_uncalibrated)\n",
    "precision_cal, recall_cal, _ = precision_recall_curve(y_test, prob_calibrated)\n",
    "\n",
    "axes[0].plot(recall_uncal, precision_uncal, label=f'未校準 (AP={metrics_uncalibrated[\"PR-AUC\"]:.3f})', color='red')\n",
    "axes[0].plot(recall_cal, precision_cal, label=f'已校準 (AP={metrics_calibrated[\"PR-AUC\"]:.3f})', color='green')\n",
    "axes[0].set_xlabel('Recall')\n",
    "axes[0].set_ylabel('Precision')\n",
    "axes[0].set_title('Precision-Recall 曲線', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(loc='best')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision@K 柱狀圖\n",
    "k_values = [10, 20, 50]\n",
    "precision_at_k_uncal = []\n",
    "precision_at_k_cal = []\n",
    "\n",
    "for k in k_values:\n",
    "    if f'P@{k}' in metrics_uncalibrated:\n",
    "        precision_at_k_uncal.append(metrics_uncalibrated[f'P@{k}'])\n",
    "        precision_at_k_cal.append(metrics_calibrated[f'P@{k}'])\n",
    "\n",
    "x = np.arange(len(k_values))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[1].bar(x - width/2, precision_at_k_uncal, width, label='未校準', color='red', alpha=0.7)\n",
    "bars2 = axes[1].bar(x + width/2, precision_at_k_cal, width, label='已校準', color='green', alpha=0.7)\n",
    "\n",
    "axes[1].set_xlabel('K')\n",
    "axes[1].set_ylabel('Precision@K')\n",
    "axes[1].set_title('Precision@K 比較', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels([f'Top {k}' for k in k_values])\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 添加數值標籤\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.2f}',\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbec4f4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 比較兩種方法的效能\n",
    "print(\"🔬 方法比較：合成注入 vs 監督式學習\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'metrics_supervised' in locals() and metrics_supervised is not None:\n",
    "    # 建立比較表\n",
    "    comparison_df = pd.DataFrame([\n",
    "        metrics_calibrated,  # 合成注入方法\n",
    "        metrics_supervised   # 監督式方法\n",
    "    ])\n",
    "    comparison_df['Model'] = ['合成注入', '監督式']\n",
    "\n",
    "    print(\"\\n📊 效能指標對比:\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "\n",
    "    # 視覺化比較\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "    # 指標列表\n",
    "    metrics_to_compare = ['PR-AUC', 'ROC-AUC', 'Brier Score', 'ECE', 'P@10', 'P@20']\n",
    "\n",
    "    for idx, metric in enumerate(metrics_to_compare):\n",
    "        row = idx // 3\n",
    "        col = idx % 3\n",
    "        ax = axes[row, col]\n",
    "\n",
    "        if metric in comparison_df.columns:\n",
    "            values = [\n",
    "                metrics_calibrated.get(metric, 0),\n",
    "                metrics_supervised.get(metric, 0)\n",
    "            ]\n",
    "            colors = ['blue', 'orange']\n",
    "            bars = ax.bar(['合成注入', '監督式'], values, color=colors, alpha=0.7)\n",
    "\n",
    "            # 添加數值標籤\n",
    "            for bar, val in zip(bars, values):\n",
    "                if val is not None and not pd.isna(val):\n",
    "                    ax.text(bar.get_x() + bar.get_width()/2., val,\n",
    "                           f'{val:.3f}',\n",
    "                           ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "            ax.set_title(metric, fontsize=12, fontweight='bold')\n",
    "            ax.set_ylabel('分數')\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "            # 根據指標類型設置 y 軸範圍\n",
    "            if metric in ['PR-AUC', 'ROC-AUC', 'P@10', 'P@20']:\n",
    "                ax.set_ylim([0, 1.1])\n",
    "            elif metric == 'ECE':\n",
    "                ax.set_ylim([0, 0.2])\n",
    "\n",
    "    plt.suptitle('合成注入 vs 監督式學習 效能比較', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 優劣勢分析\n",
    "    print(\"\\n💡 分析總結:\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 計算相對改善\n",
    "    pr_auc_diff = (metrics_supervised['PR-AUC'] - metrics_calibrated['PR-AUC']) / metrics_calibrated['PR-AUC'] * 100\n",
    "    ece_diff = (metrics_calibrated['ECE'] - metrics_supervised['ECE']) / metrics_calibrated['ECE'] * 100\n",
    "\n",
    "    print(\"📈 **合成注入方法**的優勢:\")\n",
    "    print(\"   • 不需要大量標註資料\")\n",
    "    print(\"   • 可以控制訓練樣本的參數分布\")\n",
    "    print(\"   • 適合快速原型開發和測試\")\n",
    "    print(f\"   • 在本實驗中 ECE: {metrics_calibrated['ECE']:.3f}\")\n",
    "\n",
    "    print(\"\\n📊 **監督式學習**的優勢:\")\n",
    "    print(\"   • 使用真實天文資料，更接近實際應用\")\n",
    "    print(\"   • 能學習到真實資料中的複雜模式\")\n",
    "    print(\"   • 對真實噪音和系統誤差有更好的魯棒性\")\n",
    "    print(f\"   • 在本實驗中 PR-AUC: {metrics_supervised['PR-AUC']:.3f}\")\n",
    "\n",
    "    if pr_auc_diff > 0:\n",
    "        print(f\"\\n🏆 監督式方法在 PR-AUC 上提升了 {pr_auc_diff:.1f}%\")\n",
    "    else:\n",
    "        print(f\"\\n🏆 合成注入方法在 PR-AUC 上領先 {-pr_auc_diff:.1f}%\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n⚠️ 無法進行比較（監督式模型未訓練）\")\n",
    "    print(\"   原因：真實資料樣本不足或無法下載\")\n",
    "    print(\"   建議：\")\n",
    "    print(\"   1. 確保已執行 01_tap_download.ipynb\")\n",
    "    print(\"   2. 檢查網路連線\")\n",
    "    print(\"   3. 增加處理的樣本數量\")\n",
    "    comparison_df = None\n",
    "\n",
    "print(\"\\n=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3267e7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 比較兩種方法的效能\n",
    "print(\"🔬 方法比較：合成注入 vs 監督式學習\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'metrics_supervised' in locals() and metrics_supervised is not None:\n",
    "    # 建立比較表\n",
    "    comparison_df = pd.DataFrame([\n",
    "        metrics_calibrated,  # 合成注入方法\n",
    "        metrics_supervised   # 監督式方法\n",
    "    ])\n",
    "    comparison_df['Model'] = ['合成注入', '監督式']\n",
    "\n",
    "    print(\"\\n📊 效能指標對比:\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "\n",
    "    # 視覺化比較\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "    # 指標列表\n",
    "    metrics_to_compare = ['PR-AUC', 'ROC-AUC', 'Brier Score', 'ECE', 'P@10', 'P@20']\n",
    "\n",
    "    for idx, metric in enumerate(metrics_to_compare):\n",
    "        row = idx // 3\n",
    "        col = idx % 3\n",
    "        ax = axes[row, col]\n",
    "\n",
    "        if metric in comparison_df.columns:\n",
    "            values = [\n",
    "                metrics_calibrated.get(metric, 0),\n",
    "                metrics_supervised.get(metric, 0)\n",
    "            ]\n",
    "            colors = ['blue', 'orange']\n",
    "            bars = ax.bar(['合成注入', '監督式'], values, color=colors, alpha=0.7)\n",
    "\n",
    "            # 添加數值標籤\n",
    "            for bar, val in zip(bars, values):\n",
    "                if val is not None and not pd.isna(val):\n",
    "                    ax.text(bar.get_x() + bar.get_width()/2., val,\n",
    "                           f'{val:.3f}',\n",
    "                           ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "            ax.set_title(metric, fontsize=12, fontweight='bold')\n",
    "            ax.set_ylabel('分數')\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "            # 根據指標類型設置 y 軸範圍\n",
    "            if metric in ['PR-AUC', 'ROC-AUC', 'P@10', 'P@20']:\n",
    "                ax.set_ylim([0, 1.1])\n",
    "            elif metric == 'ECE':\n",
    "                ax.set_ylim([0, 0.2])\n",
    "\n",
    "    plt.suptitle('合成注入 vs 監督式學習 效能比較', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 優劣勢分析\n",
    "    print(\"\\n💡 分析總結:\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 計算相對改善\n",
    "    pr_auc_diff = (metrics_supervised['PR-AUC'] - metrics_calibrated['PR-AUC']) / metrics_calibrated['PR-AUC'] * 100\n",
    "    ece_diff = (metrics_calibrated['ECE'] - metrics_supervised['ECE']) / metrics_calibrated['ECE'] * 100\n",
    "\n",
    "    print(\"📈 **合成注入方法**的優勢:\")\n",
    "    print(\"   • 不需要大量標註資料\")\n",
    "    print(\"   • 可以控制訓練樣本的參數分布\")\n",
    "    print(\"   • 適合快速原型開發和測試\")\n",
    "    print(f\"   • 在本實驗中 ECE: {metrics_calibrated['ECE']:.3f}\")\n",
    "\n",
    "    print(\"\\n📊 **監督式學習**的優勢:\")\n",
    "    print(\"   • 使用真實天文資料，更接近實際應用\")\n",
    "    print(\"   • 能學習到真實資料中的複雜模式\")\n",
    "    print(\"   • 對真實噪音和系統誤差有更好的魯棒性\")\n",
    "    print(f\"   • 在本實驗中 PR-AUC: {metrics_supervised['PR-AUC']:.3f}\")\n",
    "\n",
    "    if pr_auc_diff > 0:\n",
    "        print(f\"\\n🏆 監督式方法在 PR-AUC 上提升了 {pr_auc_diff:.1f}%\")\n",
    "    else:\n",
    "        print(f\"\\n🏆 合成注入方法在 PR-AUC 上領先 {-pr_auc_diff:.1f}%\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n⚠️ 無法進行比較（監督式模型未訓練）\")\n",
    "    print(\"   原因：真實資料樣本不足或無法下載\")\n",
    "    print(\"   建議：\")\n",
    "    print(\"   1. 確保已執行 01_tap_download.ipynb\")\n",
    "    print(\"   2. 檢查網路連線\")\n",
    "    print(\"   3. 增加處理的樣本數量\")\n",
    "    comparison_df = None\n",
    "\n",
    "print(\"\\n=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceef1323",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 📂 Data Loading (Colab-compatible)\n",
    "print(\"📂 載入真實資料集...\")\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add notebooks dir to path for data_loader\n",
    "notebooks_dir = Path('.') if Path('data_loader_colab.py').exists() else Path('../notebooks')\n",
    "if str(notebooks_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(notebooks_dir))\n",
    "\n",
    "from data_loader_colab import setup_data_directory, load_datasets\n",
    "\n",
    "# Setup and load data\n",
    "data_dir, IN_COLAB = setup_data_directory()\n",
    "datasets = load_datasets(data_dir)\n",
    "\n",
    "# Extract datasets with proper fallback\n",
    "toi_positive = pd.DataFrame()\n",
    "eb_negative = pd.DataFrame()\n",
    "toi_fp = pd.DataFrame()\n",
    "\n",
    "# Load TOI data (positive samples)\n",
    "if 'supervised_dataset' in datasets:\n",
    "    supervised_df = datasets['supervised_dataset']\n",
    "    print(f\"✅ Loaded supervised_dataset: {len(supervised_df)} rows\")\n",
    "    \n",
    "    # Extract positive samples from supervised dataset\n",
    "    if 'label' in supervised_df.columns:\n",
    "        toi_positive = supervised_df[supervised_df['label'] == 1].copy()\n",
    "        toi_positive['source'] = 'TOI'\n",
    "        print(f\"✅ TOI 正樣本: {len(toi_positive)} 個\")\n",
    "        \n",
    "        # Extract negative samples (False Positives and EBs)\n",
    "        negative_samples = supervised_df[supervised_df['label'] == 0].copy()\n",
    "        \n",
    "        # Split negatives by source if available\n",
    "        if 'source' in negative_samples.columns:\n",
    "            eb_negative = negative_samples[negative_samples['source'].str.contains('EB|Kepler', case=False, na=False)].copy()\n",
    "            toi_fp = negative_samples[negative_samples['source'].str.contains('FP|False', case=False, na=False)].copy()\n",
    "        else:\n",
    "            # If no source column, split randomly\n",
    "            split_idx = len(negative_samples) // 2\n",
    "            eb_negative = negative_samples.iloc[:split_idx].copy()\n",
    "            toi_fp = negative_samples.iloc[split_idx:].copy()\n",
    "            \n",
    "        eb_negative['source'] = 'Kepler_EB'\n",
    "        toi_fp['source'] = 'TOI_FP'\n",
    "        \n",
    "        print(f\"✅ Kepler EB 負樣本: {len(eb_negative)} 個\")\n",
    "        print(f\"✅ TOI 假陽性負樣本: {len(toi_fp)} 個\")\n",
    "else:\n",
    "    # Fallback: try loading individual files\n",
    "    if 'toi_positive' in datasets:\n",
    "        toi_positive = datasets['toi_positive'].copy()\n",
    "        toi_positive['label'] = 1\n",
    "        toi_positive['source'] = 'TOI'\n",
    "        print(f\"✅ TOI 正樣本: {len(toi_positive)} 個\")\n",
    "    \n",
    "    if 'koi_false_positives' in datasets:\n",
    "        toi_fp = datasets['koi_false_positives'].copy()\n",
    "        toi_fp['label'] = 0\n",
    "        toi_fp['source'] = 'TOI_FP'\n",
    "        print(f\"✅ TOI 假陽性負樣本: {len(toi_fp)} 個\")\n",
    "    \n",
    "    if 'toi_negative' in datasets:\n",
    "        eb_negative = datasets['toi_negative'].copy()\n",
    "        eb_negative['label'] = 0\n",
    "        eb_negative['source'] = 'Kepler_EB'\n",
    "        print(f\"✅ Kepler EB 負樣本: {len(eb_negative)} 個\")\n",
    "\n",
    "# Check if we have data\n",
    "if toi_positive.empty and eb_negative.empty and toi_fp.empty:\n",
    "    print(\"⚠️ 找不到資料！\")\n",
    "    print(\"💡 請先執行 01_tap_download.ipynb 下載資料\")\n",
    "    print(\"💡 或確保 data/ 目錄包含以下檔案:\")\n",
    "    print(\"   - supervised_dataset.csv\")\n",
    "    print(\"   - toi_positive.csv\")\n",
    "    print(\"   - toi_negative.csv\")\n",
    "    print(\"   - koi_false_positives.csv\")\n",
    "\n",
    "print(f\"\\n📊 總計:\")\n",
    "print(f\"   正樣本: {len(toi_positive)}\")\n",
    "print(f\"   負樣本: {len(eb_negative) + len(toi_fp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2575a526",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 📂 Data Loading (Colab-compatible)\n",
    "print(\"📂 載入真實資料集...\")\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add notebooks dir to path for data_loader\n",
    "notebooks_dir = Path('.') if Path('data_loader_colab.py').exists() else Path('../notebooks')\n",
    "if str(notebooks_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(notebooks_dir))\n",
    "\n",
    "from data_loader_colab import setup_data_directory, load_datasets\n",
    "\n",
    "# Setup and load data\n",
    "data_dir, IN_COLAB = setup_data_directory()\n",
    "datasets = load_datasets(data_dir)\n",
    "\n",
    "# Extract datasets with proper fallback\n",
    "toi_positive = pd.DataFrame()\n",
    "eb_negative = pd.DataFrame()\n",
    "toi_fp = pd.DataFrame()\n",
    "\n",
    "# Load TOI data (positive samples)\n",
    "if 'supervised_dataset' in datasets:\n",
    "    supervised_df = datasets['supervised_dataset']\n",
    "    print(f\"✅ Loaded supervised_dataset: {len(supervised_df)} rows\")\n",
    "    \n",
    "    # Extract positive samples from supervised dataset\n",
    "    if 'label' in supervised_df.columns:\n",
    "        toi_positive = supervised_df[supervised_df['label'] == 1].copy()\n",
    "        toi_positive['source'] = 'TOI'\n",
    "        print(f\"✅ TOI 正樣本: {len(toi_positive)} 個\")\n",
    "        \n",
    "        # Extract negative samples (False Positives and EBs)\n",
    "        negative_samples = supervised_df[supervised_df['label'] == 0].copy()\n",
    "        \n",
    "        # Split negatives by source if available\n",
    "        if 'source' in negative_samples.columns:\n",
    "            eb_negative = negative_samples[negative_samples['source'].str.contains('EB|Kepler', case=False, na=False)].copy()\n",
    "            toi_fp = negative_samples[negative_samples['source'].str.contains('FP|False', case=False, na=False)].copy()\n",
    "        else:\n",
    "            # If no source column, split randomly\n",
    "            split_idx = len(negative_samples) // 2\n",
    "            eb_negative = negative_samples.iloc[:split_idx].copy()\n",
    "            toi_fp = negative_samples.iloc[split_idx:].copy()\n",
    "            \n",
    "        eb_negative['source'] = 'Kepler_EB'\n",
    "        toi_fp['source'] = 'TOI_FP'\n",
    "        \n",
    "        print(f\"✅ Kepler EB 負樣本: {len(eb_negative)} 個\")\n",
    "        print(f\"✅ TOI 假陽性負樣本: {len(toi_fp)} 個\")\n",
    "else:\n",
    "    # Fallback: try loading individual files\n",
    "    if 'toi_positive' in datasets:\n",
    "        toi_positive = datasets['toi_positive'].copy()\n",
    "        toi_positive['label'] = 1\n",
    "        toi_positive['source'] = 'TOI'\n",
    "        print(f\"✅ TOI 正樣本: {len(toi_positive)} 個\")\n",
    "    \n",
    "    if 'koi_false_positives' in datasets:\n",
    "        toi_fp = datasets['koi_false_positives'].copy()\n",
    "        toi_fp['label'] = 0\n",
    "        toi_fp['source'] = 'TOI_FP'\n",
    "        print(f\"✅ TOI 假陽性負樣本: {len(toi_fp)} 個\")\n",
    "    \n",
    "    if 'toi_negative' in datasets:\n",
    "        eb_negative = datasets['toi_negative'].copy()\n",
    "        eb_negative['label'] = 0\n",
    "        eb_negative['source'] = 'Kepler_EB'\n",
    "        print(f\"✅ Kepler EB 負樣本: {len(eb_negative)} 個\")\n",
    "\n",
    "# Check if we have data\n",
    "if toi_positive.empty and eb_negative.empty and toi_fp.empty:\n",
    "    print(\"⚠️ 找不到資料！\")\n",
    "    print(\"💡 請先執行 01_tap_download.ipynb 下載資料\")\n",
    "    print(\"💡 或確保 data/ 目錄包含以下檔案:\")\n",
    "    print(\"   - supervised_dataset.csv\")\n",
    "    print(\"   - toi_positive.csv\")\n",
    "    print(\"   - toi_negative.csv\")\n",
    "    print(\"   - koi_false_positives.csv\")\n",
    "\n",
    "print(f\"\\n📊 總計:\")\n",
    "print(f\"   正樣本: {len(toi_positive)}\")\n",
    "print(f\"   負樣本: {len(eb_negative) + len(toi_fp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772394fe",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 視覺化參數分布\n",
    "positive_labels = labels_df[labels_df['label'] == 1]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# 週期分布\n",
    "axes[0, 0].hist(positive_labels['period'], bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('週期 (天)')\n",
    "axes[0, 0].set_ylabel('數量')\n",
    "axes[0, 0].set_title('週期分布')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 深度分布\n",
    "axes[0, 1].hist(positive_labels['depth'] * 1e6, bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[0, 1].set_xlabel('深度 (ppm)')\n",
    "axes[0, 1].set_ylabel('數量')\n",
    "axes[0, 1].set_title('凌日深度分布')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 持續時間分布\n",
    "axes[1, 0].hist(positive_labels['duration'] * 24, bins=30, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[1, 0].set_xlabel('持續時間 (小時)')\n",
    "axes[1, 0].set_ylabel('數量')\n",
    "axes[1, 0].set_title('凌日持續時間分布')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# SNR 分布\n",
    "axes[1, 1].hist(positive_labels['snr_estimate'], bins=30, edgecolor='black', alpha=0.7, color='red')\n",
    "axes[1, 1].set_xlabel('SNR 估計')\n",
    "axes[1, 1].set_ylabel('數量')\n",
    "axes[1, 1].set_title('信噪比分布')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('合成凌日參數分布', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 統計摘要\n",
    "print(\"\\n📊 參數統計摘要：\")\n",
    "print(positive_labels[['period', 'depth', 'duration', 'snr_estimate']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07871087",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 訓練多個模型\n",
    "models = {}\n",
    "print(\"🚀 開始訓練模型...\\n\")\n",
    "\n",
    "# 1. Logistic Regression\n",
    "print(\"1️⃣ 訓練 Logistic Regression...\")\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "models['LogisticRegression'] = lr_model\n",
    "print(f\"   訓練分數: {lr_model.score(X_train_scaled, y_train):.3f}\")\n",
    "print(f\"   測試分數: {lr_model.score(X_test_scaled, y_test):.3f}\")\n",
    "\n",
    "# 2. Random Forest\n",
    "print(\"\\n2️⃣ 訓練 Random Forest...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)  # Random Forest 不需要標準化\n",
    "models['RandomForest'] = rf_model\n",
    "print(f\"   訓練分數: {rf_model.score(X_train, y_train):.3f}\")\n",
    "print(f\"   測試分數: {rf_model.score(X_test, y_test):.3f}\")\n",
    "\n",
    "# 3. XGBoost\n",
    "print(\"\\n3️⃣ 訓練 XGBoost...\")\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "models['XGBoost'] = xgb_model\n",
    "print(f\"   訓練分數: {xgb_model.score(X_train, y_train):.3f}\")\n",
    "print(f\"   測試分數: {xgb_model.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dd9309",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 🚀 GitHub Push 終極解決方案 (03 - Synthetic Injection & Supervised Training Results)\n",
    "# 一鍵推送合成注入與監督式訓練結果至 GitHub\n",
    "\n",
    "import subprocess, os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def ultimate_push_to_github_03(token=None):\n",
    "    \"\"\"\n",
    "    終極一鍵推送解決方案 - 合成注入與監督式訓練結果版\n",
    "    解決所有 Colab 與本地環境的 Git/LFS 問題\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"🚀 合成注入與監督式訓練結果 GitHub 推送開始...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 步驟 1: 環境偵測與設定\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        IN_COLAB = True\n",
    "        working_dir = \"/content\"\n",
    "        print(\"🌍 偵測到 Google Colab 環境\")\n",
    "    except ImportError:\n",
    "        IN_COLAB = False\n",
    "        working_dir = os.getcwd()\n",
    "        print(\"💻 偵測到本地環境\")\n",
    "\n",
    "    # 步驟 2: Token 輸入\n",
    "    if not token:\n",
    "        print(\"📋 請輸入 GitHub Personal Access Token:\")\n",
    "        print(\"   1. 前往 https://github.com/settings/tokens\")\n",
    "        print(\"   2. 點擊 'Generate new token (classic)'\")\n",
    "        print(\"   3. 勾選 'repo' 權限\")\n",
    "        print(\"   4. 複製生成的 token\")\n",
    "        token = input(\"🔐 貼上你的 token (ghp_...): \").strip()\n",
    "        if not token.startswith('ghp_'):\n",
    "            print(\"❌ Token 格式錯誤，應該以 'ghp_' 開頭\")\n",
    "            return False\n",
    "\n",
    "    # 步驟 3: Git 倉庫初始化與設定\n",
    "    print(\"\\n📋 步驟 1/4: Git 倉庫設定...\")\n",
    "\n",
    "    try:\n",
    "        # 切換到工作目錄\n",
    "        if IN_COLAB:\n",
    "            os.chdir(working_dir)\n",
    "\n",
    "        # 檢查是否已是 Git 倉庫\n",
    "        git_check = subprocess.run(['git', 'rev-parse', '--git-dir'],\n",
    "                                   capture_output=True, text=True)\n",
    "\n",
    "        if git_check.returncode != 0:\n",
    "            print(\"   🔧 初始化 Git 倉庫...\")\n",
    "            subprocess.run(['git', 'init'], check=True)\n",
    "            print(\"   ✅ Git 倉庫初始化完成\")\n",
    "        else:\n",
    "            print(\"   ✅ 已在 Git 倉庫中\")\n",
    "\n",
    "        # 設定 Git 用戶（如果未設定）\n",
    "        try:\n",
    "            subprocess.run(['git', 'config', 'user.name', 'Colab User'], check=True)\n",
    "            subprocess.run(['git', 'config', 'user.email', 'colab@spaceapps.com'], check=True)\n",
    "            print(\"   ✅ Git 用戶設定完成\")\n",
    "        except:\n",
    "            print(\"   ⚠️ Git 用戶設定跳過\")\n",
    "\n",
    "        # 設定遠端倉庫（自動偵測或使用預設）\n",
    "        try:\n",
    "            remote_check = subprocess.run(['git', 'remote', 'get-url', 'origin'],\n",
    "                                        capture_output=True, text=True)\n",
    "            if remote_check.returncode != 0:\n",
    "                print(\"   🔧 設定遠端倉庫...\")\n",
    "                # 使用預設倉庫 URL（用戶需要修改為自己的倉庫）\n",
    "                default_repo = \"https://github.com/exoplanet-spaceapps/exoplanet-starter.git\"\n",
    "                subprocess.run(['git', 'remote', 'add', 'origin', default_repo], check=True)\n",
    "                print(f\"   ✅ 遠端倉庫設定: {default_repo}\")\n",
    "                print(\"   💡 請確保你有該倉庫的寫入權限，或修改為你的倉庫\")\n",
    "            else:\n",
    "                print(f\"   ✅ 遠端倉庫已設定: {remote_check.stdout.strip()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ 遠端倉庫設定警告: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Git 設定失敗: {e}\")\n",
    "        return False\n",
    "\n",
    "    # 步驟 4: Git LFS 設定\n",
    "    print(\"\\n📋 步驟 2/4: Git LFS 設定...\")\n",
    "\n",
    "    try:\n",
    "        # 安裝 Git LFS（Colab）\n",
    "        if IN_COLAB:\n",
    "            print(\"   📦 在 Colab 中安裝 Git LFS...\")\n",
    "            subprocess.run(['apt-get', 'update', '-qq'], check=True)\n",
    "            subprocess.run(['apt-get', 'install', '-y', '-qq', 'git-lfs'], check=True)\n",
    "            print(\"   ✅ Git LFS 已安裝\")\n",
    "\n",
    "        # 初始化 LFS\n",
    "        try:\n",
    "            subprocess.run(['git', 'lfs', 'install'], check=True)\n",
    "            print(\"   ✅ Git LFS 初始化完成\")\n",
    "        except:\n",
    "            print(\"   ⚠️ Git LFS 初始化跳過（可能已設定）\")\n",
    "\n",
    "        # 設定 LFS 追蹤（容錯處理）\n",
    "        lfs_patterns = ['*.csv', '*.json', '*.pkl', '*.parquet', '*.h5', '*.hdf5', '*.joblib']\n",
    "        for pattern in lfs_patterns:\n",
    "            try:\n",
    "                result = subprocess.run(['git', 'lfs', 'track', pattern],\n",
    "                                      capture_output=True, text=True)\n",
    "                if result.returncode == 0:\n",
    "                    print(f\"   📦 LFS 追蹤: {pattern}\")\n",
    "                else:\n",
    "                    print(f\"   ⚠️ LFS 追蹤 {pattern} 警告: {result.stderr.strip()}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️ LFS 追蹤 {pattern} 跳過: {e}\")\n",
    "\n",
    "        # 添加 .gitattributes 到 staging\n",
    "        try:\n",
    "            subprocess.run(['git', 'add', '.gitattributes'], check=False)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ Git LFS 設定警告: {e}\")\n",
    "        print(\"   💡 繼續執行，但大檔案可能無法正確追蹤\")\n",
    "\n",
    "    # 步驟 5: 添加檔案並提交\n",
    "    print(\"\\n📋 步驟 3/4: 添加檔案與提交...\")\n",
    "\n",
    "    try:\n",
    "        # 確保重要目錄存在\n",
    "        important_dirs = ['data', 'notebooks', 'app', 'scripts', 'model']\n",
    "        for dir_name in important_dirs:\n",
    "            dir_path = Path(dir_name)\n",
    "            if dir_path.exists():\n",
    "                print(f\"   📂 找到目錄: {dir_name}\")\n",
    "            elif IN_COLAB and dir_name in ['data', 'model']:\n",
    "                # 在 Colab 中創建相關目錄\n",
    "                dir_path.mkdir(parents=True, exist_ok=True)\n",
    "                print(f\"   📂 創建目錄: {dir_name}\")\n",
    "\n",
    "        # 添加所有檔案\n",
    "        subprocess.run(['git', 'add', '.'], check=True)\n",
    "        print(\"   ✅ 檔案添加完成\")\n",
    "\n",
    "        # 檢查是否有變更\n",
    "        status_result = subprocess.run(['git', 'status', '--porcelain'],\n",
    "                                      capture_output=True, text=True, check=True)\n",
    "\n",
    "        if not status_result.stdout.strip():\n",
    "            print(\"   ✅ 沒有新的變更需要提交\")\n",
    "            return True\n",
    "\n",
    "        # 創建提交\n",
    "        commit_message = \"\"\"feat: complete synthetic injection & supervised training pipeline\n",
    "\n",
    "- 🧪 完成合成凌日注入資料生成 (200 正類 + 200 負類)\n",
    "- 🔍 實作 BLS/TLS 特徵提取與重要性分析\n",
    "- 🤖 訓練多個模型: LogisticRegression, RandomForest, XGBoost\n",
    "- 📊 實現 Isotonic 機率校準 (ECE, Brier Score, 可靠度曲線)\n",
    "- 📈 監督式學習: 真實 TOI + Kepler EB 資料訓練\n",
    "- 📋 方法比較: 合成注入 vs 監督式學習效能對比\n",
    "- 💾 模型持久化: model/ranker.joblib + feature_schema.json\n",
    "- 📊 完整評估指標: PR-AUC, ROC-AUC, Precision@K, ECE\n",
    "\n",
    "Co-Authored-By: hctsai1006 <39769660@cuni.cz>\n",
    "        \"\"\"\n",
    "\n",
    "        subprocess.run(['git', 'commit', '-m', commit_message], check=True)\n",
    "        print(\"   ✅ 提交完成\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"   ❌ 檔案提交失敗: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ 檔案處理失敗: {e}\")\n",
    "        return False\n",
    "\n",
    "    # 步驟 6: 推送到 GitHub\n",
    "    print(\"\\n📋 步驟 4/4: 推送到 GitHub...\")\n",
    "\n",
    "    try:\n",
    "        # 獲取遠端 URL 並插入 token\n",
    "        remote_result = subprocess.run(['git', 'remote', 'get-url', 'origin'],\n",
    "                                      capture_output=True, text=True, check=True)\n",
    "        remote_url = remote_result.stdout.strip()\n",
    "\n",
    "        # 構造帶 token 的 URL\n",
    "        if remote_url.startswith('https://github.com/'):\n",
    "            # 提取倉庫路徑\n",
    "            repo_path = remote_url.replace('https://github.com/', '').replace('.git', '')\n",
    "            auth_url = f\"https://{token}@github.com/{repo_path}.git\"\n",
    "        else:\n",
    "            print(f\"   ⚠️ 遠端 URL 格式異常: {remote_url}\")\n",
    "            auth_url = remote_url\n",
    "\n",
    "        # 推送\n",
    "        push_result = subprocess.run([\n",
    "            'git', 'push', auth_url, 'main'\n",
    "        ], capture_output=True, text=True, timeout=300)\n",
    "\n",
    "        if push_result.returncode == 0:\n",
    "            print(\"   ✅ 推送成功！\")\n",
    "            print(f\"   📡 推送輸出: {push_result.stdout[:200]}...\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"   ❌ 推送失敗: {push_result.stderr}\")\n",
    "            # 嘗試推送到其他分支\n",
    "            try:\n",
    "                alt_push = subprocess.run([\n",
    "                    'git', 'push', auth_url, 'HEAD:main'\n",
    "                ], capture_output=True, text=True, timeout=300)\n",
    "                if alt_push.returncode == 0:\n",
    "                    print(\"   ✅ 備用推送成功！\")\n",
    "                    return True\n",
    "            except:\n",
    "                pass\n",
    "            return False\n",
    "\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"   ❌ 推送超時，請檢查網路連接\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ 推送失敗: {e}\")\n",
    "        return False\n",
    "\n",
    "    finally:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"📋 合成注入與監督式訓練結果推送完成!\")\n",
    "        if IN_COLAB:\n",
    "            print(\"💡 如果遇到問題:\")\n",
    "            print(\"   1. 確保 token 有 'repo' 權限\")\n",
    "            print(\"   2. 確保你有目標倉庫的寫入權限\")\n",
    "            print(\"   3. 檢查倉庫 URL 是否正確\")\n",
    "\n",
    "# 呼叫函數（請在執行時提供 token）\n",
    "print(\"🔐 準備推送合成注入與監督式訓練結果...\")\n",
    "print(\"💡 執行方式: ultimate_push_to_github_03(token='你的GitHub_token')\")\n",
    "print(\"📝 或直接執行下方 cell 並在提示時輸入 token\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b19c5b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 建立並儲存特徵架構\n",
    "feature_schema = create_feature_schema(\n",
    "    feature_cols,\n",
    "    output_path=\"data/feature_schema.json\"\n",
    ")\n",
    "\n",
    "print(\"📝 特徵架構已建立\")\n",
    "print(f\"   特徵數量: {feature_schema['n_features']}\")\n",
    "print(f\"   版本: {feature_schema['version']}\")\n",
    "print(f\"   儲存位置: data/feature_schema.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39dcb02",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 準備訓練資料\n",
    "X = features_df[feature_cols].values\n",
    "y = features_df['label'].values\n",
    "\n",
    "# 處理無效值\n",
    "X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# 分割訓練集和測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 標準化特徵\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"📊 資料集統計:\")\n",
    "print(f\"   訓練集: {len(X_train)} 樣本\")\n",
    "print(f\"   測試集: {len(X_test)} 樣本\")\n",
    "print(f\"   正樣本比例 (訓練): {y_train.mean():.2%}\")\n",
    "print(f\"   正樣本比例 (測試): {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd75311f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 從真實光曲線提取特徵\n",
    "if 'supervised_samples_df' in locals() and len(supervised_samples_df) > 0:\n",
    "    print(\"🔍 提取真實資料特徵...\")\n",
    "\n",
    "    supervised_features = []\n",
    "\n",
    "    for idx, row in supervised_samples_df.iterrows():\n",
    "        # 執行 BLS\n",
    "        bls_result = run_bls(row['time'], row['flux'])\n",
    "\n",
    "        # 提取特徵\n",
    "        features = extract_features(row['time'], row['flux'], bls_result, compute_advanced=True)\n",
    "        features['sample_id'] = row['sample_id']\n",
    "        features['label'] = row['label']\n",
    "        features['source'] = row['source']\n",
    "        features['true_period'] = row['period']\n",
    "\n",
    "        supervised_features.append(features)\n",
    "\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"   處理進度: {idx+1}/{len(supervised_samples_df)}\")\n",
    "\n",
    "    supervised_features_df = pd.DataFrame(supervised_features)\n",
    "    print(f\"\\n✅ 特徵提取完成: {len(supervised_features_df)} 個樣本\")\n",
    "\n",
    "    # 顯示特徵統計\n",
    "    print(\"\\n📊 真實資料特徵統計:\")\n",
    "    feature_cols_real = [col for col in supervised_features_df.columns\n",
    "                         if col not in ['sample_id', 'label', 'source', 'true_period']]\n",
    "    print(supervised_features_df[feature_cols_real].describe())\n",
    "else:\n",
    "    print(\"⚠️ 無真實樣本可用於監督式學習\")\n",
    "    supervised_features_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04cdc9b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 從真實光曲線提取特徵\n",
    "if 'supervised_samples_df' in locals() and len(supervised_samples_df) > 0:\n",
    "    print(\"🔍 提取真實資料特徵...\")\n",
    "\n",
    "    supervised_features = []\n",
    "\n",
    "    for idx, row in supervised_samples_df.iterrows():\n",
    "        # 執行 BLS\n",
    "        bls_result = run_bls(row['time'], row['flux'])\n",
    "\n",
    "        # 提取特徵\n",
    "        features = extract_features(row['time'], row['flux'], bls_result, compute_advanced=True)\n",
    "        features['sample_id'] = row['sample_id']\n",
    "        features['label'] = row['label']\n",
    "        features['source'] = row['source']\n",
    "        features['true_period'] = row['period']\n",
    "\n",
    "        supervised_features.append(features)\n",
    "\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"   處理進度: {idx+1}/{len(supervised_samples_df)}\")\n",
    "\n",
    "    supervised_features_df = pd.DataFrame(supervised_features)\n",
    "    print(f\"\\n✅ 特徵提取完成: {len(supervised_features_df)} 個樣本\")\n",
    "\n",
    "    # 顯示特徵統計\n",
    "    print(\"\\n📊 真實資料特徵統計:\")\n",
    "    feature_cols_real = [col for col in supervised_features_df.columns\n",
    "                         if col not in ['sample_id', 'label', 'source', 'true_period']]\n",
    "    print(supervised_features_df[feature_cols_real].describe())\n",
    "else:\n",
    "    print(\"⚠️ 無真實樣本可用於監督式學習\")\n",
    "    supervised_features_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40499380",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 計算特徵重要性\n",
    "print(\"🎯 計算特徵重要性...\")\n",
    "\n",
    "importance_df = compute_feature_importance(\n",
    "    features_df,\n",
    "    features_df['label'].values,\n",
    "    method=\"random_forest\"\n",
    ")\n",
    "\n",
    "# 視覺化特徵重要性\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "top_features = importance_df.head(10)\n",
    "bars = ax.barh(range(len(top_features)), top_features['importance'].values)\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['feature'].values)\n",
    "ax.set_xlabel('重要性分數')\n",
    "ax.set_title('特徵重要性排名 (Top 10)', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 添加數值標籤\n",
    "for i, (bar, val) in enumerate(zip(bars, top_features['importance'].values)):\n",
    "    ax.text(val, bar.get_y() + bar.get_height()/2, f'{val:.3f}', \n",
    "            ha='left', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🏆 Top 5 最重要特徵:\")\n",
    "for idx, row in importance_df.head(5).iterrows():\n",
    "    print(f\"   {idx+1}. {row['feature']}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdad6404",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 🌍 Environment Detection\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = 'google.colab' in sys.modules or '/content' in os.getcwd()\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"🌍 Running in: Google Colab\")\n",
    "    \n",
    "    # Clone repo if needed\n",
    "    project_dir = Path('/content/exoplanet-starter')\n",
    "    if not project_dir.exists():\n",
    "        print(\"📥 Cloning repository...\")\n",
    "        !git clone https://github.com/exoplanet-spaceapps/exoplanet-starter.git\n",
    "        print(\"✅ Repository cloned\")\n",
    "    \n",
    "    # Change to project directory\n",
    "    os.chdir(str(project_dir))\n",
    "    \n",
    "    # Add to Python path\n",
    "    sys.path.insert(0, str(project_dir))\n",
    "    sys.path.insert(0, str(project_dir / 'src'))\n",
    "    sys.path.insert(0, str(project_dir / 'notebooks'))\n",
    "    \n",
    "    print(f\"📂 Working directory: {os.getcwd()}\")\n",
    "    print(f\"✅ Python path configured\")\n",
    "    \n",
    "else:\n",
    "    print(\"💻 Running in: Local environment\")\n",
    "    # Local paths\n",
    "    project_dir = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "    sys.path.insert(0, str(project_dir / 'src'))\n",
    "    sys.path.insert(0, str(project_dir))\n",
    "\n",
    "print(f\"📍 Project directory: {project_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f78b8c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 下載真實光曲線作為基礎\n",
    "print(\"📡 下載基礎光曲線...\")\n",
    "\n",
    "try:\n",
    "    # 使用 TIC 25155310 (TOI-431) 作為基礎\n",
    "    target = \"TIC 25155310\"\n",
    "    search_result = lk.search_lightcurve(target, mission=\"TESS\", author=\"SPOC\")\n",
    "    lc = search_result[0].download()\n",
    "    \n",
    "    # 清理和去趨勢\n",
    "    lc_clean = lc.remove_nans()\n",
    "    lc_flat = lc_clean.flatten(window_length=401)\n",
    "    \n",
    "    base_time = lc_flat.time.value\n",
    "    base_flux = lc_flat.flux.value\n",
    "    \n",
    "    print(f\"✅ 成功下載 {target}\")\n",
    "    print(f\"   資料點數: {len(base_time)}\")\n",
    "    print(f\"   時間跨度: {base_time[-1] - base_time[0]:.1f} 天\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ 無法下載真實光曲線: {e}\")\n",
    "    print(\"   使用模擬光曲線...\")\n",
    "    \n",
    "    # 生成模擬光曲線（27天 TESS 觀測）\n",
    "    base_time = np.linspace(0, 27, 20000)\n",
    "    base_flux = np.ones(20000) + np.random.normal(0, 0.0001, 20000)\n",
    "    \n",
    "    print(f\"✅ 生成模擬光曲線\")\n",
    "    print(f\"   資料點數: {len(base_time)}\")\n",
    "    print(f\"   時間跨度: {base_time[-1] - base_time[0]:.1f} 天\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f9f69d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 生成合成資料集\n",
    "print(\"\\n🔨 生成合成資料集...\")\n",
    "print(\"   參數範圍：\")\n",
    "print(\"   • 週期: 0.6 - 10.0 天\")\n",
    "print(\"   • 深度: 0.0005 - 0.02 (500 - 20000 ppm)\")\n",
    "print(\"   • 持續時間: 週期的 2% - 10%\")\n",
    "\n",
    "samples_df, labels_df = generate_synthetic_dataset(\n",
    "    base_time=base_time,\n",
    "    base_flux=base_flux,\n",
    "    n_positive=200,\n",
    "    n_negative=200,\n",
    "    period_range=(0.6, 10.0),\n",
    "    depth_range=(0.0005, 0.02),\n",
    "    duration_fraction_range=(0.02, 0.1),\n",
    "    noise_level=0.0001,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ 生成 {len(samples_df)} 個樣本\")\n",
    "print(f\"   正樣本（有凌日）: {len(samples_df[samples_df['label'] == 1])}\")\n",
    "print(f\"   負樣本（無凌日）: {len(samples_df[samples_df['label'] == 0])}\")\n",
    "\n",
    "# 儲存資料集\n",
    "dataset_paths = save_synthetic_dataset(\n",
    "    samples_df,\n",
    "    labels_df,\n",
    "    output_dir=\"data/synthetic\",\n",
    "    format=\"parquet\"\n",
    ")\n",
    "\n",
    "print(f\"\\n💾 資料集已儲存至:\")\n",
    "for key, path in dataset_paths.items():\n",
    "    print(f\"   {key}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ced4b7f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_and_process_lightcurve(target_id, mission=\"TESS\"):\n",
    "    \"\"\"\n",
    "    下載並處理單個目標的光曲線\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    target_id : str\n",
    "        目標識別碼（TIC 或 KIC）\n",
    "    mission : str\n",
    "        任務名稱（TESS 或 Kepler）\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (time, flux) 或 (None, None) 如果失敗\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 搜尋光曲線\n",
    "        search_result = lk.search_lightcurve(target_id, mission=mission, author=\"SPOC\" if mission==\"TESS\" else \"Kepler\")\n",
    "        if len(search_result) == 0:\n",
    "            return None, None\n",
    "\n",
    "        # 下載第一個結果\n",
    "        lc = search_result[0].download()\n",
    "\n",
    "        # 清理和去趨勢\n",
    "        lc_clean = lc.remove_nans()\n",
    "        if len(lc_clean) < 100:  # 太少資料點\n",
    "            return None, None\n",
    "\n",
    "        lc_flat = lc_clean.flatten(window_length=401)\n",
    "\n",
    "        return lc_flat.time.value, lc_flat.flux.value\n",
    "\n",
    "    except Exception as e:\n",
    "        return None, None\n",
    "\n",
    "# 示範：處理部分真實樣本\n",
    "print(\"🔬 處理真實光曲線樣本...\")\n",
    "print(\"   （為節省時間，僅處理前 50 個樣本）\")\n",
    "\n",
    "supervised_samples = []\n",
    "supervised_labels = []\n",
    "\n",
    "# 處理 TOI 正樣本（最多 25 個）\n",
    "n_toi_samples = min(25, len(toi_positive)) if 'toi_positive' in locals() else 0\n",
    "if n_toi_samples > 0:\n",
    "    print(f\"\\n處理 {n_toi_samples} 個 TOI 正樣本...\")\n",
    "    for idx, row in toi_positive.head(n_toi_samples).iterrows():\n",
    "        tic_id = f\"TIC {int(row['tid'])}\"\n",
    "        time_data, flux_data = download_and_process_lightcurve(tic_id, \"TESS\")\n",
    "\n",
    "        if time_data is not None:\n",
    "            supervised_samples.append({\n",
    "                'sample_id': f\"toi_{row['tid']}\",\n",
    "                'time': time_data,\n",
    "                'flux': flux_data,\n",
    "                'label': 1,\n",
    "                'period': row.get('pl_orbper', np.nan),\n",
    "                'source': 'TOI'\n",
    "            })\n",
    "            print(f\"   ✓ {tic_id}\")\n",
    "        else:\n",
    "            print(f\"   ✗ {tic_id} (無法下載)\")\n",
    "\n",
    "# 處理 Kepler EB 負樣本（最多 25 個）\n",
    "n_eb_samples = min(25, len(eb_negative)) if 'eb_negative' in locals() else 0\n",
    "if n_eb_samples > 0:\n",
    "    print(f\"\\n處理 {n_eb_samples} 個 Kepler EB 負樣本...\")\n",
    "    for idx, row in eb_negative.head(n_eb_samples).iterrows():\n",
    "        kic_id = f\"KIC {int(row['KIC'])}\"\n",
    "        time_data, flux_data = download_and_process_lightcurve(kic_id, \"Kepler\")\n",
    "\n",
    "        if time_data is not None:\n",
    "            supervised_samples.append({\n",
    "                'sample_id': f\"eb_{row['KIC']}\",\n",
    "                'time': time_data,\n",
    "                'flux': flux_data,\n",
    "                'label': 0,\n",
    "                'period': row.get('period', np.nan),\n",
    "                'source': 'Kepler_EB'\n",
    "            })\n",
    "            print(f\"   ✓ {kic_id}\")\n",
    "        else:\n",
    "            print(f\"   ✗ {kic_id} (無法下載)\")\n",
    "\n",
    "supervised_samples_df = pd.DataFrame(supervised_samples) if supervised_samples else pd.DataFrame()\n",
    "print(f\"\\n✅ 成功處理 {len(supervised_samples_df)} 個真實樣本\")\n",
    "if len(supervised_samples_df) > 0:\n",
    "    print(f\"   正樣本: {len(supervised_samples_df[supervised_samples_df['label']==1])}\")\n",
    "    print(f\"   負樣本: {len(supervised_samples_df[supervised_samples_df['label']==0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde0ed66",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_and_process_lightcurve(target_id, mission=\"TESS\"):\n",
    "    \"\"\"\n",
    "    下載並處理單個目標的光曲線\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    target_id : str\n",
    "        目標識別碼（TIC 或 KIC）\n",
    "    mission : str\n",
    "        任務名稱（TESS 或 Kepler）\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (time, flux) 或 (None, None) 如果失敗\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 搜尋光曲線\n",
    "        search_result = lk.search_lightcurve(target_id, mission=mission, author=\"SPOC\" if mission==\"TESS\" else \"Kepler\")\n",
    "        if len(search_result) == 0:\n",
    "            return None, None\n",
    "\n",
    "        # 下載第一個結果\n",
    "        lc = search_result[0].download()\n",
    "\n",
    "        # 清理和去趨勢\n",
    "        lc_clean = lc.remove_nans()\n",
    "        if len(lc_clean) < 100:  # 太少資料點\n",
    "            return None, None\n",
    "\n",
    "        lc_flat = lc_clean.flatten(window_length=401)\n",
    "\n",
    "        return lc_flat.time.value, lc_flat.flux.value\n",
    "\n",
    "    except Exception as e:\n",
    "        return None, None\n",
    "\n",
    "# 示範：處理部分真實樣本\n",
    "print(\"🔬 處理真實光曲線樣本...\")\n",
    "print(\"   （為節省時間，僅處理前 50 個樣本）\")\n",
    "\n",
    "supervised_samples = []\n",
    "supervised_labels = []\n",
    "\n",
    "# 處理 TOI 正樣本（最多 25 個）\n",
    "n_toi_samples = min(25, len(toi_positive)) if 'toi_positive' in locals() else 0\n",
    "if n_toi_samples > 0:\n",
    "    print(f\"\\n處理 {n_toi_samples} 個 TOI 正樣本...\")\n",
    "    for idx, row in toi_positive.head(n_toi_samples).iterrows():\n",
    "        tic_id = f\"TIC {int(row['tid'])}\"\n",
    "        time_data, flux_data = download_and_process_lightcurve(tic_id, \"TESS\")\n",
    "\n",
    "        if time_data is not None:\n",
    "            supervised_samples.append({\n",
    "                'sample_id': f\"toi_{row['tid']}\",\n",
    "                'time': time_data,\n",
    "                'flux': flux_data,\n",
    "                'label': 1,\n",
    "                'period': row.get('pl_orbper', np.nan),\n",
    "                'source': 'TOI'\n",
    "            })\n",
    "            print(f\"   ✓ {tic_id}\")\n",
    "        else:\n",
    "            print(f\"   ✗ {tic_id} (無法下載)\")\n",
    "\n",
    "# 處理 Kepler EB 負樣本（最多 25 個）\n",
    "n_eb_samples = min(25, len(eb_negative)) if 'eb_negative' in locals() else 0\n",
    "if n_eb_samples > 0:\n",
    "    print(f\"\\n處理 {n_eb_samples} 個 Kepler EB 負樣本...\")\n",
    "    for idx, row in eb_negative.head(n_eb_samples).iterrows():\n",
    "        kic_id = f\"KIC {int(row['KIC'])}\"\n",
    "        time_data, flux_data = download_and_process_lightcurve(kic_id, \"Kepler\")\n",
    "\n",
    "        if time_data is not None:\n",
    "            supervised_samples.append({\n",
    "                'sample_id': f\"eb_{row['KIC']}\",\n",
    "                'time': time_data,\n",
    "                'flux': flux_data,\n",
    "                'label': 0,\n",
    "                'period': row.get('period', np.nan),\n",
    "                'source': 'Kepler_EB'\n",
    "            })\n",
    "            print(f\"   ✓ {kic_id}\")\n",
    "        else:\n",
    "            print(f\"   ✗ {kic_id} (無法下載)\")\n",
    "\n",
    "supervised_samples_df = pd.DataFrame(supervised_samples) if supervised_samples else pd.DataFrame()\n",
    "print(f\"\\n✅ 成功處理 {len(supervised_samples_df)} 個真實樣本\")\n",
    "if len(supervised_samples_df) > 0:\n",
    "    print(f\"   正樣本: {len(supervised_samples_df[supervised_samples_df['label']==1])}\")\n",
    "    print(f\"   負樣本: {len(supervised_samples_df[supervised_samples_df['label']==0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c769e7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 🚀 執行 GitHub Push (03 - 合成注入與監督式訓練)\n",
    "# 取消註解下面這行來執行推送:\n",
    "# ultimate_push_to_github_03()\n",
    "\n",
    "print(\"📋 合成注入與監督式訓練管線完成！\")\n",
    "print(\"💡 請在需要推送結果時執行上面的 ultimate_push_to_github_03() 函數\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085504e6",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [2]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bcf7ee",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1576ca6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 8. Probability Calibration Comparison\n",
    "\n",
    "比較兩種機率校準方法：**Isotonic Regression** 與 **Platt Scaling (Sigmoid)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aae0483",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1. 環境設定與依賴安裝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df74fd4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 2. 導入套件與模組"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42deb411",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3. 資料生成：合成凌日注入\n",
    "\n",
    "### 3.1 下載基礎光曲線"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc56817",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 3.2 生成合成資料集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d62b83b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 3.3 參數分布視覺化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa69666",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 5.3 機率校準"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e21bd4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 6.2 可靠度曲線視覺化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf0fe47",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 6.3 PR 曲線與 Precision@K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109001b0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 7. 模型持久化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbc9652",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 8. 總結報告"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072f0bad",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 9.2 下載並處理真實光曲線"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9757c62",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 9.5 方法比較：合成注入 vs 監督式學習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf8276b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 9.6 儲存監督式模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e54fc48",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 10. 完整總結報告"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92423b83",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 9.2 下載並處理真實光曲線"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cdb15c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 9.5 方法比較：合成注入 vs 監督式學習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366f69db",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 9.6 儲存監督式模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4826da25",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 10. 完整總結報告"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.915705,
   "end_time": "2025-09-29T23:32:56.767194",
   "environment_variables": {},
   "exception": true,
   "input_path": "03_injection_train.ipynb",
   "output_path": "03_injection_train.ipynb",
   "parameters": {},
   "start_time": "2025-09-29T23:32:47.851489",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}