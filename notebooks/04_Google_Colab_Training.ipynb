{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Exoplanet Detection - Google Colab Training (2025)\n",
    "\n",
    "**Optimized for Google Colab with latest packages (October 2025)**\n",
    "\n",
    "This notebook:\n",
    "- Downloads pre-extracted features from GitHub Release\n",
    "- Trains XGBoost model on balanced dataset (500+500)\n",
    "- Optimized for Colab Free/Pro with GPU support\n",
    "- Latest package versions (XGBoost 2.1.x, scikit-learn 1.5.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Step 1: Environment Setup\n",
    "\n",
    "Install latest packages compatible with Python 3.10+ (Colab default in 2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Python version\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Install/upgrade packages (2025 latest versions)\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q xgboost>=2.1.0 scikit-learn>=1.5.0 pandas>=2.2.0 numpy>=1.26.0 matplotlib>=3.9.0 seaborn>=0.13.0\n",
    "\n",
    "print(\"✅ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installations\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(f\"XGBoost: {xgb.__version__}\")\n",
    "print(f\"scikit-learn: {sklearn.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📥 Step 2: Download Pre-extracted Features\n",
    "\n",
    "Download `balanced_features.csv` from GitHub Release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "# GitHub Release URL (update with your actual release)\n",
    "REPO_OWNER = \"exoplanet-spaceapps\"\n",
    "REPO_NAME = \"exoplanet-starter\"\n",
    "RELEASE_TAG = \"v1.0-features\"  # Update this!\n",
    "ASSET_NAME = \"balanced_features.csv\"\n",
    "\n",
    "# Download URL\n",
    "download_url = f\"https://github.com/{REPO_OWNER}/{REPO_NAME}/releases/download/{RELEASE_TAG}/{ASSET_NAME}\"\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs('data', exist_ok=True)\n",
    "features_path = Path('data/balanced_features.csv')\n",
    "\n",
    "# Download if not exists\n",
    "if not features_path.exists():\n",
    "    print(f\"📥 Downloading features from: {download_url}\")\n",
    "    urllib.request.urlretrieve(download_url, features_path)\n",
    "    print(f\"✅ Downloaded: {features_path}\")\n",
    "else:\n",
    "    print(f\"✅ Features already exist: {features_path}\")\n",
    "\n",
    "# Verify file\n",
    "file_size = features_path.stat().st_size / 1024 / 1024\n",
    "print(f\"📊 File size: {file_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Step 3: Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features\n",
    "df = pd.read_csv('data/balanced_features.csv')\n",
    "\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter successful extractions\n",
    "df_success = df[df['status'] == 'success'].copy()\n",
    "\n",
    "print(f\"Successful extractions: {len(df_success)} ({len(df_success)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Label distribution\n",
    "label_counts = df_success['label'].value_counts()\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(f\"  True (label=1): {label_counts.get(1, 0)}\")\n",
    "print(f\"  False (label=0): {label_counts.get(0, 0)}\")\n",
    "\n",
    "# Data info\n",
    "df_success.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Step 4: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Feature columns\n",
    "feature_columns = [\n",
    "    'flux_mean', 'flux_std', 'flux_median', 'flux_mad',\n",
    "    'flux_skew', 'flux_kurt',\n",
    "    'bls_period', 'bls_duration', 'bls_depth', 'bls_power', 'bls_snr'\n",
    "]\n",
    "\n",
    "# Prepare X and y\n",
    "X = df_success[feature_columns].copy()\n",
    "y = df_success['label'].copy()\n",
    "\n",
    "# Handle NaN values\n",
    "nan_counts = X.isnull().sum()\n",
    "if nan_counts.sum() > 0:\n",
    "    print(\"⚠️ NaN values detected, filling with median:\")\n",
    "    for col in feature_columns:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            median_val = X[col].median()\n",
    "            X[col].fillna(median_val, inplace=True)\n",
    "            print(f\"  {col}: {nan_counts[col]} NaNs filled with {median_val:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ Features shape: {X.shape}\")\n",
    "print(f\"✅ Labels shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"\\nTrain labels: {dict(y_train.value_counts())}\")\n",
    "print(f\"Test labels: {dict(y_test.value_counts())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 Step 5: Train XGBoost Model\n",
    "\n",
    "Using latest XGBoost 2.1.x API (2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost parameters (2025 best practices)\n",
    "xgb_params = {\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 100,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'random_state': 42,\n",
    "    'tree_method': 'hist',  # Faster on CPU/GPU (2025 default)\n",
    "    'device': 'cuda' if xgb.device.is_cuda_available() else 'cpu'  # Auto GPU detection (XGBoost 2.1+)\n",
    "}\n",
    "\n",
    "print(f\"Training device: {xgb_params['device']}\")\n",
    "print(f\"\\nParameters:\")\n",
    "for key, val in xgb_params.items():\n",
    "    print(f\"  {key}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train model\n",
    "model = xgb.XGBClassifier(**xgb_params)\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    verbose=10  # Show progress every 10 iterations\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Step 6: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "y_pred_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "metrics = {\n",
    "    'Train': {\n",
    "        'Accuracy': accuracy_score(y_train, y_pred_train),\n",
    "        'Precision': precision_score(y_train, y_pred_train),\n",
    "        'Recall': recall_score(y_train, y_pred_train),\n",
    "        'F1': f1_score(y_train, y_pred_train),\n",
    "        'ROC-AUC': roc_auc_score(y_train, y_pred_proba_train)\n",
    "    },\n",
    "    'Test': {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred_test),\n",
    "        'Precision': precision_score(y_test, y_pred_test),\n",
    "        'Recall': recall_score(y_test, y_pred_test),\n",
    "        'F1': f1_score(y_test, y_pred_test),\n",
    "        'ROC-AUC': roc_auc_score(y_test, y_pred_proba_test)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display metrics\n",
    "metrics_df = pd.DataFrame(metrics).T\n",
    "print(\"\\n📊 Model Performance:\")\n",
    "print(metrics_df.round(4))\n",
    "\n",
    "# Highlight test metrics\n",
    "print(f\"\\n🎯 **Test Set Performance:**\")\n",
    "print(f\"  Accuracy:  {metrics['Test']['Accuracy']:.2%}\")\n",
    "print(f\"  Precision: {metrics['Test']['Precision']:.2%}\")\n",
    "print(f\"  Recall:    {metrics['Test']['Recall']:.2%}\")\n",
    "print(f\"  F1:        {metrics['Test']['F1']:.2%}\")\n",
    "print(f\"  ROC-AUC:   {metrics['Test']['ROC-AUC']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Exoplanet', 'Exoplanet'],\n",
    "            yticklabels=['No Exoplanet', 'Exoplanet'])\n",
    "plt.title('Confusion Matrix (Test Set)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConfusion Matrix Breakdown:\")\n",
    "print(f\"  True Negatives:  {cm[0, 0]} (correctly predicted no exoplanet)\")\n",
    "print(f\"  False Positives: {cm[0, 1]} (incorrectly predicted exoplanet)\")\n",
    "print(f\"  False Negatives: {cm[1, 0]} (missed exoplanet)\")\n",
    "print(f\"  True Positives:  {cm[1, 1]} (correctly predicted exoplanet)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_test)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve - Exoplanet Detection', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Step 7: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['feature'], feature_importance['importance'])\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.title('Feature Importance - Exoplanet Detection', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFeature Importance Ranking:\")\n",
    "for idx, row in feature_importance.iterrows():\n",
    "    print(f\"  {row['feature']}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 Step 8: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save model (XGBoost 2.1+ JSON format)\n",
    "model_path = 'models/xgboost_model.json'\n",
    "model.save_model(model_path)\n",
    "print(f\"✅ Model saved: {model_path}\")\n",
    "\n",
    "# Save training report\n",
    "report = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'environment': 'Google Colab',\n",
    "    'xgboost_version': xgb.__version__,\n",
    "    'sklearn_version': sklearn.__version__,\n",
    "    'dataset': {\n",
    "        'total_samples': len(df_success),\n",
    "        'train_samples': len(X_train),\n",
    "        'test_samples': len(X_test),\n",
    "        'features': feature_columns\n",
    "    },\n",
    "    'model': {\n",
    "        'type': 'XGBClassifier',\n",
    "        'parameters': xgb_params,\n",
    "        'device': xgb_params['device']\n",
    "    },\n",
    "    'metrics': {\n",
    "        'train': {k: float(v) for k, v in metrics['Train'].items()},\n",
    "        'test': {k: float(v) for k, v in metrics['Test'].items()}\n",
    "    },\n",
    "    'confusion_matrix': {\n",
    "        'true_negatives': int(cm[0, 0]),\n",
    "        'false_positives': int(cm[0, 1]),\n",
    "        'false_negatives': int(cm[1, 0]),\n",
    "        'true_positives': int(cm[1, 1])\n",
    "    },\n",
    "    'feature_importance': feature_importance.to_dict('records')\n",
    "}\n",
    "\n",
    "report_path = 'models/colab_training_report.json'\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(f\"✅ Report saved: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📤 Step 9: Download Model (Optional)\n",
    "\n",
    "Download trained model to your local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "    \n",
    "    print(\"📥 Downloading model files...\")\n",
    "    files.download('models/xgboost_model.json')\n",
    "    files.download('models/colab_training_report.json')\n",
    "    print(\"✅ Download complete!\")\n",
    "except ImportError:\n",
    "    print(\"ℹ️ Not running in Colab. Files saved locally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Summary\n",
    "\n",
    "**Training Complete!**\n",
    "\n",
    "Your exoplanet detection model is ready for inference:\n",
    "- Model: `models/xgboost_model.json`\n",
    "- Report: `models/colab_training_report.json`\n",
    "\n",
    "**Next Steps:**\n",
    "1. Upload model to GitHub Release\n",
    "2. Use model for inference on new lightcurves\n",
    "3. Deploy as web service or integrate with frontend\n",
    "\n",
    "**Optimization Tips for Colab Pro:**\n",
    "- Enable GPU: Runtime → Change runtime type → GPU\n",
    "- Use High-RAM runtime for larger datasets\n",
    "- Consider hyperparameter tuning with cross-validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
