{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 Â· è©•ä¼°æŒ‡æ¨™å„€è¡¨æ¿ï¼ˆåˆæˆæ³¨å…¥ vs ç›£ç£å¼å­¸ç¿’ï¼‰\n",
    "\n",
    "## è©•ä¼°é¢å‘\n",
    "1. **æ•ˆèƒ½æŒ‡æ¨™**ï¼šPR-AUC, ROC-AUC, Precision@K, Recall@Known\n",
    "2. **æ ¡æº–æŒ‡æ¨™**ï¼šECE, Brier Score, å¯é åº¦æ›²ç·š\n",
    "3. **éŒ¯èª¤åˆ†æ**ï¼šå‡é™½æ€§ç‡, èª¤å·®æ¡ˆä¾‹ç•«å»Š\n",
    "4. **æ¨è«–æ•ˆèƒ½**ï¼šå»¶é²æ™‚é–“, ååé‡\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "source": "# æ­¥é©Ÿ 0: å®‰è£å¥—ä»¶èˆ‡ä¿®å¾© NumPy 2.0 ç›¸å®¹æ€§ (Colab ç’°å¢ƒ)\n# âš ï¸ é‡è¦: è‹¥åœ¨ Google Colabï¼ŒåŸ·è¡Œæ­¤ cell å¾Œè«‹æ‰‹å‹•é‡å•Ÿ Runtime (Runtime â†’ Restart runtime)\n\nimport sys\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    print(\"ğŸ“ åµæ¸¬åˆ° Google Colab ç’°å¢ƒ\")\n    print(\"ğŸ”§ å®‰è£ç›¸å®¹ç‰ˆæœ¬å¥—ä»¶...\")\n    !pip install -q numpy==1.26.4 pandas matplotlib scikit-learn seaborn joblib\n    print(\"âœ… å¥—ä»¶å®‰è£å®Œæˆ!\")\n    print(\"âš ï¸ è«‹ç¾åœ¨æ‰‹å‹•é‡å•Ÿ Runtime: Runtime â†’ Restart runtime\")\n    print(\"   ç„¶å¾Œå¾ä¸‹ä¸€å€‹ cell ç¹¼çºŒåŸ·è¡Œ\")\nelse:\n    print(\"ğŸ’» æœ¬åœ°ç’°å¢ƒï¼Œè·³éå¥—ä»¶å®‰è£\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒè¨­å®šèˆ‡å¥—ä»¶å°å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç’°å¢ƒè¨­å®š\n",
    "import sys, subprocess\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def pipi(*pkgs):\n",
    "    \"\"\"å®‰è£å¥—ä»¶çš„è¼”åŠ©å‡½å¼\"\"\"\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *pkgs])\n",
    "\n",
    "# å®‰è£å¿…è¦å¥—ä»¶\n",
    "print(\"ğŸš€ æ­£åœ¨è¨­å®šç’°å¢ƒ...\")\n",
    "try:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import sklearn\n",
    "    print(\"âœ… åŸºç¤å¥—ä»¶å·²å®‰è£\")\n",
    "except Exception:\n",
    "    pipi(\"numpy<2\", \"pandas\", \"matplotlib\", \"seaborn\", \"scikit-learn\", \"joblib\")\n",
    "    print(\"âœ… å¥—ä»¶å®‰è£å®Œæˆ\")\n",
    "\n",
    "# è¨­å®šå·¥ä½œç›®éŒ„\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    import os\n",
    "    if not os.path.exists('/content/exoplanet-starter'):\n",
    "        !git clone https://github.com/exoplanet-spaceapps/exoplanet-starter.git /content/exoplanet-starter\n",
    "        os.chdir('/content/exoplanet-starter')\n",
    "    sys.path.append('/content/exoplanet-starter')\n",
    "else:\n",
    "    import os\n",
    "    os.chdir(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "    sys.path.append(os.getcwd())\n",
    "\n",
    "print(\"ç’°å¢ƒè¨­å®šå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°å…¥å¿…è¦å¥—ä»¶\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    average_precision_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    calibration_curve,\n",
    "    brier_score_loss\n",
    ")\n",
    "\n",
    "# è¨­å®šè¦–è¦ºåŒ–é¢¨æ ¼\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"ğŸ“š å¥—ä»¶å°å…¥å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. è¼‰å…¥æ¨¡å‹å’Œæ¸¬è©¦è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡æ“¬æ¸¬è©¦è³‡æ–™ï¼ˆå¯¦éš›æ‡‰ç”¨æ™‚å¾çœŸå¯¦è³‡æ–™è¼‰å…¥ï¼‰\n",
    "np.random.seed(42)\n",
    "\n",
    "# ç”Ÿæˆæ¨¡æ“¬æ¸¬è©¦é›†\n",
    "n_test_samples = 500\n",
    "X_test = np.random.randn(n_test_samples, 14)  # 14å€‹ç‰¹å¾µ\n",
    "y_test = np.random.binomial(1, 0.3, n_test_samples)  # 30% æ­£é¡\n",
    "\n",
    "# æ¨¡æ“¬å…©å€‹æ¨¡å‹çš„é æ¸¬æ©Ÿç‡\n",
    "# åˆæˆæ³¨å…¥æ¨¡å‹ï¼ˆç¨å¾®éåº¦è‡ªä¿¡ï¼‰\n",
    "prob_synthetic = np.clip(\n",
    "    y_test * np.random.beta(8, 2, n_test_samples) + \n",
    "    (1 - y_test) * np.random.beta(2, 8, n_test_samples),\n",
    "    0.01, 0.99\n",
    ")\n",
    "\n",
    "# ç›£ç£å¼æ¨¡å‹ï¼ˆè¼ƒå¥½æ ¡æº–ï¼‰\n",
    "prob_supervised = np.clip(\n",
    "    y_test * np.random.beta(6, 3, n_test_samples) + \n",
    "    (1 - y_test) * np.random.beta(3, 6, n_test_samples),\n",
    "    0.01, 0.99\n",
    ")\n",
    "\n",
    "print(f\"âœ… è¼‰å…¥æ¸¬è©¦è³‡æ–™:\")\n",
    "print(f\"   æ¨£æœ¬æ•¸: {n_test_samples}\")\n",
    "print(f\"   æ­£é¡æ¯”ä¾‹: {y_test.mean():.2%}\")\n",
    "print(f\"   ç‰¹å¾µç¶­åº¦: {X_test.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. è¨ˆç®—è©•ä¼°æŒ‡æ¨™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_comprehensive_metrics(y_true, y_prob, model_name=\"Model\"):\n",
    "    \"\"\"è¨ˆç®—å®Œæ•´çš„è©•ä¼°æŒ‡æ¨™é›†\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # 1. åŸºç¤æŒ‡æ¨™\n",
    "    metrics['PR-AUC'] = average_precision_score(y_true, y_prob)\n",
    "    metrics['ROC-AUC'] = roc_auc_score(y_true, y_prob)\n",
    "    metrics['Brier Score'] = brier_score_loss(y_true, y_prob)\n",
    "    \n",
    "    # 2. Precision@K\n",
    "    k_values = [10, 20, 50]\n",
    "    sorted_indices = np.argsort(y_prob)[::-1]\n",
    "    for k in k_values:\n",
    "        if k <= len(y_true):\n",
    "            top_k_true = y_true[sorted_indices[:k]]\n",
    "            metrics[f'P@{k}'] = np.mean(top_k_true)\n",
    "    \n",
    "    # 3. Recall@Known (å‡è¨­å‰30%æ˜¯å·²çŸ¥è¡Œæ˜Ÿ)\n",
    "    known_planets = int(0.3 * np.sum(y_true))\n",
    "    if known_planets > 0:\n",
    "        threshold = np.sort(y_prob[y_true == 1])[-known_planets] if known_planets <= np.sum(y_true) else 0.5\n",
    "        predicted_positive = y_prob >= threshold\n",
    "        metrics['Recall@Known'] = np.sum((predicted_positive) & (y_true == 1)) / np.sum(y_true)\n",
    "    \n",
    "    # 4. False Positive Rate (at 90% recall)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    idx_90_recall = np.argmax(tpr >= 0.9)\n",
    "    metrics['FPR@90Recall'] = fpr[idx_90_recall]\n",
    "    \n",
    "    # 5. Expected Calibration Error (ECE)\n",
    "    ece = calculate_ece(y_true, y_prob)\n",
    "    metrics['ECE'] = ece\n",
    "    \n",
    "    # 6. æ··æ·†çŸ©é™£ï¼ˆä½¿ç”¨0.5é–¾å€¼ï¼‰\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    metrics['True Positives'] = tp\n",
    "    metrics['False Positives'] = fp\n",
    "    metrics['True Negatives'] = tn\n",
    "    metrics['False Negatives'] = fn\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def calculate_ece(y_true, y_prob, n_bins=10):\n",
    "    \"\"\"è¨ˆç®—æœŸæœ›æ ¡æº–èª¤å·® (ECE)\"\"\"\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    \n",
    "    ece = 0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = (y_prob > bin_lower) & (y_prob <= bin_upper)\n",
    "        prop_in_bin = in_bin.mean()\n",
    "        \n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = y_true[in_bin].mean()\n",
    "            avg_confidence_in_bin = y_prob[in_bin].mean()\n",
    "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "    \n",
    "    return ece\n",
    "\n",
    "# è¨ˆç®—å…©å€‹æ¨¡å‹çš„æŒ‡æ¨™\n",
    "metrics_synthetic = calculate_comprehensive_metrics(y_test, prob_synthetic, \"åˆæˆæ³¨å…¥\")\n",
    "metrics_supervised = calculate_comprehensive_metrics(y_test, prob_supervised, \"ç›£ç£å¼\")\n",
    "\n",
    "# å»ºç«‹æ¯”è¼ƒè¡¨\n",
    "comparison_df = pd.DataFrame({\n",
    "    'åˆæˆæ³¨å…¥': metrics_synthetic,\n",
    "    'ç›£ç£å¼': metrics_supervised\n",
    "}).T\n",
    "\n",
    "print(\"ğŸ“Š è©•ä¼°æŒ‡æ¨™å°æ¯”:\")\n",
    "print(comparison_df[['PR-AUC', 'ROC-AUC', 'ECE', 'Brier Score', 'P@10', 'P@20']].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æŒ‡æ¨™å°ç…§è¡¨è¦–è¦ºåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºæŒ‡æ¨™å°ç…§è¦–è¦ºåŒ–\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# é¸æ“‡è¦è¦–è¦ºåŒ–çš„æŒ‡æ¨™\n",
    "metrics_to_plot = ['PR-AUC', 'ROC-AUC', 'ECE', 'Brier Score', 'P@10', 'FPR@90Recall']\n",
    "colors = ['#3498db', '#e74c3c']  # è—è‰²=åˆæˆï¼Œç´…è‰²=ç›£ç£\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    values = [\n",
    "        metrics_synthetic.get(metric, 0),\n",
    "        metrics_supervised.get(metric, 0)\n",
    "    ]\n",
    "    \n",
    "    bars = ax.bar(['åˆæˆæ³¨å…¥', 'ç›£ç£å¼'], values, color=colors, alpha=0.7)\n",
    "    \n",
    "    # æ·»åŠ æ•¸å€¼æ¨™ç±¤\n",
    "    for bar, val in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    ax.set_title(metric, fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('å€¼', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # è¨­ç½®Yè»¸ç¯„åœ\n",
    "    if metric in ['PR-AUC', 'ROC-AUC', 'P@10']:\n",
    "        ax.set_ylim([0, 1.1])\n",
    "    elif metric in ['ECE', 'Brier Score', 'FPR@90Recall']:\n",
    "        ax.set_ylim([0, max(values) * 1.2])\n",
    "\n",
    "plt.suptitle('ğŸ¯ æ¨¡å‹æ•ˆèƒ½æŒ‡æ¨™å°æ¯”å„€è¡¨æ¿', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# åˆ¤æ–·å„ªå‹¢\n",
    "print(\"\\nğŸ’¡ åˆ†ææ‘˜è¦:\")\n",
    "if metrics_synthetic['PR-AUC'] > metrics_supervised['PR-AUC']:\n",
    "    print(f\"   â€¢ åˆæˆæ³¨å…¥åœ¨ PR-AUC ä¸Šé ˜å…ˆ {(metrics_synthetic['PR-AUC'] - metrics_supervised['PR-AUC'])*100:.1f}%\")\n",
    "else:\n",
    "    print(f\"   â€¢ ç›£ç£å¼åœ¨ PR-AUC ä¸Šé ˜å…ˆ {(metrics_supervised['PR-AUC'] - metrics_synthetic['PR-AUC'])*100:.1f}%\")\n",
    "\n",
    "if metrics_synthetic['ECE'] < metrics_supervised['ECE']:\n",
    "    print(f\"   â€¢ åˆæˆæ³¨å…¥æœ‰æ›´å¥½çš„æ ¡æº– (ECE: {metrics_synthetic['ECE']:.3f})\")\n",
    "else:\n",
    "    print(f\"   â€¢ ç›£ç£å¼æœ‰æ›´å¥½çš„æ ¡æº– (ECE: {metrics_supervised['ECE']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PR æ›²ç·šå’Œ ROC æ›²ç·š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¹ªè£½ PR å’Œ ROC æ›²ç·š\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# PR æ›²ç·š\n",
    "precision_syn, recall_syn, _ = precision_recall_curve(y_test, prob_synthetic)\n",
    "precision_sup, recall_sup, _ = precision_recall_curve(y_test, prob_supervised)\n",
    "\n",
    "ax1.plot(recall_syn, precision_syn, label=f'åˆæˆæ³¨å…¥ (AP={metrics_synthetic[\"PR-AUC\"]:.3f})', \n",
    "         color='#3498db', linewidth=2)\n",
    "ax1.plot(recall_sup, precision_sup, label=f'ç›£ç£å¼ (AP={metrics_supervised[\"PR-AUC\"]:.3f})', \n",
    "         color='#e74c3c', linewidth=2, linestyle='--')\n",
    "\n",
    "ax1.set_xlabel('Recall', fontsize=11)\n",
    "ax1.set_ylabel('Precision', fontsize=11)\n",
    "ax1.set_title('Precision-Recall æ›²ç·š', fontsize=12, fontweight='bold')\n",
    "ax1.legend(loc='best')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "# ROC æ›²ç·š\n",
    "fpr_syn, tpr_syn, _ = roc_curve(y_test, prob_synthetic)\n",
    "fpr_sup, tpr_sup, _ = roc_curve(y_test, prob_supervised)\n",
    "\n",
    "ax2.plot(fpr_syn, tpr_syn, label=f'åˆæˆæ³¨å…¥ (AUC={metrics_synthetic[\"ROC-AUC\"]:.3f})', \n",
    "         color='#3498db', linewidth=2)\n",
    "ax2.plot(fpr_sup, tpr_sup, label=f'ç›£ç£å¼ (AUC={metrics_supervised[\"ROC-AUC\"]:.3f})', \n",
    "         color='#e74c3c', linewidth=2, linestyle='--')\n",
    "ax2.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Random')\n",
    "\n",
    "ax2.set_xlabel('False Positive Rate', fontsize=11)\n",
    "ax2.set_ylabel('True Positive Rate', fontsize=11)\n",
    "ax2.set_title('ROC æ›²ç·š', fontsize=12, fontweight='bold')\n",
    "ax2.legend(loc='best')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim([0, 1])\n",
    "ax2.set_ylim([0, 1])\n",
    "\n",
    "plt.suptitle('ğŸ“ˆ åˆ†é¡æ•ˆèƒ½æ›²ç·š', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. å¯é åº¦æ›²ç·šï¼ˆCalibration Curvesï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¹ªè£½å¯é åº¦æ›²ç·š\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "n_bins = 10\n",
    "\n",
    "# åˆæˆæ³¨å…¥æ¨¡å‹\n",
    "fraction_pos_syn, mean_pred_syn = calibration_curve(y_test, prob_synthetic, n_bins=n_bins)\n",
    "axes[0].plot(mean_pred_syn, fraction_pos_syn, 'o-', color='#3498db', linewidth=2, \n",
    "             markersize=8, label='åˆæˆæ³¨å…¥')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='å®Œç¾æ ¡æº–')\n",
    "\n",
    "# è¨ˆç®—æ¯å€‹binçš„æ¨£æœ¬æ•¸\n",
    "bin_counts_syn = np.histogram(prob_synthetic, bins=n_bins)[0]\n",
    "for i, (x, y) in enumerate(zip(mean_pred_syn, fraction_pos_syn)):\n",
    "    if i < len(bin_counts_syn):\n",
    "        axes[0].annotate(f'n={bin_counts_syn[i]}', (x, y), \n",
    "                        textcoords=\"offset points\", xytext=(0,5), \n",
    "                        ha='center', fontsize=8, alpha=0.7)\n",
    "\n",
    "axes[0].set_xlabel('å¹³å‡é æ¸¬æ©Ÿç‡', fontsize=11)\n",
    "axes[0].set_ylabel('å¯¦éš›æ­£é¡æ¯”ä¾‹', fontsize=11)\n",
    "axes[0].set_title(f'åˆæˆæ³¨å…¥ å¯é åº¦æ›²ç·š (ECE={metrics_synthetic[\"ECE\"]:.3f})', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].legend(loc='best')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xlim([0, 1])\n",
    "axes[0].set_ylim([0, 1])\n",
    "\n",
    "# ç›£ç£å¼æ¨¡å‹\n",
    "fraction_pos_sup, mean_pred_sup = calibration_curve(y_test, prob_supervised, n_bins=n_bins)\n",
    "axes[1].plot(mean_pred_sup, fraction_pos_sup, 'o-', color='#e74c3c', linewidth=2, \n",
    "             markersize=8, label='ç›£ç£å¼')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', label='å®Œç¾æ ¡æº–')\n",
    "\n",
    "# è¨ˆç®—æ¯å€‹binçš„æ¨£æœ¬æ•¸\n",
    "bin_counts_sup = np.histogram(prob_supervised, bins=n_bins)[0]\n",
    "for i, (x, y) in enumerate(zip(mean_pred_sup, fraction_pos_sup)):\n",
    "    if i < len(bin_counts_sup):\n",
    "        axes[1].annotate(f'n={bin_counts_sup[i]}', (x, y), \n",
    "                        textcoords=\"offset points\", xytext=(0,5), \n",
    "                        ha='center', fontsize=8, alpha=0.7)\n",
    "\n",
    "axes[1].set_xlabel('å¹³å‡é æ¸¬æ©Ÿç‡', fontsize=11)\n",
    "axes[1].set_ylabel('å¯¦éš›æ­£é¡æ¯”ä¾‹', fontsize=11)\n",
    "axes[1].set_title(f'ç›£ç£å¼ å¯é åº¦æ›²ç·š (ECE={metrics_supervised[\"ECE\"]:.3f})', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].legend(loc='best')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xlim([0, 1])\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "plt.suptitle('ğŸ¯ æ©Ÿç‡æ ¡æº–åˆ†æ', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ æ ¡æº–åˆ†æ:\")\n",
    "print(f\"   â€¢ åˆæˆæ³¨å…¥ ECE: {metrics_synthetic['ECE']:.3f}\")\n",
    "print(f\"   â€¢ ç›£ç£å¼ ECE: {metrics_supervised['ECE']:.3f}\")\n",
    "print(f\"   â€¢ {'ç›£ç£å¼' if metrics_supervised['ECE'] < metrics_synthetic['ECE'] else 'åˆæˆæ³¨å…¥'}æ¨¡å‹æœ‰æ›´å¥½çš„æ©Ÿç‡æ ¡æº–\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. éŒ¯èª¤æ¡ˆä¾‹åˆ†æç•«å»Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰¾å‡ºéŒ¯èª¤æ¡ˆä¾‹\n",
    "threshold = 0.5\n",
    "\n",
    "# åˆæˆæ³¨å…¥æ¨¡å‹éŒ¯èª¤\n",
    "pred_syn = (prob_synthetic >= threshold).astype(int)\n",
    "errors_syn = pred_syn != y_test\n",
    "fp_indices_syn = np.where((pred_syn == 1) & (y_test == 0))[0][:5]  # å‰5å€‹å‡é™½æ€§\n",
    "fn_indices_syn = np.where((pred_syn == 0) & (y_test == 1))[0][:5]  # å‰5å€‹å‡é™°æ€§\n",
    "\n",
    "# ç›£ç£å¼æ¨¡å‹éŒ¯èª¤\n",
    "pred_sup = (prob_supervised >= threshold).astype(int)\n",
    "errors_sup = pred_sup != y_test\n",
    "fp_indices_sup = np.where((pred_sup == 1) & (y_test == 0))[0][:5]  # å‰5å€‹å‡é™½æ€§\n",
    "fn_indices_sup = np.where((pred_sup == 0) & (y_test == 1))[0][:5]  # å‰5å€‹å‡é™°æ€§\n",
    "\n",
    "# å‰µå»ºéŒ¯èª¤æ¡ˆä¾‹ç•«å»Š\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(4, 5, hspace=0.4, wspace=0.3)\n",
    "\n",
    "# åˆæˆæ³¨å…¥ - å‡é™½æ€§\n",
    "for i, idx in enumerate(fp_indices_syn):\n",
    "    ax = fig.add_subplot(gs[0, i])\n",
    "    ax.bar(['é æ¸¬'], [prob_synthetic[idx]], color='red', alpha=0.7)\n",
    "    ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_title(f'FP #{idx}\\nP={prob_synthetic[idx]:.2f}', fontsize=9)\n",
    "    ax.set_ylabel('æ©Ÿç‡' if i == 0 else '')\n",
    "    if i == 0:\n",
    "        ax.text(-0.5, 0.5, 'åˆæˆ\\nå‡é™½æ€§', fontsize=10, fontweight='bold', \n",
    "                rotation=90, va='center')\n",
    "\n",
    "# åˆæˆæ³¨å…¥ - å‡é™°æ€§\n",
    "for i, idx in enumerate(fn_indices_syn):\n",
    "    ax = fig.add_subplot(gs[1, i])\n",
    "    ax.bar(['é æ¸¬'], [prob_synthetic[idx]], color='blue', alpha=0.7)\n",
    "    ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_title(f'FN #{idx}\\nP={prob_synthetic[idx]:.2f}', fontsize=9)\n",
    "    ax.set_ylabel('æ©Ÿç‡' if i == 0 else '')\n",
    "    if i == 0:\n",
    "        ax.text(-0.5, 0.5, 'åˆæˆ\\nå‡é™°æ€§', fontsize=10, fontweight='bold', \n",
    "                rotation=90, va='center')\n",
    "\n",
    "# ç›£ç£å¼ - å‡é™½æ€§\n",
    "for i, idx in enumerate(fp_indices_sup):\n",
    "    ax = fig.add_subplot(gs[2, i])\n",
    "    ax.bar(['é æ¸¬'], [prob_supervised[idx]], color='red', alpha=0.7)\n",
    "    ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_title(f'FP #{idx}\\nP={prob_supervised[idx]:.2f}', fontsize=9)\n",
    "    ax.set_ylabel('æ©Ÿç‡' if i == 0 else '')\n",
    "    if i == 0:\n",
    "        ax.text(-0.5, 0.5, 'ç›£ç£\\nå‡é™½æ€§', fontsize=10, fontweight='bold', \n",
    "                rotation=90, va='center')\n",
    "\n",
    "# ç›£ç£å¼ - å‡é™°æ€§\n",
    "for i, idx in enumerate(fn_indices_sup):\n",
    "    ax = fig.add_subplot(gs[3, i])\n",
    "    ax.bar(['é æ¸¬'], [prob_supervised[idx]], color='blue', alpha=0.7)\n",
    "    ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_title(f'FN #{idx}\\nP={prob_supervised[idx]:.2f}', fontsize=9)\n",
    "    ax.set_ylabel('æ©Ÿç‡' if i == 0 else '')\n",
    "    if i == 0:\n",
    "        ax.text(-0.5, 0.5, 'ç›£ç£\\nå‡é™°æ€§', fontsize=10, fontweight='bold', \n",
    "                rotation=90, va='center')\n",
    "\n",
    "plt.suptitle('ğŸ” éŒ¯èª¤æ¡ˆä¾‹ç•«å»Šï¼ˆå‡é™½æ€§ vs å‡é™°æ€§ï¼‰', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# éŒ¯èª¤çµ±è¨ˆ\n",
    "print(\"\\nğŸ“Š éŒ¯èª¤çµ±è¨ˆ:\")\n",
    "print(f\"åˆæˆæ³¨å…¥æ¨¡å‹:\")\n",
    "print(f\"   â€¢ å‡é™½æ€§: {metrics_synthetic['False Positives']}\")\n",
    "print(f\"   â€¢ å‡é™°æ€§: {metrics_synthetic['False Negatives']}\")\n",
    "print(f\"   â€¢ ç¸½éŒ¯èª¤ç‡: {errors_syn.mean():.2%}\")\n",
    "print(f\"\\nç›£ç£å¼æ¨¡å‹:\")\n",
    "print(f\"   â€¢ å‡é™½æ€§: {metrics_supervised['False Positives']}\")\n",
    "print(f\"   â€¢ å‡é™°æ€§: {metrics_supervised['False Negatives']}\")\n",
    "print(f\"   â€¢ ç¸½éŒ¯èª¤ç‡: {errors_sup.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. æ¨è«–å»¶é²æ™‚é–“æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡æ“¬æ¨è«–å»¶é²æ¸¬è©¦\n",
    "import time\n",
    "\n",
    "def measure_inference_latency(n_samples=100, n_runs=10):\n",
    "    \"\"\"æ¸¬é‡æ¨è«–å»¶é²æ™‚é–“\"\"\"\n",
    "    X_bench = np.random.randn(n_samples, 14)\n",
    "    \n",
    "    # æ¨¡æ“¬åˆæˆæ³¨å…¥æ¨¡å‹æ¨è«–\n",
    "    synthetic_times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        # æ¨¡æ“¬æ¨è«–ï¼ˆå¯¦éš›æ‡‰ç”¨ä¸­å‘¼å«çœŸå¯¦æ¨¡å‹ï¼‰\n",
    "        _ = np.random.rand(n_samples)\n",
    "        time.sleep(0.01)  # æ¨¡æ“¬è¨ˆç®—æ™‚é–“\n",
    "        synthetic_times.append(time.time() - start)\n",
    "    \n",
    "    # æ¨¡æ“¬ç›£ç£å¼æ¨¡å‹æ¨è«–\n",
    "    supervised_times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        # æ¨¡æ“¬æ¨è«–ï¼ˆå¯¦éš›æ‡‰ç”¨ä¸­å‘¼å«çœŸå¯¦æ¨¡å‹ï¼‰\n",
    "        _ = np.random.rand(n_samples)\n",
    "        time.sleep(0.012)  # ç›£ç£å¼ç¨æ…¢\n",
    "        supervised_times.append(time.time() - start)\n",
    "    \n",
    "    return synthetic_times, supervised_times\n",
    "\n",
    "# æ¸¬é‡å»¶é²\n",
    "print(\"â±ï¸ æ¸¬é‡æ¨è«–å»¶é²...\")\n",
    "syn_times, sup_times = measure_inference_latency()\n",
    "\n",
    "# è¨ˆç®—çµ±è¨ˆ\n",
    "latency_stats = pd.DataFrame({\n",
    "    'åˆæˆæ³¨å…¥': {\n",
    "        'å¹³å‡å»¶é²(ms)': np.mean(syn_times) * 1000,\n",
    "        'ä¸­ä½æ•¸(ms)': np.median(syn_times) * 1000,\n",
    "        'æœ€å°å€¼(ms)': np.min(syn_times) * 1000,\n",
    "        'æœ€å¤§å€¼(ms)': np.max(syn_times) * 1000,\n",
    "        'æ¨™æº–å·®(ms)': np.std(syn_times) * 1000\n",
    "    },\n",
    "    'ç›£ç£å¼': {\n",
    "        'å¹³å‡å»¶é²(ms)': np.mean(sup_times) * 1000,\n",
    "        'ä¸­ä½æ•¸(ms)': np.median(sup_times) * 1000,\n",
    "        'æœ€å°å€¼(ms)': np.min(sup_times) * 1000,\n",
    "        'æœ€å¤§å€¼(ms)': np.max(sup_times) * 1000,\n",
    "        'æ¨™æº–å·®(ms)': np.std(sup_times) * 1000\n",
    "    }\n",
    "}).T\n",
    "\n",
    "print(\"\\nğŸ“Š æ¨è«–å»¶é²çµ±è¨ˆ (100æ¨£æœ¬æ‰¹æ¬¡):\")\n",
    "print(latency_stats.round(2))\n",
    "\n",
    "# è¦–è¦ºåŒ–å»¶é²åˆ†å¸ƒ\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "positions = [1, 2]\n",
    "bp = ax.boxplot(\n",
    "    [np.array(syn_times) * 1000, np.array(sup_times) * 1000],\n",
    "    positions=positions,\n",
    "    widths=0.6,\n",
    "    patch_artist=True,\n",
    "    labels=['åˆæˆæ³¨å…¥', 'ç›£ç£å¼']\n",
    ")\n",
    "\n",
    "# è¨­å®šé¡è‰²\n",
    "colors = ['#3498db', '#e74c3c']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.set_ylabel('å»¶é²æ™‚é–“ (ms)', fontsize=11)\n",
    "ax.set_title('æ¨è«–å»¶é²æ™‚é–“åˆ†å¸ƒ', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ååé‡è¨ˆç®—\n",
    "throughput_syn = 100 / np.mean(syn_times)  # samples/second\n",
    "throughput_sup = 100 / np.mean(sup_times)\n",
    "\n",
    "print(f\"\\nâš¡ ååé‡:\")\n",
    "print(f\"   â€¢ åˆæˆæ³¨å…¥: {throughput_syn:.0f} æ¨£æœ¬/ç§’\")\n",
    "print(f\"   â€¢ ç›£ç£å¼: {throughput_sup:.0f} æ¨£æœ¬/ç§’\")\n",
    "print(f\"   â€¢ é€Ÿåº¦å·®ç•°: {(throughput_syn/throughput_sup - 1)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ç¶œåˆè©•ä¼°å ±å‘Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”¢ç”Ÿç¶œåˆè©•ä¼°å ±å‘Š\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ“Š ç¶œåˆè©•ä¼°å ±å‘Š\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# å»ºç«‹å„ªå‹¢æ¯”è¼ƒ\n",
    "synthetic_wins = 0\n",
    "supervised_wins = 0\n",
    "\n",
    "# æ¯”è¼ƒå„é …æŒ‡æ¨™\n",
    "comparison_metrics = ['PR-AUC', 'ROC-AUC', 'ECE', 'Brier Score', 'P@10']\n",
    "for metric in comparison_metrics:\n",
    "    if metric in ['ECE', 'Brier Score']:  # è¶Šä½è¶Šå¥½\n",
    "        if metrics_synthetic[metric] < metrics_supervised[metric]:\n",
    "            synthetic_wins += 1\n",
    "        else:\n",
    "            supervised_wins += 1\n",
    "    else:  # è¶Šé«˜è¶Šå¥½\n",
    "        if metrics_synthetic[metric] > metrics_supervised[metric]:\n",
    "            synthetic_wins += 1\n",
    "        else:\n",
    "            supervised_wins += 1\n",
    "\n",
    "print(f\"\\nğŸ† ç¸½é«”å„ªå‹¢:\")\n",
    "print(f\"   â€¢ åˆæˆæ³¨å…¥: {synthetic_wins}/{len(comparison_metrics)} æŒ‡æ¨™é ˜å…ˆ\")\n",
    "print(f\"   â€¢ ç›£ç£å¼: {supervised_wins}/{len(comparison_metrics)} æŒ‡æ¨™é ˜å…ˆ\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ é—œéµæŒ‡æ¨™æ‘˜è¦:\")\n",
    "print(f\"\\n   ã€æ•ˆèƒ½æŒ‡æ¨™ã€‘\")\n",
    "print(f\"   PR-AUC:\")\n",
    "print(f\"     â€¢ åˆæˆæ³¨å…¥: {metrics_synthetic['PR-AUC']:.3f}\")\n",
    "print(f\"     â€¢ ç›£ç£å¼: {metrics_supervised['PR-AUC']:.3f}\")\n",
    "print(f\"   ROC-AUC:\")\n",
    "print(f\"     â€¢ åˆæˆæ³¨å…¥: {metrics_synthetic['ROC-AUC']:.3f}\")\n",
    "print(f\"     â€¢ ç›£ç£å¼: {metrics_supervised['ROC-AUC']:.3f}\")\n",
    "\n",
    "print(f\"\\n   ã€æ ¡æº–æŒ‡æ¨™ã€‘\")\n",
    "print(f\"   ECE (æœŸæœ›æ ¡æº–èª¤å·®):\")\n",
    "print(f\"     â€¢ åˆæˆæ³¨å…¥: {metrics_synthetic['ECE']:.3f}\")\n",
    "print(f\"     â€¢ ç›£ç£å¼: {metrics_supervised['ECE']:.3f}\")\n",
    "print(f\"   Brier Score:\")\n",
    "print(f\"     â€¢ åˆæˆæ³¨å…¥: {metrics_synthetic['Brier Score']:.3f}\")\n",
    "print(f\"     â€¢ ç›£ç£å¼: {metrics_supervised['Brier Score']:.3f}\")\n",
    "\n",
    "print(f\"\\n   ã€å¯¦ç”¨æŒ‡æ¨™ã€‘\")\n",
    "print(f\"   Precision@10:\")\n",
    "print(f\"     â€¢ åˆæˆæ³¨å…¥: {metrics_synthetic['P@10']:.3f}\")\n",
    "print(f\"     â€¢ ç›£ç£å¼: {metrics_supervised['P@10']:.3f}\")\n",
    "print(f\"   FPR@90% Recall:\")\n",
    "print(f\"     â€¢ åˆæˆæ³¨å…¥: {metrics_synthetic['FPR@90Recall']:.3f}\")\n",
    "print(f\"     â€¢ ç›£ç£å¼: {metrics_supervised['FPR@90Recall']:.3f}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ å»ºè­°:\")\n",
    "if synthetic_wins > supervised_wins:\n",
    "    print(\"   âœ“ åˆæˆæ³¨å…¥æ¨¡å‹åœ¨å¤šæ•¸æŒ‡æ¨™ä¸Šè¡¨ç¾è¼ƒä½³\")\n",
    "    print(\"   âœ“ é©åˆç”¨æ–¼è³‡æ–™ç¨€ç¼ºçš„æƒ…æ³\")\n",
    "    print(\"   âœ“ å¯å¿«é€Ÿè¿­ä»£å’Œæ¸¬è©¦\")\n",
    "else:\n",
    "    print(\"   âœ“ ç›£ç£å¼æ¨¡å‹åœ¨å¤šæ•¸æŒ‡æ¨™ä¸Šè¡¨ç¾è¼ƒä½³\")\n",
    "    print(\"   âœ“ æ›´æ¥è¿‘çœŸå¯¦æ‡‰ç”¨å ´æ™¯\")\n",
    "    print(\"   âœ“ å°çœŸå¯¦å™ªéŸ³æœ‰æ›´å¥½çš„é­¯æ£’æ€§\")\n",
    "\n",
    "print(f\"\\nâš ï¸ é™åˆ¶èˆ‡é¢¨éšª:\")\n",
    "print(\"   â€¢ åˆæˆæ³¨å…¥å¯èƒ½ç„¡æ³•å®Œå…¨æ¨¡æ“¬çœŸå¯¦çš„ç³»çµ±èª¤å·®\")\n",
    "print(\"   â€¢ ç›£ç£å¼å­¸ç¿’ä¾è³´æ¨™è¨»è³‡æ–™çš„å“è³ªå’Œæ•¸é‡\")\n",
    "print(\"   â€¢ å…©ç¨®æ–¹æ³•éƒ½å¯èƒ½å°æœªè¦‹éçš„å‡Œæ—¥æ¨¡å¼è¡¨ç¾ä¸ä½³\")\n",
    "print(\"   â€¢ éœ€è¦å®šæœŸæ›´æ–°æ¨¡å‹ä»¥é©æ‡‰æ–°çš„è§€æ¸¬è³‡æ–™\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… è©•ä¼°å®Œæˆï¼\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. åŒ¯å‡ºè©•ä¼°çµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŒ¯å‡ºè©•ä¼°çµæœ\n",
    "output_dir = Path(\"results\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 1. åŒ¯å‡ºæŒ‡æ¨™å°æ¯”è¡¨\n",
    "comparison_df.to_csv(output_dir / \"metrics_comparison.csv\")\n",
    "print(f\"âœ… æŒ‡æ¨™å°æ¯”è¡¨å·²å„²å­˜: {output_dir / 'metrics_comparison.csv'}\")\n",
    "\n",
    "# 2. åŒ¯å‡ºå»¶é²çµ±è¨ˆ\n",
    "latency_stats.to_csv(output_dir / \"latency_statistics.csv\")\n",
    "print(f\"âœ… å»¶é²çµ±è¨ˆå·²å„²å­˜: {output_dir / 'latency_statistics.csv'}\")\n",
    "\n",
    "# 3. ç”¢ç”Ÿæ‘˜è¦å ±å‘Š\n",
    "report = {\n",
    "    \"evaluation_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"test_samples\": int(n_test_samples),\n",
    "    \"positive_ratio\": float(y_test.mean()),\n",
    "    \"models_compared\": [\"åˆæˆæ³¨å…¥\", \"ç›£ç£å¼\"],\n",
    "    \"best_model\": \"åˆæˆæ³¨å…¥\" if synthetic_wins > supervised_wins else \"ç›£ç£å¼\",\n",
    "    \"key_findings\": {\n",
    "        \"pr_auc_difference\": float(metrics_synthetic['PR-AUC'] - metrics_supervised['PR-AUC']),\n",
    "        \"ece_difference\": float(metrics_synthetic['ECE'] - metrics_supervised['ECE']),\n",
    "        \"throughput_ratio\": float(throughput_syn / throughput_sup)\n",
    "    },\n",
    "    \"recommendations\": [\n",
    "        \"è€ƒæ…®çµåˆå…©ç¨®æ–¹æ³•çš„å„ªå‹¢\",\n",
    "        \"å®šæœŸä½¿ç”¨æ–°è³‡æ–™é‡æ–°è¨“ç·´\",\n",
    "        \"ç›£æ§ç·šä¸Šé æ¸¬çš„æ ¡æº–å“è³ª\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(output_dir / \"evaluation_summary.json\", 'w') as f:\n",
    "    json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… è©•ä¼°æ‘˜è¦å·²å„²å­˜: {output_dir / 'evaluation_summary.json'}\")\n",
    "\n",
    "print(\"\\nğŸ“ æ‰€æœ‰è©•ä¼°çµæœå·²å„²å­˜è‡³ results/ ç›®éŒ„\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}