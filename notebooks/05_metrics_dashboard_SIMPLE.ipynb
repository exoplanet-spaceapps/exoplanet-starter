{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 · 評估指標儀表板（合成注入 vs 監督式學習）\n",
    "\n",
    "## 評估面向\n",
    "1. **效能指標**：PR-AUC, ROC-AUC, Precision@K, Recall@Known\n",
    "2. **校準指標**：ECE, Brier Score, 可靠度曲線\n",
    "3. **錯誤分析**：假陽性率, 誤差案例畫廊\n",
    "4. **推論效能**：延遲時間, 吞吐量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 導入基礎套件\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    average_precision_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    brier_score_loss\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# 設定視覺化風格\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"📚 套件導入完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 模擬測試資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模擬測試資料\n",
    "np.random.seed(42)\n",
    "\n",
    "n_test_samples = 500\n",
    "X_test = np.random.randn(n_test_samples, 14)\n",
    "y_test = np.random.binomial(1, 0.3, n_test_samples)\n",
    "\n",
    "# 模擬兩個模型的預測機率\n",
    "prob_synthetic = np.clip(\n",
    "    y_test * np.random.beta(8, 2, n_test_samples) + \n",
    "    (1 - y_test) * np.random.beta(2, 8, n_test_samples),\n",
    "    0.01, 0.99\n",
    ")\n",
    "\n",
    "prob_supervised = np.clip(\n",
    "    y_test * np.random.beta(6, 3, n_test_samples) + \n",
    "    (1 - y_test) * np.random.beta(3, 6, n_test_samples),\n",
    "    0.01, 0.99\n",
    ")\n",
    "\n",
    "print(f\"✅ 測試資料: {n_test_samples} 樣本, 正類比例: {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 計算評估指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_prob):\n",
    "    \"\"\"計算評估指標\"\"\"\n",
    "    metrics = {}\n",
    "    metrics['PR-AUC'] = average_precision_score(y_true, y_prob)\n",
    "    metrics['ROC-AUC'] = roc_auc_score(y_true, y_prob)\n",
    "    metrics['Brier Score'] = brier_score_loss(y_true, y_prob)\n",
    "    \n",
    "    # Precision@10\n",
    "    sorted_indices = np.argsort(y_prob)[::-1]\n",
    "    metrics['P@10'] = np.mean(y_true[sorted_indices[:10]])\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "metrics_synthetic = calculate_metrics(y_test, prob_synthetic)\n",
    "metrics_supervised = calculate_metrics(y_test, prob_supervised)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    '合成注入': metrics_synthetic,\n",
    "    '監督式': metrics_supervised\n",
    "}).T\n",
    "\n",
    "print(\"📊 評估指標對比:\")\n",
    "print(comparison_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 視覺化對比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 繪製指標對比\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "metrics_to_plot = ['PR-AUC', 'ROC-AUC', 'Brier Score', 'P@10']\n",
    "colors = ['#3498db', '#e74c3c']\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    values = [metrics_synthetic[metric], metrics_supervised[metric]]\n",
    "    bars = ax.bar(['合成注入', '監督式'], values, color=colors, alpha=0.7)\n",
    "    \n",
    "    for bar, val in zip(bars, values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., val,\n",
    "                f'{val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax.set_title(metric, fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('值')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('🎯 模型效能指標對比', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/metrics_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ 指標對比圖已儲存\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PR 和 ROC 曲線"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# PR 曲線\n",
    "precision_syn, recall_syn, _ = precision_recall_curve(y_test, prob_synthetic)\n",
    "precision_sup, recall_sup, _ = precision_recall_curve(y_test, prob_supervised)\n",
    "\n",
    "ax1.plot(recall_syn, precision_syn, label=f'合成注入 (AP={metrics_synthetic[\"PR-AUC\"]:.3f})', \n",
    "         color='#3498db', linewidth=2)\n",
    "ax1.plot(recall_sup, precision_sup, label=f'監督式 (AP={metrics_supervised[\"PR-AUC\"]:.3f})', \n",
    "         color='#e74c3c', linewidth=2, linestyle='--')\n",
    "ax1.set_xlabel('Recall')\n",
    "ax1.set_ylabel('Precision')\n",
    "ax1.set_title('Precision-Recall 曲線')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ROC 曲線\n",
    "fpr_syn, tpr_syn, _ = roc_curve(y_test, prob_synthetic)\n",
    "fpr_sup, tpr_sup, _ = roc_curve(y_test, prob_supervised)\n",
    "\n",
    "ax2.plot(fpr_syn, tpr_syn, label=f'合成注入 (AUC={metrics_synthetic[\"ROC-AUC\"]:.3f})', \n",
    "         color='#3498db', linewidth=2)\n",
    "ax2.plot(fpr_sup, tpr_sup, label=f'監督式 (AUC={metrics_supervised[\"ROC-AUC\"]:.3f})', \n",
    "         color='#e74c3c', linewidth=2, linestyle='--')\n",
    "ax2.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Random')\n",
    "ax2.set_xlabel('False Positive Rate')\n",
    "ax2.set_ylabel('True Positive Rate')\n",
    "ax2.set_title('ROC 曲線')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('📈 分類效能曲線', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/performance_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ PR/ROC 曲線已儲存\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 匯出結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建結果目錄\n",
    "output_dir = Path(\"results\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 匯出指標\n",
    "comparison_df.to_csv(output_dir / \"metrics_comparison.csv\")\n",
    "\n",
    "# 匯出摘要\n",
    "report = {\n",
    "    \"evaluation_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"test_samples\": int(n_test_samples),\n",
    "    \"positive_ratio\": float(y_test.mean()),\n",
    "    \"synthetic_metrics\": {k: float(v) for k, v in metrics_synthetic.items()},\n",
    "    \"supervised_metrics\": {k: float(v) for k, v in metrics_supervised.items()}\n",
    "}\n",
    "\n",
    "with open(output_dir / \"evaluation_summary.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ 結果已匯出至 results/ 目錄\")\n",
    "print(f\"   • metrics_comparison.csv\")\n",
    "print(f\"   • evaluation_summary.json\")\n",
    "print(f\"   • metrics_comparison.png\")\n",
    "print(f\"   • performance_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ 評估完成\n",
    "\n",
    "本notebook成功生成:\n",
    "- 評估指標對比表\n",
    "- 視覺化圖表\n",
    "- JSON摘要報告\n",
    "\n",
    "所有結果已儲存至 `results/` 目錄。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}