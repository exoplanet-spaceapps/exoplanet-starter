{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 Â· è©•ä¼°æŒ‡æ¨™å„€è¡¨æ¿ï¼ˆåˆæˆæ³¨å…¥ vs ç›£ç£å¼å­¸ç¿’ï¼‰\n",
    "\n",
    "## è©•ä¼°é¢å‘\n",
    "1. **æ•ˆèƒ½æŒ‡æ¨™**ï¼šPR-AUC, ROC-AUC, Precision@K, Recall@Known\n",
    "2. **æ ¡æº–æŒ‡æ¨™**ï¼šECE, Brier Score, å¯é åº¦æ›²ç·š\n",
    "3. **éŒ¯èª¤åˆ†æ**ï¼šå‡é™½æ€§ç‡, èª¤å·®æ¡ˆä¾‹ç•«å»Š\n",
    "4. **æ¨è«–æ•ˆèƒ½**ï¼šå»¶é²æ™‚é–“, ååé‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°å…¥åŸºç¤å¥—ä»¶\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    average_precision_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    brier_score_loss\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# è¨­å®šè¦–è¦ºåŒ–é¢¨æ ¼\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"ğŸ“š å¥—ä»¶å°å…¥å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. æ¨¡æ“¬æ¸¬è©¦è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡æ“¬æ¸¬è©¦è³‡æ–™\n",
    "np.random.seed(42)\n",
    "\n",
    "n_test_samples = 500\n",
    "X_test = np.random.randn(n_test_samples, 14)\n",
    "y_test = np.random.binomial(1, 0.3, n_test_samples)\n",
    "\n",
    "# æ¨¡æ“¬å…©å€‹æ¨¡å‹çš„é æ¸¬æ©Ÿç‡\n",
    "prob_synthetic = np.clip(\n",
    "    y_test * np.random.beta(8, 2, n_test_samples) + \n",
    "    (1 - y_test) * np.random.beta(2, 8, n_test_samples),\n",
    "    0.01, 0.99\n",
    ")\n",
    "\n",
    "prob_supervised = np.clip(\n",
    "    y_test * np.random.beta(6, 3, n_test_samples) + \n",
    "    (1 - y_test) * np.random.beta(3, 6, n_test_samples),\n",
    "    0.01, 0.99\n",
    ")\n",
    "\n",
    "print(f\"âœ… æ¸¬è©¦è³‡æ–™: {n_test_samples} æ¨£æœ¬, æ­£é¡æ¯”ä¾‹: {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. è¨ˆç®—è©•ä¼°æŒ‡æ¨™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_prob):\n",
    "    \"\"\"è¨ˆç®—è©•ä¼°æŒ‡æ¨™\"\"\"\n",
    "    metrics = {}\n",
    "    metrics['PR-AUC'] = average_precision_score(y_true, y_prob)\n",
    "    metrics['ROC-AUC'] = roc_auc_score(y_true, y_prob)\n",
    "    metrics['Brier Score'] = brier_score_loss(y_true, y_prob)\n",
    "    \n",
    "    # Precision@10\n",
    "    sorted_indices = np.argsort(y_prob)[::-1]\n",
    "    metrics['P@10'] = np.mean(y_true[sorted_indices[:10]])\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "metrics_synthetic = calculate_metrics(y_test, prob_synthetic)\n",
    "metrics_supervised = calculate_metrics(y_test, prob_supervised)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'åˆæˆæ³¨å…¥': metrics_synthetic,\n",
    "    'ç›£ç£å¼': metrics_supervised\n",
    "}).T\n",
    "\n",
    "print(\"ğŸ“Š è©•ä¼°æŒ‡æ¨™å°æ¯”:\")\n",
    "print(comparison_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. è¦–è¦ºåŒ–å°æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¹ªè£½æŒ‡æ¨™å°æ¯”\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "metrics_to_plot = ['PR-AUC', 'ROC-AUC', 'Brier Score', 'P@10']\n",
    "colors = ['#3498db', '#e74c3c']\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    values = [metrics_synthetic[metric], metrics_supervised[metric]]\n",
    "    bars = ax.bar(['åˆæˆæ³¨å…¥', 'ç›£ç£å¼'], values, color=colors, alpha=0.7)\n",
    "    \n",
    "    for bar, val in zip(bars, values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., val,\n",
    "                f'{val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax.set_title(metric, fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('å€¼')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('ğŸ¯ æ¨¡å‹æ•ˆèƒ½æŒ‡æ¨™å°æ¯”', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/metrics_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… æŒ‡æ¨™å°æ¯”åœ–å·²å„²å­˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PR å’Œ ROC æ›²ç·š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# PR æ›²ç·š\n",
    "precision_syn, recall_syn, _ = precision_recall_curve(y_test, prob_synthetic)\n",
    "precision_sup, recall_sup, _ = precision_recall_curve(y_test, prob_supervised)\n",
    "\n",
    "ax1.plot(recall_syn, precision_syn, label=f'åˆæˆæ³¨å…¥ (AP={metrics_synthetic[\"PR-AUC\"]:.3f})', \n",
    "         color='#3498db', linewidth=2)\n",
    "ax1.plot(recall_sup, precision_sup, label=f'ç›£ç£å¼ (AP={metrics_supervised[\"PR-AUC\"]:.3f})', \n",
    "         color='#e74c3c', linewidth=2, linestyle='--')\n",
    "ax1.set_xlabel('Recall')\n",
    "ax1.set_ylabel('Precision')\n",
    "ax1.set_title('Precision-Recall æ›²ç·š')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ROC æ›²ç·š\n",
    "fpr_syn, tpr_syn, _ = roc_curve(y_test, prob_synthetic)\n",
    "fpr_sup, tpr_sup, _ = roc_curve(y_test, prob_supervised)\n",
    "\n",
    "ax2.plot(fpr_syn, tpr_syn, label=f'åˆæˆæ³¨å…¥ (AUC={metrics_synthetic[\"ROC-AUC\"]:.3f})', \n",
    "         color='#3498db', linewidth=2)\n",
    "ax2.plot(fpr_sup, tpr_sup, label=f'ç›£ç£å¼ (AUC={metrics_supervised[\"ROC-AUC\"]:.3f})', \n",
    "         color='#e74c3c', linewidth=2, linestyle='--')\n",
    "ax2.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Random')\n",
    "ax2.set_xlabel('False Positive Rate')\n",
    "ax2.set_ylabel('True Positive Rate')\n",
    "ax2.set_title('ROC æ›²ç·š')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('ğŸ“ˆ åˆ†é¡æ•ˆèƒ½æ›²ç·š', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/performance_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… PR/ROC æ›²ç·šå·²å„²å­˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. åŒ¯å‡ºçµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰µå»ºçµæœç›®éŒ„\n",
    "output_dir = Path(\"results\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# åŒ¯å‡ºæŒ‡æ¨™\n",
    "comparison_df.to_csv(output_dir / \"metrics_comparison.csv\")\n",
    "\n",
    "# åŒ¯å‡ºæ‘˜è¦\n",
    "report = {\n",
    "    \"evaluation_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"test_samples\": int(n_test_samples),\n",
    "    \"positive_ratio\": float(y_test.mean()),\n",
    "    \"synthetic_metrics\": {k: float(v) for k, v in metrics_synthetic.items()},\n",
    "    \"supervised_metrics\": {k: float(v) for k, v in metrics_supervised.items()}\n",
    "}\n",
    "\n",
    "with open(output_dir / \"evaluation_summary.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"âœ… çµæœå·²åŒ¯å‡ºè‡³ results/ ç›®éŒ„\")\n",
    "print(f\"   â€¢ metrics_comparison.csv\")\n",
    "print(f\"   â€¢ evaluation_summary.json\")\n",
    "print(f\"   â€¢ metrics_comparison.png\")\n",
    "print(f\"   â€¢ performance_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… è©•ä¼°å®Œæˆ\n",
    "\n",
    "æœ¬notebookæˆåŠŸç”Ÿæˆ:\n",
    "- è©•ä¼°æŒ‡æ¨™å°æ¯”è¡¨\n",
    "- è¦–è¦ºåŒ–åœ–è¡¨\n",
    "- JSONæ‘˜è¦å ±å‘Š\n",
    "\n",
    "æ‰€æœ‰çµæœå·²å„²å­˜è‡³ `results/` ç›®éŒ„ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}