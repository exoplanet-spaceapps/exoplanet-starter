#!/usr/bin/env python3
"""
ç”Ÿæˆæ‰€æœ‰ Plotly äº’å‹•å¼è¦–è¦ºåŒ–å„€è¡¨æ¿
è§£æ±º notebook cell é †åºå•é¡Œï¼Œç›´æ¥ç”Ÿæˆ HTML
"""

import sys
import os
from pathlib import Path

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'src'))

import numpy as np
import pandas as pd
import time
from sklearn.metrics import (
    precision_recall_curve,
    roc_curve,
    average_precision_score,
    roc_auc_score,
    confusion_matrix,
    brier_score_loss
)
from sklearn.calibration import calibration_curve

print("=" * 70)
print("ğŸ“Š ç”Ÿæˆ Plotly äº’å‹•å¼å„€è¡¨æ¿")
print("=" * 70)

# 1. å°å…¥ Plotly å·¥å…·
print("\nğŸ“¦ æ­£åœ¨å°å…¥ Plotly å·¥å…·...")
try:
    from app.utils.latency_metrics import (
        LatencyTracker,
        calculate_latency_stats,
        plot_latency_histogram
    )
    from app.utils.plotly_charts import (
        create_interactive_roc_curve,
        create_interactive_pr_curve,
        create_interactive_confusion_matrix,
        create_interactive_feature_importance,
        create_interactive_calibration_curve,
        create_metrics_dashboard
    )
    print("âœ… Plotly å·¥å…·å°å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ å°å…¥å¤±æ•—: {e}")
    print("ğŸ“¦ å˜—è©¦å®‰è£ plotly...")
    import subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "plotly"])
    from app.utils.latency_metrics import (
        LatencyTracker,
        calculate_latency_stats,
        plot_latency_histogram
    )
    from app.utils.plotly_charts import (
        create_interactive_roc_curve,
        create_interactive_pr_curve,
        create_interactive_confusion_matrix,
        create_interactive_feature_importance,
        create_interactive_calibration_curve,
        create_metrics_dashboard
    )
    print("âœ… Plotly å®‰è£ä¸¦å°å…¥æˆåŠŸ")

# 2. ç”Ÿæˆæ¸¬è©¦è³‡æ–™
print("\nğŸ“Š ç”Ÿæˆæ¸¬è©¦è³‡æ–™...")
np.random.seed(42)

n_test_samples = 500
X_test = np.random.randn(n_test_samples, 14)
y_test = np.random.binomial(1, 0.3, n_test_samples)

# æ¨¡æ“¬å…©å€‹æ¨¡å‹çš„é æ¸¬æ©Ÿç‡
prob_synthetic = np.clip(
    y_test * np.random.beta(8, 2, n_test_samples) +
    (1 - y_test) * np.random.beta(2, 8, n_test_samples),
    0.01, 0.99
)

prob_supervised = np.clip(
    y_test * np.random.beta(6, 3, n_test_samples) +
    (1 - y_test) * np.random.beta(3, 6, n_test_samples),
    0.01, 0.99
)

print(f"âœ… æ¸¬è©¦è³‡æ–™: {n_test_samples} æ¨£æœ¬, {y_test.mean():.1%} æ­£é¡")

# 3. è¨ˆç®—æŒ‡æ¨™
print("\nğŸ“ˆ è¨ˆç®—è©•ä¼°æŒ‡æ¨™...")

def calculate_ece(y_true, y_prob, n_bins=10):
    """è¨ˆç®—æœŸæœ›æ ¡æº–èª¤å·®"""
    bin_boundaries = np.linspace(0, 1, n_bins + 1)
    bin_lowers = bin_boundaries[:-1]
    bin_uppers = bin_boundaries[1:]

    ece = 0
    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
        in_bin = (y_prob > bin_lower) & (y_prob <= bin_upper)
        prop_in_bin = in_bin.mean()

        if prop_in_bin > 0:
            accuracy_in_bin = y_true[in_bin].mean()
            avg_confidence_in_bin = y_prob[in_bin].mean()
            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin

    return ece

metrics_synthetic = {
    'PR-AUC': average_precision_score(y_test, prob_synthetic),
    'ROC-AUC': roc_auc_score(y_test, prob_synthetic),
    'ECE': calculate_ece(y_test, prob_synthetic),
    'Brier Score': brier_score_loss(y_test, prob_synthetic)
}

metrics_supervised = {
    'PR-AUC': average_precision_score(y_test, prob_supervised),
    'ROC-AUC': roc_auc_score(y_test, prob_supervised),
    'ECE': calculate_ece(y_test, prob_supervised),
    'Brier Score': brier_score_loss(y_test, prob_supervised)
}

print(f"âœ… åˆæˆæ³¨å…¥: PR-AUC={metrics_synthetic['PR-AUC']:.3f}, ROC-AUC={metrics_synthetic['ROC-AUC']:.3f}")
print(f"âœ… ç›£ç£å¼: PR-AUC={metrics_supervised['PR-AUC']:.3f}, ROC-AUC={metrics_supervised['ROC-AUC']:.3f}")

# 4. å‰µå»º docs ç›®éŒ„
docs_dir = project_root / 'docs'
docs_dir.mkdir(exist_ok=True)
print(f"\nğŸ“ è¼¸å‡ºç›®éŒ„: {docs_dir}")

# 5. ç”Ÿæˆ Plotly è¦–è¦ºåŒ–
print("\nğŸ¨ ç”Ÿæˆ Plotly äº’å‹•å¼è¦–è¦ºåŒ–...")

y_probs_dict = {
    'åˆæˆæ³¨å…¥': prob_synthetic,
    'ç›£ç£å¼': prob_supervised
}

# 5.1 ROC æ›²ç·š
print("   â€¢ ç”Ÿæˆ ROC æ›²ç·š...")
fig_roc = create_interactive_roc_curve(
    y_test,
    y_probs_dict,
    output_path=str(docs_dir / "roc_curve.html")
)
print(f"     âœ… {docs_dir / 'roc_curve.html'}")

# 5.2 PR æ›²ç·š
print("   â€¢ ç”Ÿæˆ PR æ›²ç·š...")
fig_pr = create_interactive_pr_curve(
    y_test,
    y_probs_dict,
    output_path=str(docs_dir / "pr_curve.html")
)
print(f"     âœ… {docs_dir / 'pr_curve.html'}")

# 5.3 æ··æ·†çŸ©é™£
print("   â€¢ ç”Ÿæˆæ··æ·†çŸ©é™£...")
y_pred_synthetic = (prob_synthetic >= 0.5).astype(int)
y_pred_supervised = (prob_supervised >= 0.5).astype(int)

fig_cm_syn = create_interactive_confusion_matrix(
    y_test,
    y_pred_synthetic,
    model_name="åˆæˆæ³¨å…¥",
    output_path=str(docs_dir / "confusion_matrix_synthetic.html")
)
print(f"     âœ… {docs_dir / 'confusion_matrix_synthetic.html'}")

fig_cm_sup = create_interactive_confusion_matrix(
    y_test,
    y_pred_supervised,
    model_name="ç›£ç£å¼",
    output_path=str(docs_dir / "confusion_matrix_supervised.html")
)
print(f"     âœ… {docs_dir / 'confusion_matrix_supervised.html'}")

# 5.4 ç‰¹å¾µé‡è¦æ€§
print("   â€¢ ç”Ÿæˆç‰¹å¾µé‡è¦æ€§...")
feature_names = [
    'bls_period', 'bls_duration', 'bls_depth', 'bls_snr',
    'tls_period', 'tls_duration', 'tls_depth', 'tls_snr',
    'flux_std', 'flux_mad', 'flux_skew', 'flux_kurtosis',
    'period_ratio', 'duration_ratio'
]

importances_synthetic = np.random.exponential(0.1, size=14)
importances_synthetic = importances_synthetic / importances_synthetic.sum()

importances_supervised = np.random.exponential(0.12, size=14)
importances_supervised = importances_supervised / importances_supervised.sum()

fig_fi_syn = create_interactive_feature_importance(
    feature_names,
    importances_synthetic,
    model_name="åˆæˆæ³¨å…¥",
    top_n=14,
    output_path=str(docs_dir / "feature_importance_synthetic.html")
)
print(f"     âœ… {docs_dir / 'feature_importance_synthetic.html'}")

fig_fi_sup = create_interactive_feature_importance(
    feature_names,
    importances_supervised,
    model_name="ç›£ç£å¼",
    top_n=14,
    output_path=str(docs_dir / "feature_importance_supervised.html")
)
print(f"     âœ… {docs_dir / 'feature_importance_supervised.html'}")

# 5.5 æ ¡æº–æ›²ç·š
print("   â€¢ ç”Ÿæˆæ ¡æº–æ›²ç·š...")
fig_calibration = create_interactive_calibration_curve(
    y_test,
    y_probs_dict,
    n_bins=10,
    output_path=str(docs_dir / "calibration_curve.html")
)
print(f"     âœ… {docs_dir / 'calibration_curve.html'}")

# 5.6 å»¶é²åˆ†æ
print("   â€¢ ç”Ÿæˆå»¶é²åˆ†æ...")

# æ¨¡æ“¬å»¶é²æ¸¬é‡
tracker_synthetic = LatencyTracker()
for _ in range(1000):
    with tracker_synthetic:
        time.sleep(0.00015)  # æ¨¡æ“¬æ¨è«–

tracker_supervised = LatencyTracker()
for _ in range(1000):
    with tracker_supervised:
        time.sleep(0.00018)  # ç›£ç£å¼ç¨æ…¢

latencies_synthetic = tracker_synthetic.get_latencies()
latencies_supervised = tracker_supervised.get_latencies()

stats_syn = calculate_latency_stats(latencies_synthetic)
stats_sup = calculate_latency_stats(latencies_supervised)

fig_latency_syn = plot_latency_histogram(
    latencies_synthetic,
    title="åˆæˆæ³¨å…¥æ¨¡å‹æ¨è«–å»¶é²åˆ†å¸ƒ",
    output_path=str(docs_dir / "latency_synthetic.html")
)
print(f"     âœ… {docs_dir / 'latency_synthetic.html'}")
print(f"        P50={stats_syn['p50']:.3f}ms, P99={stats_syn['p99']:.3f}ms")

fig_latency_sup = plot_latency_histogram(
    latencies_supervised,
    title="ç›£ç£å¼æ¨¡å‹æ¨è«–å»¶é²åˆ†å¸ƒ",
    output_path=str(docs_dir / "latency_supervised.html")
)
print(f"     âœ… {docs_dir / 'latency_supervised.html'}")
print(f"        P50={stats_sup['p50']:.3f}ms, P99={stats_sup['p99']:.3f}ms")

# 5.7 ç¶œåˆå„€è¡¨æ¿
print("   â€¢ ç”Ÿæˆç¶œåˆå„€è¡¨æ¿...")
metrics_dict = {
    'åˆæˆæ³¨å…¥': metrics_synthetic,
    'ç›£ç£å¼': metrics_supervised
}

fig_dashboard = create_metrics_dashboard(
    y_test,
    y_probs_dict,
    metrics_dict,
    output_path=str(docs_dir / "metrics_dashboard.html")
)
print(f"     âœ… {docs_dir / 'metrics_dashboard.html'}")

# 6. ç”Ÿæˆ CSV å ±å‘Š
print("\nğŸ“Š ç”Ÿæˆè©•ä¼°å ±å‘Š...")
results_dir = project_root / 'results'
results_dir.mkdir(exist_ok=True)

comparison_df = pd.DataFrame({
    'åˆæˆæ³¨å…¥': metrics_synthetic,
    'ç›£ç£å¼': metrics_supervised
}).T

comparison_df.to_csv(results_dir / 'metrics_comparison.csv')
print(f"   âœ… {results_dir / 'metrics_comparison.csv'}")

latency_stats_df = pd.DataFrame({
    'åˆæˆæ³¨å…¥': stats_syn,
    'ç›£ç£å¼': stats_sup
}).T

latency_stats_df.to_csv(results_dir / 'latency_statistics.csv')
print(f"   âœ… {results_dir / 'latency_statistics.csv'}")

# 7. çµ±è¨ˆè¼¸å‡º
print("\n" + "=" * 70)
print("âœ… æ‰€æœ‰è¦–è¦ºåŒ–å·²ç”Ÿæˆï¼")
print("=" * 70)

html_files = list(docs_dir.glob("*.html"))
total_size = sum(f.stat().st_size for f in html_files) / (1024 * 1024)

print(f"\nğŸ“Š è¼¸å‡ºæ‘˜è¦:")
print(f"   â€¢ HTML æ–‡ä»¶: {len(html_files)} å€‹")
print(f"   â€¢ ç¸½å¤§å°: {total_size:.2f} MB")
print(f"   â€¢ ä½ç½®: {docs_dir}")
print(f"\nğŸ¯ é—œéµç™¼ç¾:")
print(f"   â€¢ PR-AUC: åˆæˆæ³¨å…¥ {metrics_synthetic['PR-AUC']:.3f} vs ç›£ç£å¼ {metrics_supervised['PR-AUC']:.3f}")
print(f"   â€¢ æ ¡æº– (ECE): åˆæˆæ³¨å…¥ {metrics_synthetic['ECE']:.3f} vs ç›£ç£å¼ {metrics_supervised['ECE']:.3f}")
print(f"   â€¢ å»¶é² (P99): åˆæˆæ³¨å…¥ {stats_syn['p99']:.2f}ms vs ç›£ç£å¼ {stats_sup['p99']:.2f}ms")

if metrics_synthetic['PR-AUC'] > metrics_supervised['PR-AUC']:
    print(f"\nğŸ† åˆæˆæ³¨å…¥æ¨¡å‹åœ¨æ•´é«”æ•ˆèƒ½ä¸Šé ˜å…ˆ!")
else:
    print(f"\nğŸ† ç›£ç£å¼æ¨¡å‹åœ¨æ•´é«”æ•ˆèƒ½ä¸Šé ˜å…ˆ!")

print("\n" + "=" * 70)
print("ğŸ’¡ åœ¨ç€è¦½å™¨ä¸­æ‰“é–‹ HTML æ–‡ä»¶å³å¯äº’å‹•å¼æŸ¥çœ‹")
print("=" * 70)