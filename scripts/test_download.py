#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æµ‹è¯•ä¸‹è½½è„šæœ¬ - ä¸‹è½½ 100 ä¸ªæ ·æœ¬æµ‹è¯•æµç¨‹
ç›´æ¥è¿è¡Œï¼špython scripts/test_download.py
"""

import os
import sys
import json
import time
import warnings
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

# è®¾ç½® Windows æ§åˆ¶å°ç¼–ç 
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

import numpy as np
import pandas as pd
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))
os.chdir(PROJECT_ROOT)

warnings.filterwarnings('ignore')

print("=" * 70)
print("[TEST] Exoplanet Detection - Test Download Script")
print("=" * 70)
print()

# æ£€æŸ¥ä¾èµ–
print("[CHECK] Checking dependencies...")
try:
    import lightkurve as lk
    import joblib
    print(f"   [OK] lightkurve: {lk.__version__}")
    print(f"   [OK] numpy: {np.__version__}")
    print(f"   [OK] pandas: {pd.__version__}")
except ImportError as e:
    print(f"   [ERROR] Missing dependency: {e}")
    print("\nPlease install dependencies:")
    print("   pip install lightkurve pandas numpy tqdm joblib")
    sys.exit(1)

# é…ç½®è·¯å¾„
BASE_DIR = PROJECT_ROOT
DATA_DIR = BASE_DIR / 'data'
LIGHTCURVE_DIR = DATA_DIR / 'lightcurves'
CHECKPOINT_DIR = BASE_DIR / 'checkpoints'

LIGHTCURVE_DIR.mkdir(parents=True, exist_ok=True)
CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)

print(f"\n[PATH] Working directory: {BASE_DIR}")
print(f"   Data: {DATA_DIR}")
print(f"   Lightcurves: {LIGHTCURVE_DIR}")
print(f"   Checkpoints: {CHECKPOINT_DIR}")

# é…ç½®
CONFIG = {
    'max_workers': 4,
    'max_retries': 3,
    'timeout': 60,
    'batch_size': 100,
    'save_interval': 20,
    'test_samples': 100,  # æµ‹è¯•æ ·æœ¬æ•°
}

print(f"\n[CONFIG] Test configuration:")
for key, val in CONFIG.items():
    print(f"   {key}: {val}")

# åŠ è½½æ•°æ®é›†
print(f"\n[DATA] Loading dataset...")
dataset_path = DATA_DIR / 'supervised_dataset.csv'
if not dataset_path.exists():
    print(f"[ERROR] Dataset not found: {dataset_path}")
    sys.exit(1)

samples_df = pd.read_csv(dataset_path)
print(f"   Total samples: {len(samples_df):,}")

# æµ‹è¯•æ¨¡å¼ï¼šåªå–å‰ 100 ä¸ª
samples_df = samples_df.head(CONFIG['test_samples'])
print(f"   [TEST MODE] Processing {len(samples_df)} samples")

# æ·»åŠ  ID
if 'sample_id' not in samples_df.columns:
    samples_df['sample_id'] = [f"SAMPLE_{i:06d}" for i in range(len(samples_df))]

if 'tic_id' not in samples_df.columns:
    if 'tid' in samples_df.columns:
        samples_df['tic_id'] = samples_df['tid']
    elif 'target_id' in samples_df.columns:
        samples_df['tic_id'] = samples_df['target_id']

print(f"   Positive: {samples_df['label'].sum()}")
print(f"   Negative: {(~samples_df['label'].astype(bool)).sum()}")


# ä¸‹è½½å‡½æ•°
def download_single_lightcurve(row: pd.Series, retries: int = 3) -> dict:
    """ä¸‹è½½å•ä¸ªå…‰æ›²çº¿"""
    sample_id = row['sample_id']
    tic_id = int(float(row['tic_id']))

    result = {
        'sample_id': sample_id,
        'tic_id': tic_id,
        'status': 'failed',
        'file_path': None,
        'n_sectors': 0,
        'error': None
    }

    # æ£€æŸ¥ç¼“å­˜
    file_path = LIGHTCURVE_DIR / f"{sample_id}_TIC{tic_id}.pkl"
    if file_path.exists():
        result['status'] = 'cached'
        result['file_path'] = str(file_path)
        return result

    # ä¸‹è½½
    for attempt in range(retries):
        try:
            search_result = lk.search_lightcurve(f"TIC {tic_id}", author='SPOC')

            if search_result is None or len(search_result) == 0:
                result['error'] = 'no_data_found'
                return result

            lc_collection = search_result.download_all()

            if lc_collection is None or len(lc_collection) == 0:
                result['error'] = 'download_failed'
                return result

            # ä¿å­˜
            save_data = {
                'sample_id': sample_id,
                'tic_id': tic_id,
                'lc_collection': lc_collection,
                'n_sectors': len(lc_collection),
                'download_time': datetime.now().isoformat(),
                'sectors': [lc.meta.get('SECTOR', 'unknown') for lc in lc_collection]
            }

            joblib.dump(save_data, file_path)

            result['status'] = 'success'
            result['file_path'] = str(file_path)
            result['n_sectors'] = len(lc_collection)
            return result

        except Exception as e:
            result['error'] = str(e)[:100]  # é™åˆ¶é”™è¯¯ä¿¡æ¯é•¿åº¦
            if attempt < retries - 1:
                time.sleep(2 ** attempt)
                continue

    return result


# åŠ è½½è¿›åº¦
def load_checkpoint():
    checkpoint_path = CHECKPOINT_DIR / 'download_progress.parquet'
    if checkpoint_path.exists():
        df = pd.read_parquet(checkpoint_path)
        print(f"   ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹: {len(df)} æ¡è®°å½•")
        return df
    return pd.DataFrame()


def save_checkpoint(progress_df):
    checkpoint_path = CHECKPOINT_DIR / 'download_progress.parquet'
    progress_df.to_parquet(checkpoint_path, index=False)
    print(f"   ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {len(progress_df)} æ¡è®°å½•")


# ä¸»æ‰§è¡Œ
print("\n" + "=" * 70)
print("ğŸš€ å¼€å§‹æµ‹è¯•ä¸‹è½½")
print("=" * 70)

progress_df = load_checkpoint()

# ç¡®å®šå¾…ä¸‹è½½
if len(progress_df) > 0:
    completed_ids = set(progress_df[progress_df['status'].isin(['success', 'cached'])]['sample_id'])
    remaining_samples = samples_df[~samples_df['sample_id'].isin(completed_ids)]
else:
    remaining_samples = samples_df.copy()

print(f"\nğŸ“Š ä¸‹è½½è¿›åº¦:")
print(f"   æ€»æ ·æœ¬: {len(samples_df)}")
print(f"   å·²å®Œæˆ: {len(samples_df) - len(remaining_samples)}")
print(f"   å¾…ä¸‹è½½: {len(remaining_samples)}")

if len(remaining_samples) == 0:
    print("\n[OK] æ‰€æœ‰æ ·æœ¬å·²ä¸‹è½½ï¼")
else:
    print(f"\n[TIME]: {len(remaining_samples) * 5 / 60 / CONFIG['max_workers']:.1f} åˆ†é’Ÿ")
    print(f"   å¹¶å‘æ•°: {CONFIG['max_workers']}")
    print()

    start_time = time.time()
    results = []

    # å¹¶å‘ä¸‹è½½
    with ThreadPoolExecutor(max_workers=CONFIG['max_workers']) as executor:
        future_to_row = {
            executor.submit(download_single_lightcurve, row, CONFIG['max_retries']): row
            for _, row in remaining_samples.iterrows()
        }

        with tqdm(total=len(remaining_samples), desc="ğŸ“¥ ä¸‹è½½ä¸­") as pbar:
            for future in as_completed(future_to_row):
                result = future.result()
                results.append(result)
                pbar.update(1)

                # å®šæœŸä¿å­˜
                if len(results) % CONFIG['save_interval'] == 0:
                    temp_df = pd.concat([progress_df, pd.DataFrame(results)], ignore_index=True)
                    save_checkpoint(temp_df)

                    success = sum(1 for r in results if r['status'] == 'success')
                    cached = sum(1 for r in results if r['status'] == 'cached')
                    failed = sum(1 for r in results if r['status'] == 'failed')

                    pbar.set_postfix({
                        'success': success,
                        'cached': cached,
                        'failed': failed
                    })

    # æœ€ç»ˆä¿å­˜
    if len(results) > 0:
        progress_df = pd.concat([progress_df, pd.DataFrame(results)], ignore_index=True)
        save_checkpoint(progress_df)

    elapsed = time.time() - start_time

    print(f"\nğŸ‰ ä¸‹è½½å®Œæˆ!")
    print(f"   æ€»è€—æ—¶: {elapsed / 60:.1f} åˆ†é’Ÿ")
    print(f"   å¹³å‡é€Ÿåº¦: {elapsed / len(results):.1f} ç§’/æ ·æœ¬")

# ç»Ÿè®¡
print("\n" + "=" * 70)
print("ğŸ“Š æœ€ç»ˆç»Ÿè®¡")
print("=" * 70)

status_counts = progress_df['status'].value_counts()
for status, count in status_counts.items():
    print(f"   {status}: {count}")

success_rate = (status_counts.get('success', 0) + status_counts.get('cached', 0)) / len(progress_df) * 100
print(f"\n   æˆåŠŸç‡: {success_rate:.1f}%")

# éªŒè¯æ–‡ä»¶
pkl_files = list(LIGHTCURVE_DIR.glob('*.pkl'))
print(f"   æ–‡ä»¶æ€»æ•°: {len(pkl_files)}")

if len(pkl_files) > 0:
    total_size = sum(f.stat().st_size for f in pkl_files) / 1024 / 1024
    print(f"   æ€»å¤§å°: {total_size:.1f} MB")
    print(f"   å¹³å‡å¤§å°: {total_size / len(pkl_files):.1f} MB/æ–‡ä»¶")

# ä¿å­˜æŠ¥å‘Š
report = {
    'timestamp': datetime.now().isoformat(),
    'test_mode': True,
    'test_samples': CONFIG['test_samples'],
    'total_samples': len(samples_df),
    'downloaded': int(status_counts.get('success', 0) + status_counts.get('cached', 0)),
    'failed': int(status_counts.get('failed', 0)),
    'success_rate': float(success_rate),
    'config': CONFIG,
    'storage': {
        'directory': str(LIGHTCURVE_DIR),
        'total_files': len(pkl_files),
        'total_size_mb': float(total_size) if len(pkl_files) > 0 else 0
    }
}

if progress_df[progress_df['status'] == 'failed']['error'].notna().any():
    report['errors'] = progress_df[progress_df['status'] == 'failed']['error'].value_counts().to_dict()

report_path = CHECKPOINT_DIR / 'test_download_report.json'
with open(report_path, 'w') as f:
    json.dump(report, f, indent=2)

print(f"\n[OK] æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

# éªŒè¯æ ·æœ¬
if len(pkl_files) >= 3:
    print("\n" + "=" * 70)
    print("ğŸ” éªŒè¯ä¸‹è½½æ•°æ®ï¼ˆéšæœºæŠ½æ ·ï¼‰")
    print("=" * 70)

    sample_files = np.random.choice(pkl_files, 3, replace=False)

    for pkl_file in sample_files:
        try:
            data = joblib.load(pkl_file)
            print(f"\n[OK] {pkl_file.name}")
            print(f"   TIC ID: {data['tic_id']}")
            print(f"   æ‰‡åŒºæ•°: {data['n_sectors']} {data['sectors']}")
            print(f"   ä¸‹è½½æ—¶é—´: {data['download_time']}")

            lc = data['lc_collection'][0]
            print(f"   æ•°æ®ç‚¹: {len(lc.time):,}")
            print(f"   æ—¶é—´è·¨åº¦: {float(lc.time[-1] - lc.time[0]):.1f} å¤©")
        except Exception as e:
            print(f"[ERROR] {pkl_file.name}: {e}")

print("\n" + "=" * 70)
print("[OK] æµ‹è¯•å®Œæˆï¼")
print("=" * 70)

if success_rate >= 80:
    print("\nğŸ‰ æµ‹è¯•é€šè¿‡ï¼å¯ä»¥ç»§ç»­ä»¥ä¸‹æ­¥éª¤ï¼š")
    print("   1. è¿è¡Œç‰¹å¾æå–æµ‹è¯•: python scripts/test_features.py")
    print("   2. å¦‚æœæµ‹è¯•æ»¡æ„ï¼Œä¿®æ”¹é…ç½®è¿›è¡Œå…¨é‡ä¸‹è½½")
else:
    print(f"\n[WARN] æˆåŠŸç‡è¾ƒä½ ({success_rate:.1f}%)")
    print("   å»ºè®®æ£€æŸ¥ï¼š")
    print("   - ç½‘ç»œè¿æ¥")
    print("   - MAST æœåŠ¡çŠ¶æ€: https://mast.stsci.edu/")
    print(f"   - é”™è¯¯è¯¦æƒ…: {report_path}")

print()
