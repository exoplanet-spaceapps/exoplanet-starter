"""
GPU Detection and Configuration Utilities
GPU æª¢æ¸¬èˆ‡é…ç½®å·¥å…· (2025 Best Practices)
"""
import subprocess
import logging
from typing import Dict, Any, Optional


def detect_gpu() -> Dict[str, Any]:
    """
    æª¢æ¸¬ GPU å¯ç”¨æ€§èˆ‡é…ç½®

    Returns:
        dict: GPU è³‡è¨Šå­—å…¸ï¼ŒåŒ…å«:
            - available: GPU æ˜¯å¦å¯ç”¨ (bool)
            - device_count: GPU æ•¸é‡ (int)
            - device_name: GPU åç¨± (str)
            - cuda_version: CUDA ç‰ˆæœ¬ (str)
            - memory_gb: ç¸½è¨˜æ†¶é«” (float)
            - pytorch_available: PyTorch æ˜¯å¦å¯ç”¨ (bool)
            - xgboost_gpu_support: XGBoost GPU æ˜¯å¦æ”¯æ´ (bool)
    """
    gpu_info = {
        'available': False,
        'device_count': 0,
        'device_name': None,
        'cuda_version': None,
        'memory_gb': None,
        'pytorch_available': False,
        'xgboost_gpu_support': False
    }

    # æª¢æŸ¥ PyTorch GPU æ”¯æ´
    try:
        import torch
        gpu_info['pytorch_available'] = True

        if torch.cuda.is_available():
            gpu_info['available'] = True
            gpu_info['device_count'] = torch.cuda.device_count()
            gpu_info['device_name'] = torch.cuda.get_device_name(0)
            gpu_info['cuda_version'] = torch.version.cuda
            gpu_info['memory_gb'] = torch.cuda.get_device_properties(0).total_memory / 1024**3

    except ImportError:
        pass

    # å¦‚æœ PyTorch ä¸å¯ç”¨ï¼Œå˜—è©¦ç”¨ nvidia-smi
    if not gpu_info['available']:
        try:
            result = subprocess.run(
                ['nvidia-smi', '--query-gpu=name', '--format=csv,noheader'],
                capture_output=True,
                text=True,
                check=False,
                timeout=5
            )
            if result.returncode == 0 and result.stdout.strip():
                gpu_info['available'] = True
                gpu_info['device_name'] = result.stdout.strip()
                gpu_info['device_count'] = len(result.stdout.strip().split('\n'))
        except (subprocess.TimeoutExpired, FileNotFoundError):
            pass

    # æª¢æŸ¥ XGBoost GPU æ”¯æ´
    try:
        import xgboost as xgb
        # XGBoost 2.0+ æ”¯æ´ GPU
        xgb_version = tuple(map(int, xgb.__version__.split('.')[:2]))
        if xgb_version >= (2, 0) and gpu_info['available']:
            gpu_info['xgboost_gpu_support'] = True
    except ImportError:
        pass

    return gpu_info


def get_xgboost_gpu_params(
    tree_method: str = 'hist',
    device: str = 'cuda',
    gpu_id: int = 0
) -> Dict[str, Any]:
    """
    ç²å– XGBoost 2.x GPU è¨“ç·´åƒæ•¸

    Args:
        tree_method: æ¨¹æ§‹å»ºæ–¹æ³•ï¼Œæ¨è–¦ 'hist' (æœ€å¿«)
        device: è¨­å‚™é¡å‹ï¼Œ'cuda' æˆ– 'cpu'
        gpu_id: GPU ID (å¦‚æœæœ‰å¤šå€‹ GPU)

    Returns:
        dict: XGBoost åƒæ•¸å­—å…¸

    Example:
        >>> gpu_params = get_xgboost_gpu_params()
        >>> model = xgb.XGBClassifier(**gpu_params, n_estimators=100)
    """
    gpu_info = detect_gpu()

    if not gpu_info['available'] or not gpu_info['xgboost_gpu_support']:
        return {
            'tree_method': 'hist',
            'device': 'cpu'
        }

    # XGBoost 2.x ä½¿ç”¨ device='cuda' è€Œä¸æ˜¯ gpu_id
    return {
        'tree_method': tree_method,  # 'hist' æ˜¯æœ€å¿«çš„æ–¹æ³•
        'device': device,             # 'cuda' å•Ÿç”¨ GPU
        # 'gpu_id': gpu_id,           # èˆŠç‰ˆç”¨æ³•ï¼Œ2.x ä¸éœ€è¦
        # 'predictor': 'gpu_predictor', # èˆŠç‰ˆç”¨æ³•ï¼Œ2.x æœƒè‡ªå‹•ä½¿ç”¨
    }


def log_gpu_info(logger: Optional[logging.Logger] = None) -> None:
    """
    è¨˜éŒ„ GPU è³‡è¨Šåˆ°æ—¥èªŒ

    Args:
        logger: Logger ç‰©ä»¶ï¼Œå¦‚æœç‚º None å‰‡ä½¿ç”¨ print
    """
    gpu_info = detect_gpu()

    if logger is None:
        print_fn = print
    else:
        print_fn = logger.info

    print_fn("="*60)
    print_fn("ğŸ–¥ï¸ GPU Configuration")
    print_fn("="*60)

    if gpu_info['available']:
        print_fn(f"GPU Available: âœ… YES")
        print_fn(f"GPU Count: {gpu_info['device_count']}")
        print_fn(f"GPU Name: {gpu_info['device_name']}")

        if gpu_info['cuda_version']:
            print_fn(f"CUDA Version: {gpu_info['cuda_version']}")

        if gpu_info['memory_gb']:
            print_fn(f"GPU Memory: {gpu_info['memory_gb']:.2f} GB")

        # L4 GPU ç‰¹æ®Šæç¤º
        if gpu_info['device_name'] and 'L4' in gpu_info['device_name']:
            print_fn("ğŸ’¡ NVIDIA L4 GPU Detected - Supports BF16 acceleration")

        # XGBoost GPU æ”¯æ´
        if gpu_info['xgboost_gpu_support']:
            print_fn("XGBoost GPU: âœ… Supported (use device='cuda')")
        else:
            print_fn("XGBoost GPU: âŒ Not supported (need XGBoost 2.0+)")

    else:
        print_fn("GPU Available: âŒ NO - Using CPU")
        print_fn("ğŸ’¡ For faster training, use Colab with GPU runtime:")
        print_fn("   Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU")

    print_fn("="*60)


def get_pytorch_device() -> str:
    """
    ç²å– PyTorch æœ€ä½³è¨­å‚™

    Returns:
        str: 'cuda' æˆ– 'cpu'

    Example:
        >>> device = get_pytorch_device()
        >>> model = model.to(device)
    """
    gpu_info = detect_gpu()
    return 'cuda' if gpu_info['available'] and gpu_info['pytorch_available'] else 'cpu'


def configure_gpu_memory_growth():
    """
    é…ç½® TensorFlow/PyTorch å‹•æ…‹è¨˜æ†¶é«”å¢é•·
    é¿å…ä¸€æ¬¡æ€§å ç”¨æ‰€æœ‰ GPU è¨˜æ†¶é«”
    """
    # TensorFlow
    try:
        import tensorflow as tf
        gpus = tf.config.list_physical_devices('GPU')
        if gpus:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print(f"âœ… TensorFlow GPU memory growth enabled ({len(gpus)} GPUs)")
    except (ImportError, RuntimeError):
        pass

    # PyTorch (ä¸éœ€è¦ç‰¹åˆ¥é…ç½®ï¼Œé è¨­å°±æ˜¯å‹•æ…‹åˆ†é…)
    try:
        import torch
        if torch.cuda.is_available():
            # è¨­å®š PyTorch CUDA è¨˜æ†¶é«”åˆ†é…ç­–ç•¥
            torch.cuda.empty_cache()  # æ¸…ç©ºå¿«å–
            print(f"âœ… PyTorch GPU available ({torch.cuda.device_count()} GPUs)")
    except ImportError:
        pass


def print_gpu_memory_usage():
    """
    é¡¯ç¤ºç•¶å‰ GPU è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³
    """
    try:
        import torch
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                allocated = torch.cuda.memory_allocated(i) / 1024**3
                reserved = torch.cuda.memory_reserved(i) / 1024**3
                total = torch.cuda.get_device_properties(i).total_memory / 1024**3

                print(f"GPU {i} ({torch.cuda.get_device_name(i)}):")
                print(f"  Allocated: {allocated:.2f} GB / {total:.2f} GB")
                print(f"  Reserved:  {reserved:.2f} GB / {total:.2f} GB")
                print(f"  Free:      {total - allocated:.2f} GB")
    except ImportError:
        print("PyTorch not available - cannot check GPU memory")